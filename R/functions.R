source("R/lib.R")

Sys.setenv(RETICULATE_AUTOCONFIGURE = FALSE)
library(readr)
library(dplyr)
library(stringr)
library(rjson)
library(tidyr)
library(greta)
library(greta.gp)
library(readxl)
library(RColorBrewer)
library(tensorflow)
library(purrr)
library(ggplot2)
library(R6)
library(slider)
library(cowplot)
library(lubridate)
library(readxl)
library(rvest)
library(lubridate)

tfp <- reticulate::import("tensorflow_probability")

module <- greta::.internals$utils$misc$module
fl <- greta:::fl
tf_float <- greta:::tf_float

# read in and tidy up Facebook movement data
facebook_mobility <- function() {
  
  file <- "data/fb_data/au_gadm_mobility_statistics.20200427.csv"
  data <- readr::read_csv(file) %>%
    dplyr::select(
      state = polygon_name,
      date = ds,
      "staying still" = all_day_ratio_single_tile_users
    ) %>%
    tidyr::pivot_longer(
      cols = c("staying still"),
      names_to = "metric",
      values_to = "trend"
    ) %>%
    mutate(date = lubridate::date(date)) %>%
    mutate(weekday = lubridate::wday(date))  
  
  # set the staying home variable against a baseline of the first two weeks
  baseline <- data %>%
    filter(date < lubridate::date("2020-03-15")) %>%
    group_by(state, metric, weekday) %>%
    summarise(baseline = median(trend))
  
  data <- data %>%
    left_join(baseline) %>%
    mutate(
      corrected = (trend - baseline) / abs(baseline)
    ) %>%
    mutate(
      trend = ifelse(metric == "staying still", corrected, trend)
    ) %>%
    select(
      -corrected,
      -baseline,
      -weekday
    ) %>%
    mutate(
      trend = trend * 100,
    )
  
  # add a composite national trend with population weights
  relative_population <- state_populations() %>%
    arrange(state) %>%
    mutate(fraction = population / sum(population)) %>%
    dplyr::select(-population)
  
  national_data <- data %>%
    left_join(relative_population) %>%
    group_by(date, metric) %>%
    summarise(trend = sum(trend * fraction)) %>%
    ungroup() %>%
    mutate(state = NA)
  
  bind_rows(
    national_data,
    data
  )
  
}

# load and format Jono Carroll's scraping of Aus mobility data
# - remove the grocery and pharmacy category
# (affected by panic buying, not interventions)
google_mobility <- function() {
  # get link from: https://www.google.com/covid19/mobility/index.html
  url <- "https://www.gstatic.com/covid19/mobility/Global_Mobility_Report.csv"
  data <- readr::read_csv(
    url, 
    col_types = cols(
      country_region_code = col_character(),
      country_region = col_character(),
      sub_region_1 = col_character(),
      sub_region_2 = col_character(),
      date = col_date(format = "%Y-%m-%d"),
      retail_and_recreation_percent_change_from_baseline = col_double(),
      grocery_and_pharmacy_percent_change_from_baseline = col_double(),
      parks_percent_change_from_baseline = col_double(),
      transit_stations_percent_change_from_baseline = col_double(),
      workplaces_percent_change_from_baseline = col_double(),
      residential_percent_change_from_baseline = col_double(),
      census_fips_code = col_character()
    )
  ) %>%
    filter(
      country_region == "Australia" & is.na(sub_region_2)
    ) %>%
    tidyr::pivot_longer(
      ends_with("_percent_change_from_baseline"),
      names_to = "category",
      values_to = "trend"
    ) %>%
    dplyr::select(
      state = sub_region_1,
      category = category,
      date = date,
      trend = trend
    ) %>%
    mutate(
      category = str_remove_all(category, "_percent_change_from_baseline"),
      category = str_replace_all(category, "_", " ")
    )
  data
}

# scrape out the URL to apple mobility data
apple_url <- function() {
  base_url <- "https://covid19-static.cdn-apple.com"
  json_data <- base_url %>%
    file.path("covid19-mobility-data/current/v3/index.json") %>%
    jsonlite::fromJSON()
  paste0(base_url, json_data$basePath, json_data$regions$`en-us`$csvPath)
}

# download and format Apple's mobility data
apple_mobility <- function() {
  data <- apple_url() %>%
    readr::read_csv(
      col_types = cols(
        .default = col_double(),
        geo_type = col_character(),
        region = col_character(),
        transportation_type = col_character(),
        alternative_name = col_character(),
        `sub-region` = col_character(),
        country = col_character()
      )) %>%
    tidyr::pivot_longer(
      cols = !any_of(
        c(
          "geo_type",
          "region",
          "transportation_type",
          "alternative_name",
          "sub-region",
          "country"
        )
      ),
      names_to = "date",
      values_to = "trend"
    ) %>%
    mutate(
      date = lubridate::date(date)
    ) %>%
    filter(
      region %in% ideal_regions()
    )
  
  # transform to percentage change, and correct dates (they are agreegated
  # midnight-midnight in pacific central time, for which Australian daylight
  # hours fall on the next calendar day in Australia)
  data <- data %>%
    mutate(trend = trend - 100,
           date = date + 1)
  
  # pull out the states and cities
  states <- c("New South Wales", "Victoria", "Queensland", "Western Australia", 
              "South Australia", "Australian Capital Territory", "Tasmania", 
              "Northern Territory")
  
  cities <- c("Sydney", "Melbourne", "Brisbane", "Perth", 
              "Adelaide", "Canberra", "Hobart", "Darwin")
  
  # need to use state-level data for driving, but cities for other metrics
  data <- data %>%
    filter(
      case_when(
        transportation_type == "driving" & region %in% states ~ TRUE,
        transportation_type != "driving" & region %in% cities ~ TRUE,
        TRUE ~ FALSE
      )
    ) %>%
    mutate(
      region = case_when(
        region == "Sydney" ~ "New South Wales",
        region == "Melbourne" ~ "Victoria",
        region == "Brisbane" ~ "Queensland",
        region == "Perth" ~ "Western Australia",
        region == "Adelaide" ~ "South Australia",
        region == "Hobart" ~ "Tasmania",
        region == "Canberra" ~ "Australian Capital Territory",
        region == "Darwin" ~ "Northern Territory",
        TRUE ~ region
      )
    ) %>%
    rename(state = region)
  
  data
  
}

# try a bunch of previous days to find the most recent citymapper dataset
citymapper_url <- function(min_delay = 0, max_delay = 14) {
  date <- Sys.Date()
  delay <- min_delay
  while(delay < max_delay) {
    # try various dates to find the citymapper URL
    datestring <- format(date - delay, format = "%Y%m%d")
    
    url <- paste0(
      "https://cdn.citymapper.com/data/cmi/Citymapper_Mobility_Index_",
      datestring,
      ".csv"
    )
    
    # return if successful, or increment
    if (url_exists(url)) {
      return(url)
    } else {
      delay <- delay + 1
    }
    
  }
  
  # error if we hit the timeout
  if (delay == max_delay) {
    stop ("could not find a valid citymapper URL")
  }
  
}

# load the Citymapper index (urban direction requests, mostly public transport)
# for a couple of cities
citymapper_mobility <- function() {
  
  data <- citymapper_url() %>%
    readr::read_csv(
      skip = 3,
      col_types = cols(
        .default = col_double(),
        Date = col_date(format = "")
      )
    ) %>%
    tidyr::pivot_longer(
      cols = -Date,
      names_to = "region",
      values_to = "trend"
    ) %>%
    filter(region %in% ideal_regions()) %>%
    filter(!is.na(trend)) %>%
    rename(date = Date) %>%
    mutate(
      trend = 100 * (trend - 1),
      state = case_when(
        region == "Sydney" ~ "New South Wales",
        region == "Melbourne" ~ "Victoria",
        region == "Brisbane" ~ "Queensland",
        region == "Perth" ~ "Western Australia",
        TRUE ~ as.character(NA)
      )
    )
  
}

# combine all mobility datasets
all_mobility <- function() {
  
  # load datasets and label their datastreams separately
  google <- google_mobility() %>%
    mutate(
      datastream = str_c("Google: time at ", category)
    ) %>%
    dplyr::select(-category)
  
  apple <- apple_mobility() %>%
    mutate(
      datastream = str_c("Apple: directions for ", transportation_type)
    ) %>%
    dplyr::select(
      -geo_type,
      -transportation_type
    )
  
  # facebook <- facebook_mobility() %>%
  #   mutate(
  #     datastream = str_c("Facebook: ", metric)
  #   ) %>%
  #   dplyr::select(
  #     -metric
  #   )
  
  citymapper <- citymapper_mobility() %>%
    mutate(
      datastream = str_c("Citymapper: directions")
    )
  
  # combine the datasets
  bind_rows(
    google,
    apple,
    # facebook,
    citymapper
  )
  
}

# Google dropped a bunch of previous data from the latest file. Pull a cached
# version from the tidycovid package on GitHub and replace it.
tidycovid_url <- "https://github.com/goldingn/tidycovid19/raw/e4db3ab3007576f34dcb1e8c3299b235cff6198e/cached_data/google_cmr.RDS"

# append any missing google mobility data to mobility data (because Google
# dropped a bunch of data out this one time)
append_google_data <- function(mobility_data, url = tidycovid_url) {
  
  mobility_data <- mobility_data %>%
    filter(!is.na(trend)) %>%
    filter(!is.na(state)) %>%
    select(state, date, trend, datastream)
  
  
  file <- "data/google_cmr/tidycovid_cache.RDS"
  if (!file.exists(file)) {
    dowload_tidycovid()
  }
  tmp <- readRDS(file)

  
  # get the previous data in the same format
  previous_data <- tmp$country_region %>%
    filter(iso3c == "AUS") %>%
    rename(
      retail_and_recreation = retail_recreation,
      grocery_and_pharmacy = grocery_pharmacy,
      state = region
    ) %>%
    pivot_longer(
      cols = c(
        "retail_and_recreation",
        "grocery_and_pharmacy",
        "parks",
        "transit_stations",
        "workplaces",
        "residential"
      ),
      values_to = "trend",
      names_to = "category"
    ) %>%
    mutate(
      category = str_replace_all(category, "_", " ")
    ) %>%
    mutate(
      datastream = str_c("Google: time at ", category)
    ) %>%
    select(
      state,
      date,
      trend,
      datastream
    )
  
  # find any observations that are missing in the mobility data
  missing_data <- mobility_data %>%
    anti_join(previous_data, ., by = c("state", "date", "datastream"))
  
  # add them and return
  mobility_data %>%
    anti_join(missing_data, by = c("state", "date", "datastream")) %>%
    bind_rows(missing_data) %>%
    arrange(datastream, state, date)
  
}


abbreviate_states <- function(state_names) {
  case_when(
    state_names %in% c("Australian Capital Territory", "ACT") ~ "ACT",
    state_names %in% c("New South Wales", "NSW") ~ "NSW",
    state_names %in% c("Northern Territory", "NT") ~ "NT",
    state_names %in% c("Queensland", "QLD") ~ "QLD",
    state_names %in% c("South Australia", "SA") ~ "SA",
    state_names %in% c("Tasmania", "TAS") ~ "TAS",
    state_names %in% c("Victoria", "VIC") ~ "VIC",
    state_names %in% c("Western Australia", "WA") ~ "WA"
  )
}

unabbreviate_states <- function(state_names) {
  case_when(
    state_names %in% c("Australian Capital Territory", "ACT") ~ "Australian Capital Territory",
    state_names %in% c("New South Wales", "NSW") ~ "New South Wales",
    state_names %in% c("Northern Territory", "NT") ~ "Northern Territory",
    state_names %in% c("Queensland", "QLD") ~ "Queensland",
    state_names %in% c("South Australia", "SA") ~ "South Australia",
    state_names %in% c("Tasmania", "TAS") ~ "Tasmania",
    state_names %in% c("Victoria", "VIC") ~ "Victoria",
    state_names %in% c("Western Australia", "WA") ~ "Western Australia"
  )
}

# a list of the regions we'se ideally be interested in. Apple only provides
# data for a handful of these (Australia and the four biggest cities)
ideal_regions <- function() {
  c(
    "Australia",
    "New South Wales",
    "Victoria",
    "Queensland",
    "Western Australia",
    "South Australia",
    "Australian Capital Territory",
    "Tasmania",
    "Northern Territory",
    "Sydney",
    "Melbourne",
    "Brisbane",
    "Perth",
    "Adelaide",
    "Canberra",
    "Hobart",
    "Darwin"
  )
}

interventions <- function(
  which = c(
    "all",
    "national",
    "act",
    "nsw",
    "nt",
    "qld",
    "sa",
    "tas",
    "vic",
    "wa"
  ),
  end_dates = FALSE,
  exclude_after = NA
) {
  
  which <- match.arg(which)
  
  act_interventions <- tibble::tribble(
    ~date, ~state,
    
  )
  
  nsw_interventions <- tibble::tribble(
    ~date, ~state,
    "2021-06-25", "NSW", # stay-at-home order for 4 LGAs from 11.59 PM 24th, extended to all greater sydney +++ from 11.59 PM 25th. 
    "2021-07-18", "NSW" # increased restrictions from midnight 17th https://www.nsw.gov.au/media-releases/restrictions-to-further-limit-spread-of-covid-19-delta-strain
  )
  
  nt_interventions <- tibble::tribble(
    ~date, ~state,
    "2021-06-27", "NT" # https://coronavirus.nt.gov.au/updates/items/2021-06-27-covid-19-update-lockdown-restrictions-in-place
  )
  
  qld_interventions <- tibble::tribble(
    ~date, ~state,
    "2021-01-09", "QLD",
    "2021-03-29", "QLD",
    "2021-06-29", "QLD" # starts 6 PM on 29th https://www.qld.gov.au/health/conditions/health-alerts/coronavirus-covid-19/current-status/public-health-directions/restrictions-in-qld-update
  )
  
  sa_interventions <- tibble::tribble(
    ~date, ~state,
    "2020-11-19", "SA",
    "2021-07-20", "SA" # lockdown, starts 6pm on the 20th, https://www.sahealth.sa.gov.au/wps/wcm/connect/public+content/sa+health+internet/about+us/news+and+media/all+media+releases/covid-19+update+20+july+2021
  )
  
  tas_interventions <- tibble::tribble(
    ~date, ~state,
    
  )
  
  vic_interventions <- tibble::tribble(
    ~date, ~state,
    "2020-07-01", "VIC",
    "2020-07-08", "VIC",
    "2020-08-02", "VIC",
    "2021-02-13", "VIC",
    "2021-05-28", "VIC",
    "2021-07-16", "VIC" # lockdown, 5 days from 11:59 the 15th, then extended https://www.dhhs.vic.gov.au/coronavirus-update-victoria-15-july-2021
  )
  
  wa_interventions <- tibble::tribble(
    ~date, ~state,
    "2021-01-31", "WA",
    "2021-04-24", "WA",
    "2021-06-29", "WA" # https://www.wa.gov.au/government/announcements/4-day-lockdown-introduced-perth-and-peel
  )
  
  
  national_interventions <- expand_grid(
    date = c("2020-03-16", "2020-03-24", "2020-03-29"),
    state = c("ACT", "NSW", "NT", "QLD", "SA", "TAS", "VIC", "WA")
  )
  
  
  if(end_dates){
    
    # act_interventions <-  act_interventions %>%
    #   bind_rows(
    #     tibble::tribble(
    #       ~date, ~state,
    #   
    #     )
    #   )
    
    # nsw_interventions <-  nsw_interventions %>%
    #   bind_rows(
    #     tibble::tribble(
    #       ~date, ~state,
    #       #"2021-07-17", "NSW" # https://www.abc.net.au/news/2021-07-07/covid-live-updates-coronavirus-press-conference-sydney-lockdown/100270832?utm_campaign=abc_news_web&utm_content=link&utm_medium=content_shared&utm_source=abc_news_web#live-blog-post-1201975138
    #     )
    #   )
    
    nt_interventions <-  nt_interventions %>%
      bind_rows(
        tibble::tribble(
          ~date, ~state,
          "2021-07-02", "NT" # https://coronavirus.nt.gov.au/updates/items/2021-06-28-covid-19-update-nt
        )
      )
    
    qld_interventions <-  qld_interventions %>%
      bind_rows(
        tibble::tribble(
          ~date, ~state,
          "2021-01-12", "QLD",
          "2021-04-01", "QLD",
          "2021-07-04", "QLD" # Lifted 6 PM 3rd July https://www.qld.gov.au/health/conditions/health-alerts/coronavirus-covid-19/current-status/public-health-directions/restrictions-impacted-areas
        )
      )
    
    sa_interventions <-  sa_interventions %>%
      bind_rows(
        tibble::tribble(
          ~date, ~state,
          "2020-11-22", "SA"
        )
      )
    
    # tas_interventions <-  tas_interventions %>%
    #   bind_rows(
    #     tibble::tribble(
    #       ~date, ~state,
    #       
    #     )
    #   )
    
    vic_interventions <-  vic_interventions %>%
      bind_rows(
        tibble::tribble(
          ~date, ~state,
          "2021-02-18", "VIC",
          "2021-06-11", "VIC"
        )
      )
    
    wa_interventions <-  wa_interventions %>%
      bind_rows(
        tibble::tribble(
          ~date, ~state,
          "2021-02-05", "WA",
          "2021-04-27", "WA",
          "2021-07-03", "WA" # https://www.wa.gov.au/government/announcements/end-of-lockdown-perth-and-peel-1201am-saturday-3-july
        )
      )
  }
  
  interventions <- switch(
    which,
    national = national_interventions %>%
      filter(state == "ACT") %>%
      mutate(state = "all"),
    act = act_interventions,
    nsw = nsw_interventions,
    nt  = nt_interventions,
    qld = qld_interventions,
    sa  = sa_interventions,
    tas = tas_interventions,
    vic = vic_interventions,
    wa =  wa_interventions,
    all = bind_rows(
      national_interventions,
      act_interventions,
      nsw_interventions,
      nt_interventions,
      qld_interventions,
      sa_interventions,
      tas_interventions,
      vic_interventions,
      wa_interventions,
    )
  ) %>%
    mutate(
      date = as.Date(date),
      state = factor(state)
    ) %>% 
    arrange(state, date)
  
  if(!is.na(exclude_after)){
    
    interventions <- interventions %>%
      filter(date <= as.Date(exclude_after))
  }
  
  return(interventions)
}

quarantine_dates <- function() {
  expand_grid(
    date = c("2020-03-15", "2020-03-28"),
    state = c("ACT", "NSW", "NT", "QLD", "SA", "TAS", 
      "VIC", "WA")
  ) %>%
    mutate(
      date = as.Date(date),
      state = factor(state)
    )
}

# dates of school holidays by state, from:
# 2020: https://info.australia.gov.au/about-australia/special-dates-and-events/school-term-dates
# 2021 onwards:
# https://www.education.act.gov.au/__data/assets/pdf_file/0007/801583/Term-Date-2019-2025.pdf
# https://education.nsw.gov.au/public-schools/going-to-a-public-school/calendars
# https://nt.gov.au/learning/primary-and-secondary-students/school-term-dates-in-nt
# https://education.qld.gov.au/about-us/calendar/term-dates
# https://www.education.sa.gov.au/teaching/south-australian-state-schools-term-dates
# https://www.education.tas.gov.au/about-us/term-dates/term-dates-2021/
# https://www.education.vic.gov.au/about/department/Pages/datesterm.aspx
# https://www.education.wa.edu.au/future-term-dates/
school_holiday_dates <- function() {
  dplyr::bind_rows(
    tibble::tribble(~school_holiday, ~start, ~end,
                    1, "2020-04-10", "2020-04-26",
                    2, "2020-07-04", "2020-07-19",
                    3, "2020-09-26", "2020-10-11",
                    4, "2020-12-19", "2021-01-28",
                    5, "2021-04-02", "2021-04-18",
                    6, "2021-06-26", "2021-07-11",
                    7, "2021-09-18", "2021-10-04",
                    8, "2021-12-18", "2022-01-27"
    ) %>%
      mutate(
        state = "Australian Capital Territory"
      ),
    tibble::tribble(~school_holiday, ~start, ~end,
                    1, "2020-04-10", "2020-04-26",
                    2, "2020-07-04", "2020-07-19",
                    3, "2020-09-26", "2020-10-11",
                    4, "2020-12-19", "2021-01-26",
                    5, "2021-04-05", "2021-04-16",
                    6, "2021-06-28", "2021-07-09",
                    7, "2021-09-20", "2021-10-01",
                    8, "2021-12-20", "2022-01-27"
    ) %>%
      mutate(
        state = "New South Wales"
      ),
    tibble::tribble(~school_holiday, ~start, ~end,
                    1, "2020-04-10", "2020-04-19",
                    2, "2020-06-27", "2020-07-20",
                    3, "2020-09-26", "2020-10-11",
                    4, "2020-12-18", "2021-01-31",
                    5, "2021-04-10", "2021-04-18",
                    6, "2021-06-26", "2021-07-19",
                    7, "2021-09-25", "2021-10-10",
                    8, "2021-12-17", "2022-01-30"
    ) %>%
      mutate(
        state = "Northern Territory"
      ),
    tibble::tribble(~school_holiday, ~start, ~end,
                    1, "2020-04-04", "2020-04-19",
                    2, "2020-06-27", "2020-07-12",
                    3, "2020-09-19", "2020-10-05",
                    4, "2020-12-12", "2021-01-26",
                    5, "2021-04-02", "2021-04-18",
                    6, "2021-06-26", "2021-07-11",
                    7, "2021-09-18", "2021-10-05",
                    8, "2021-12-11", "2022-01-26"
    ) %>%
      mutate(
        state = "Queensland"
      ),
    tibble::tribble(~school_holiday, ~start, ~end,
                    1, "2020-04-10", "2020-04-26",
                    2, "2020-07-04", "2020-07-19",
                    3, "2020-09-26", "2020-10-11",
                    4, "2020-12-12", "2021-01-26",
                    5, "2021-04-10", "2021-04-26",
                    6, "2021-07-03", "2021-07-18",
                    7, "2021-09-25", "2021-10-10",
                    8, "2021-12-11", "2022-01-30"
    ) %>%
      mutate(
        state = "South Australia"
      ),
    tibble::tribble(~school_holiday, ~start, ~end,
                    1, "2020-04-10", "2020-04-26",
                    2, "2020-07-04", "2020-07-19",
                    3, "2020-09-26", "2020-10-11",
                    4, "2020-12-18", "2021-02-02",
                    5, "2021-04-10", "2021-04-25",
                    6, "2021-07-03", "2021-07-19",
                    7, "2021-09-25", "2021-10-10",
                    8, "2021-12-17", "2022-02-02"
    ) %>%
      mutate(
        state = "Tasmania"
      ),
    tibble::tribble(~school_holiday, ~start, ~end,
                    # Vic extended each school holiday by a week during the
                    # pandemic
                    # https://www.education.vic.gov.au/about/department/Pages/datesterm.aspx
                    1, "2020-03-25", "2020-04-13",
                    2, "2020-06-27", "2020-07-19",
                    3, "2020-09-19", "2020-10-04",
                    4, "2020-12-19", "2021-01-26",
                    5, "2021-04-02", "2021-04-18",
                    6, "2021-06-26", "2021-07-11",
                    7, "2021-09-18", "2021-10-03",
                    8, "2021-12-18", "2022-01-27"
    ) %>%
      mutate(
        state = "Victoria"
      ),
    tibble::tribble(~school_holiday, ~start, ~end,
                    1, "2020-04-10", "2020-04-27",
                    2, "2020-07-04", "2020-07-19",
                    3, "2020-09-26", "2020-10-11",
                    4, "2020-12-18", "2021-01-31",
                    5, "2021-04-02", "2021-04-18",
                    6, "2021-07-03", "2021-07-18",
                    7, "2021-09-25", "2021-10-10",
                    8, "2021-12-17", "2022-01-30"
    ) %>%
      mutate(
        state = "Western Australia"
      )
  ) %>%
    mutate(
      start = lubridate::date(start),
      end = lubridate::date(end)
    ) %>%
    # loop through, expanding out into dates within term time
    mutate(id = row_number()) %>%
    group_by(id) %>%
    do(
      tibble(
        state = .$state,
        school_holiday = .$school_holiday, 
        date = seq(from = .$start, to =.$end, by = 1)
      )
    ) %>%
    ungroup() %>%
    select(-id)
}

download_holiday_dates <- function(destination) {
  
  # create the directory if needed
  directory <- dirname(destination)
  if (!file.exists(directory)) {
    dir.create(directory, recursive = TRUE)
  }
  
  list(
    "2020" = "https://data.gov.au/data/dataset/b1bc6077-dadd-4f61-9f8c-002ab2cdff10/resource/c4163dc4-4f5a-4cae-b787-43ef0fcf8d8b/download/australian_public_holidays_2020.csv",
    "2021" = "https://data.gov.au/data/dataset/b1bc6077-dadd-4f61-9f8c-002ab2cdff10/resource/2dee10ef-2d0c-44a0-a66b-eb8ce59d9110/download/australian_public_holidays_2021.csv",
    "2022" = "https://data.gov.au/data/dataset/b1bc6077-dadd-4f61-9f8c-002ab2cdff10/resource/d256f989-8f49-46eb-9770-1c6ee9bd2661/download/australian_public_holidays_2022.csv"
  ) %>%
    lapply(
      read_csv,
      col_types = 
        cols(
          Date = col_date(format = "%Y%m%d"),
          `Holiday Name` = col_character(),
          Information = col_character(),
          `More Information` = col_character(),
          Jurisdiction = col_character()
        )
    ) %>%
    do.call(
      bind_rows, .
    ) %>%
    mutate(
      state = toupper(Jurisdiction),
      state = unabbreviate_states(state),
      date = Date,
      name = `Holiday Name`
    ) %>%
    select(state, date, name) %>%
    write_csv(destination)
}
    
# dates of public holidays by state, from:
# https://data.gov.au/dataset/ds-dga-b1bc6077-dadd-4f61-9f8c-002ab2cdff10/details?q=
holiday_dates <- function(holiday_file = "data/holidays/public_holidays.csv") {
  
  if (!file.exists(holiday_file)) {
    download_holiday_dates(holiday_file)
  }
  
  read_csv(
    holiday_file,
    col_types = cols(
      state = col_character(),
      date = col_date(format = ""),
      name = col_character()
    )
  )

}

lockdown_lgas <- function() {
  c(
    "Banyule (C)",
    "Bayside (C)",
    "Boroondara (C)",
    "Brimbank (C)", 
    "Cardinia (S)",
    "Casey (C)",
    "Darebin (C)",
    "Frankston (C)", 
    "Glen Eira (C)",
    "Greater Dandenong (C)",
    "Hobsons Bay (C)", 
    "Hume (C)",
    "Kingston (C) (Vic.)",
    "Knox (C)",
    "Manningham (C)", 
    "Maribyrnong (C)",
    "Maroondah (C)",
    "Melbourne (C)",
    "Melton (C)", 
    "Mitchell (S)",
    "Monash (C)",
    "Moonee Valley (C)",
    "Moreland (C)", 
    "Mornington Peninsula (S)",
    "Nillumbik (S)",
    "Port Phillip (C)", 
    "Stonnington (C)",
    "Whitehorse (C)",
    "Whittlesea (C)",
    "Wyndham (C)", 
    "Yarra (C)",
    "Yarra Ranges (S)"
  )
}

nsw_lga_active_cases <- function() {
  # need to copy from here: https://www.health.nsw.gov.au/Infectious/covid-19/Pages/stats-local.aspx
  # paste in using datapasta addin
  # and remove the annoying asterisk and parentheses in title
  tibble::tribble(
    ~Local.Government.Area,  ~Cases, ~Cases.with.unknown.source, ~Total.tests,   ~Test.rate.per.1000,
    "Albury",       2,                          0,         3068,                    56,
    "Armidale Regional",       0,                          0,         1109,                    36,
    "Ballina",       0,                          0,         1986,                    45,
    "Balranald",       0,                          0,           87,                    37,
    "Bathurst Regional",       0,                          0,         2228,                    51,
    "Bayside",       2,                          0,         7368,                    41,
    "Bega Valley",       0,                          0,         2267,                    66,
    "Bellingen",       0,                          0,          484,                    37,
    "Berrigan",       0,                          0,          376,                    43,
    "Blacktown",       8,                          1,        22713,                    61,
    "Bland",       0,                          0,          269,                    45,
    "Blayney",       0,                          0,          309,                    42,
    "Blue Mountains",       5,                          0,         5687,                    72,
    "Bogan",       0,                          0,          102,                    40,
    "Bourke",       0,                          0,           92,                    36,
    "Brewarrina",       0,                          0,           56,                    35,
    "Broken Hill",       0,                          0,          611,                    35,
    "Burwood",       0,                          0,         1434,                    35,
    "Byron",       2,                          0,         1841,                    52,
    "Cabonne",       0,                          0,          342,                    25,
    "Camden",       2,                          1,        12092,                   119,
    "Campbelltown",       9,                          1,        16784,                    98,
    "Canada Bay",       0,                          0,         5867,                    61,
    "Canterbury-Bankstown",      14,                          1,        20189,                    53,
    "Carrathool",       0,                          0,           43,                    15,
    "Central Coast",       0,                          0,        16536,                    48,
    "Central Darling",       0,                          0,           64,                    35,
    "Cessnock",       0,                          0,         2158,                    36,
    "Clarence Valley",       0,                          0,         1448,                    28,
    "Cobar",       0,                          0,          176,                    38,
    "Coffs Harbour",       0,                          0,         2785,                    36,
    "Coolamon",       0,                          0,          188,                    43,
    "Coonamble",       0,                          0,          120,                    30,
    "Cootamundra-Gundagai Regional",       0,                          0,          468,                    42,
    "Cowra",       0,                          0,          450,                    35,
    "Cumberland",      13,                          0,        13060,                    54,
    "Dubbo Regional",       0,                          0,         2418,                    45,
    "Dungog",       0,                          0,          342,                    36,
    "Edward River",       0,                          0,          451,                    50,
    "Eurobodalla",       3,                          0,         4314,                   112,
    "Fairfield",      25,                          0,        16440,                    78,
    "Federation",       0,                          0,          490,                    39,
    "Forbes",       0,                          0,          308,                    31,
    "Georges River",       4,                          0,         7083,                    44,
    "Gilgandra",       0,                          0,          170,                    40,
    "Glen Innes Severn",       0,                          0,          287,                    32,
    "Goulburn Mulwaree",       0,                          0,         1419,                    46,
    "Greater Hume Shire",       0,                          0,          584,                    54,
    "Griffith",       0,                          0,         1244,                    46,
    "Gunnedah",       0,                          0,          435,                    34,
    "Gwydir",       0,                          0,           98,                    18,
    "Hawkesbury",       0,                          0,         4158,                    62,
    "Hay",       0,                          0,           69,                    23,
    "Hilltops",       0,                          0,          731,                    39,
    "Hornsby",       0,                          0,         6419,                    42,
    "Hunters Hill",       0,                          0,         1620,                   108,
    "Inner West",       0,                          0,        16140,                    80,
    "Inverell",       0,                          0,          618,                    37,
    "Junee",       0,                          0,          200,                    30,
    "Kempsey",       0,                          0,         1114,                    37,
    "Kiama",       0,                          0,         1439,                    62,
    "Ku-ring-gai",       0,                          0,         8177,                    64,
    "Kyogle",       0,                          0,          291,                    33,
    "Lachlan",       0,                          0,          170,                    28,
    "Lake Macquarie",       2,                          0,        10389,                    50,
    "Lane Cove",       0,                          0,         4544,                   113,
    "Leeton",       0,                          0,          311,                    27,
    "Lismore",       0,                          0,         2008,                    46,
    "Lithgow",       0,                          0,          854,                    40,
    "Liverpool",      32,                          2,        22937,                   101,
    "Liverpool Plains",       0,                          0,          259,                    33,
    "Lockhart",       0,                          0,          109,                    33,
    "Maitland",       0,                          0,         5846,                    69,
    "Mid-Coast",       0,                          0,         2956,                    32,
    "Mid-Western Regional",       0,                          0,         1150,                    46,
    "Moree Plains",       0,                          0,          337,                    25,
    "Mosman",       0,                          0,         1646,                    53,
    "Murray River",       0,                          0,          185,                    15,
    "Murrumbidgee",       0,                          0,          146,                    37,
    "Muswellbrook",       0,                          0,          750,                    46,
    "Nambucca",       0,                          0,          618,                    31,
    "Narrabri",       0,                          0,          379,                    29,
    "Narrandera",       0,                          0,          171,                    29,
    "Narromine",       0,                          0,          242,                    37,
    "Newcastle",       0,                          0,        11168,                    67,
    "North Sydney",       0,                          0,         3434,                    46,
    "Northern Beaches",       0,                          0,        13685,                    50,
    "Oberon",       0,                          0,          175,                    32,
    "Orange",       0,                          0,         2002,                    47,
    "Parkes",       0,                          0,          423,                    29,
    "Parramatta",      11,                          0,        11404,                    44,
    "Penrith",       2,                          1,        15811,                    74,
    "Port Macquarie-Hastings",       0,                          0,         3466,                    41,
    "Port Stephens",       3,                          0,         6845,                    93,
    "Queanbeyan-Palerang Regional",       0,                          0,         1942,                    32,
    "Randwick",       0,                          0,         9269,                    60,
    "Richmond Valley",       0,                          0,         1016,                    43,
    "Ryde",       0,                          0,         6019,                    46,
    "Shellharbour",       1,                          0,         4538,                    62,
    "Shoalhaven",       0,                          0,         4764,                    45,
    "Singleton",       0,                          0,         1517,                    65,
    "Snowy Monaro Regional",       0,                          0,         1033,                    50,
    "Snowy Valleys",       0,                          0,          642,                    44,
    "Strathfield",       0,                          0,         2665,                    57,
    "Sutherland Shire",       5,                          0,        15905,                    69,
    "Sydney",       3,                          1,        15135,                    61,
    "Tamworth Regional",       0,                          0,         2413,                    39,
    "Temora",       0,                          0,          178,                    28,
    "Tenterfield",       0,                          0,          144,                    22,
    "The Hills Shire",       6,                          0,        11766,                    66,
    "Tweed",       0,                          0,         3217,                    33,
    "Unincorporated NSW",       0,                          0,            0,                     0,
    "Upper Hunter Shire",       0,                          0,          604,                    43,
    "Upper Lachlan Shire",       0,                          0,          309,                    38,
    "Uralla",       0,                          0,          146,                    24,
    "Wagga Wagga",       0,                          0,         3385,                    52,
    "Walcha",       0,                          0,           82,                    26,
    "Walgett",       0,                          0,          305,                    51,
    "Warren",       0,                          0,          172,                    64,
    "Warrumbungle Shire",       0,                          0,          439,                    47,
    "Waverley",       1,                          1,         4832,                    65,
    "Weddin",       0,                          0,          172,                    48,
    "Wentworth",       0,                          0,          352,                    50,
    "Willoughby",       0,                          0,         3255,                    40,
    "Wingecarribee",       0,                          0,         4698,                    92,
    "Wollondilly",       5,                          0,         4349,                    82,
    "Wollongong",       4,                          1,        11981,                    55,
    "Woollahra",       0,                          0,         4161,                    70,
    "Yass Valley",       0,                          0,          466,                    27
  )
  
}


# define a latent factor for state-switching behaviour. kappa are lengths of the
# tails for early- and late-adopters; lambda are the relative contribution of
# the triggers; tau are dates of triggers
latent_behaviour_switch <- function(date_num,
                                    tau,
                                    kappa = normal(2, 1, truncation = c(0, Inf), dim = length(tau)),
                                    lambda = uniform(0, 1, dim = length(tau))) {
  
  if (length(tau) == 1) {
    lag <- date_num - tau
    result <- ilogit(lag / kappa)
  } else {
    lambda <- lambda / sum(lambda)
    lags <- outer(date_num, tau, FUN = "-")
    lag_delays <- ilogit(sweep(lags, 2, kappa, FUN = "/"))
    result <- lag_delays %*% lambda
  }
  result
  
}

# define a latent factor for a single symmetric distribution of behavioural events
latent_behavioural_event <- function(date_num, tau, kappa = normal(3, 1, truncation = c(0, Inf)), lambda = 1) {
  # work with logs for numerical stability
  # e <- exp(-lag / kappa)
  # 4 * lambda * e / (1 + e) ^ 2
  lag <- date_num - tau
  le <- -lag / kappa
  log_bump <- log(4 * lambda) + le - 2 * log1pe(le)
  exp(log_bump)
}

# a b-spline constrained to 0-1 for day of the week, with Sunday (day 1)
# fixed at 1 (for identifiability)
latent_spline <- function (days = 1:7, knots = 4) {
  bases <- splines::bs(days, df = knots)
  # for identifiability, make one weight positive. That way the rest can define
  # their sign relative to this, and the loading defines the overall sign
  weights <- normal(0, 1, truncation = c(-Inf, 0), dim = knots)#, 1, dim = knots, truncation = c(-1, 1))
  spline <- bases %*% weights
  spline <- spline - min(spline)
  spline <- spline / max(spline)
  spline
  
}

# a hinge effect, 0 before the inflection point, then linear to 1 at the most recent period
latent_hinge <- function(inflection_dates, date_num) {
  last_date_num <- max(date_num)
  effect_periods <- last_date_num - inflection_dates
  time_remaining <- date_num - last_date_num
  distancing_effects <- 1 + kronecker(time_remaining, effect_periods, FUN = "/")
  nullify <- (sign(distancing_effects) + 1) / 2
  distancing_effects * nullify
}

state_populations <- function() {
  tibble::tribble(
    ~state, ~population,
    "Australian Capital Territory", 426709,
    "New South Wales", 8089526,
    "Northern Territory", 245869,
    "Queensland", 5095100,
    "South Australia", 1751693,	
    "Tasmania", 534281,
    "Victoria", 6594804,
    "Western Australia", 2621680
  )
}

posterior_sims <- function(vector, draws, nsim = NULL) {
  if (is.null(nsim)) {
    nsim <- coda::niter(draws) * coda::nchain(draws)
  }
  calculate(vector, values = draws, nsim = nsim)[[1]][, , 1]
}

# summarise the posterior for a vector greta array
summarise_vec_posterior <- function(vector,
                                    draws,
                                    quantiles = c(0.025, 0.975),
                                    nsim = 1000) {
  vector_sim <- posterior_sims(vector, draws, nsim = nsim)
  posterior_mean <- colMeans(vector_sim)
  posterior_ci <- t(apply(vector_sim, 2, quantile, quantiles))
  cbind(mean = posterior_mean, posterior_ci)
}

# add a polygon for a credible interval to a base plot
add_mean_ci <- function(posterior_summary,
                        dates,
                        col = grey(0.8),
                        border_col = grey(0.6),
                        line_col = grey(0.4),
                        lwd = 3) {
  polygon(x = c(dates, rev(dates)),
          y = c(posterior_summary[, 2],
                rev(posterior_summary[, 3])),
          lwd = 0.5,
          col = col,
          border = border_col)
  lines(posterior_summary[, 1] ~ dates,
        lwd = lwd,
        col = line_col)
}

add_gridlines <- function(key_dates, vertical = TRUE, horizontal = TRUE) {
  if (horizontal) {
    abline(h = 0, col = grey(0.4), lty = 3)
  }
  if (vertical) {
    abline(v = key_dates, col = grey(0.6), lwd = 2)
  }
}

# colours for plotting
pal <- function(colour = "green") {
  
  # look up the base colour
  brewer_cols <- brewer.pal(8, "Set2")
  # display.brewer.pal(8, "Set2")
  colour_names <- c(
    "green",
    "red",
    "blue",
    "pink",
    "bright green",
    "yellow",
    "beige",
    "grey"
  )
  idx <- match(colour, colour_names)
  base_colour <- brewer_cols[idx]
  
  # create a four-colour palette based on this colour
  pal <- colorRampPalette(c("white", base_colour, "black"))(10)
  pal[c(4, 5, 6, 7)]
  
}

plot_latent_factor <- function (factor, draws, dates, key_dates,
                                cols = grey(c(0.9, 0.7, 0.5, 0.3)), title = "") {
  
  est <- summarise_vec_posterior(factor, draws)
  plot(est[, 1] ~ dates,
       type = "n",
       ylim = c(0, 1),
       ylab = "relative effect",
       xlab = "",
       las = 1)
  add_gridlines(key_dates, horizontal = FALSE)
  add_mean_ci(est,
              dates,
              col = cols[1],
              border_col = cols[2],
              line_col = cols[3])
  title(main = title,
        col.main = grey(0.3))
}

gamma_cdf <- function(x, shape, rate) {
  stop("not yet implemented")
}

# greta function for the lognormal CDF (equivalent to plnorm(x, meanlog, sdlog))
# x must not be a greta array, though meanlog and sdlog can be. A greta array of
# CDF values is returned, equal to 0 where x was 0 or lower.
# lognormal_cdf <- function(x, meanlog, sdlog) {
#   
#   # filter out any invalid dates, to replace their CDF with 0
#   valid <- x > 0
#   x <- x[valid]
#   
#   p <- iprobit((log(x) - meanlog) / sdlog)
#   
#   # if any were missing, replace their cumulative density with 0
#   if (any(!valid)) {
#     result <- zeros(length(valid))
#     result[valid] <- p
#     p <- result
#   }
#   
#   p
#   
# }

lognormal_cdf <- function(x, meanlog, sdlog) {
  
  op("lognormal_cdf",
     x,
     meanlog,
     sdlog,
     tf_operation = "tf_lognormal_cdf",
     dim = dim(x))
  
}

# tensorflow function for the CDF of a lognormal distribution
tf_lognormal_cdf <- function(x, meanlog, sdlog) {
  
  d <- tfp$distributions$LogNormal(meanlog, sdlog)
  
  # This does not support zero or negative values (returns NA instead of 0), so
  # we need to avoid those. We also can't compute the CDF on negative values
  # then replace the bad values, since that breaks the gradients
  supported <- tf$greater(x, greta:::fl(0))
  ones <- tf$ones_like(x)
  zeros <- tf$zeros_like(x)
  
  x_clean <- tf$where(supported, x, ones)
  cdf_clean <- d$cdf(x_clean)
  mask <- tf$where(supported, ones, zeros)
  cdf_clean * mask
  
}

# probability mass function of a negative binomial distribution
negative_binomial_pmf <- function(x, size, prob) {
  
  op("negative_binomial_pmf",
     x,
     size,
     prob,
     tf_operation = "tf_negative_binomial_pmf",
     dim = dim(x))
  
}

tf_negative_binomial_pmf <- function(x, size, prob) {
  
  
  d <- tfp$distributions$NegativeBinomial(
    total_count = size,
    probs = greta:::fl(1) - prob)
  
  d$prob(x)
  
}

exponential_cdf <- function (x, rate) {
  
  # filter out any invalid dates, to replace their CDF with 0
  valid <- x > 0
  x <- x[valid]
  
  p <- 1 - exp(-rate * x)
  
  # if any were missing, replace their cumulative density with 0
  if (any(!valid)) {
    result <- zeros(length(valid))
    result[valid] <- p
    p <- result
  }
  
  p
  
}

# Given samples 'x' of a parameter, approximate the distribution by a (possibly
# truncated) normal distribution, and return a variable greta array following
# that distribution
parameter <- function(x, truncation = c(-Inf, Inf)) {
  normal(mean(x), sd(x), truncation = truncation)
}

nishiura_samples <- function () {
  
  # priors for the parameters of the lognormal distribution over the serial interval from Nishiura et
  # al., as stored in the EpiNow source code 
  read_csv(
    file = "data/parameters/nishiura_samples.csv",
    col_types = cols(param1 = col_double(),
                     param2 = col_double())
  )
  
}

# Pull out generation interval distribution estimates from Ganyani et al. Use
# baseline GI estimates for China from (Table 1, row 3). This corresponds to a
# GI/SI mean of 3.95, which is around the 4 that seems to be a consensus for the
# SI mean. We do not use the version allowing for a negative serial interval
# (which is suggested by other including Du et al., and could possibly be an
# artefact of incorrect alignment of infector/infectee pairs), though when this is
# used as a prior the model is able to estimate parameters corresponding to
# that scenario.
ganyani_gi <- function(which = c("Tianjin", "Singapore")) {
  
  tianjin <- tibble::tribble(
    ~which, ~est, ~lower, ~upper,
    "mean", 3.95, 3.01, 4.91,
    "sd", 1.51, 0.74, 2.97
  )
  
  singapore <- tibble::tribble(
    ~which, ~est, ~lower, ~upper,
    "mean", 5.20, 3.78, 6.78,
    "sd", 1.72, 0.91, 3.93
  )
  
  ganyani <- switch(match.arg(which),
                    Tianjin = tianjin,
                    Singapore = singapore)
  
  ganyani <- ganyani %>%
    mutate(
      sd_above = (upper - est) / 1.96,
      sd_below = (est - lower) / 1.96,
      sd = (sd_above + sd_below) / 2
    ) %>%
    select(which, est, sd)
  
  ganyani %>%
    group_by(which) %>%
    nest() %>%
    pull(data) %>%
    `names<-`(ganyani$which)
  
}

# The two (baseline) generation intervals of Ganyani et al.
ganyani_cdf <- function(which) {
  
  gi_params <- ganyani_gi(which)
  
  beta <- gi_params$mean$est / (gi_params$sd$est ^ 2)
  alpha <- gi_params$mean$est * beta
  
  gi_cdf <- function(days) {
    pgamma(days, alpha, beta)
  }
  
  gi_cdf
  
}

# the pre-intervention serial interval of Taslim Ali et al.
taslim_ali_cdf <- function() {
  
  params <- lognormal_prior(mean = 7.8, sd = 5.2)
  
  gi_cdf <- function(days) {
    plnorm(days, meanlog = params$mean, sdlog = params$sd)
  }
  
  gi_cdf
}

# the serial interval of Nishiura et al.
nishiura_cdf <- function() {
  
  # generation interval distribution; use SI distribution from Nishiura et al.
  nishiura <- nishiura_samples()
  meanlog <- mean(nishiura$param1)
  sdlog <- mean(nishiura$param2)
  
  gi_cdf <- function(days) {
    plnorm(days, meanlog, sdlog)
  }
  
  gi_cdf
  
}

# the serial interval distribution of Bi et al. (used in Impeerial Report 13 /
# Flaxman et al.)
bi_cdf <- function() {
  
  gi_cdf <- function(days) {
    pgamma(days, 2.29, 0.36)
  }
  
  gi_cdf
  
}

# Discretised generation interval with truncated continuous distribution. Given
# a number of days post-infection, compute the probability of the generation
# interval having that length as the density of a truncated distribution (given
# by an R function for its cumulative density function 'cdf') on time
# over the duration of that day.
gi_probability <- function(cdf,
                           days = seq(bounds[1], bounds[2]),
                           bounds = c(0, 20)) {
  
  # days of infectiousness
  n_days <- length(days)
  # set invalid days to -1 (density 0)
  out_of_bounds <- days < bounds[1] | days > bounds[2]
  days[out_of_bounds] <- -1
  
  # get discretised probability, without accounting for truncation  
  p <- cdf(days + 1) - cdf(days)
  
  # adjust density for truncation
  upper_bound <- cdf(bounds[2] + 1)
  lower_bound <- cdf(bounds[1])
  p <- p / (upper_bound - lower_bound)
  
  p
  
}

# build an upper-triangular circulant matrix of the number of days from one set
# of times to another, set to -999 if the difference is not supported (negative
# or exceeds max_days)
time_difference_matrix <- function (n_days) {
  
  mat <- matrix(0, n_days, n_days)
  row(mat) - col(mat)
  
}

# given a positive integer 'n_days' of the number of days for which to compute
# values and a discrete vector 'disaggregation_probs' of probabilities that data
# for one day should actually be assigned that number of days into the future,
# return a symmetric matrix that can be used in a matrix multiply to
# disaggregate daily data into expected counts on future days, according to that
# probability distribution. Doing this disaggregation using a circulant matrix
# of probabilities with masked lower values, is more efficient in greta than
# looping since it can easily be parallelised. Note this is the same operation
# as cases_known_outcome_matrix() in goldingn/australia_covid_ascertainment
disaggregation_matrix <- function (cdf, n_days, max_days = 20, ...) {
  
  diff <- time_difference_matrix(n_days, max_days)
  si_disaggregation <- gi_probability(cdf, days, ...)
  
}

# apply a fixed serial interval a date-by-state matrix 'cases' of case counts to get
# the expected number of infectious people at each time. If 'fixed', use a
# deterministic serial interval distribution base don prior means, othrwise
# treeat the parameters of the distribution as unknown parameters, to account
# for uncrtainty in the distribution.
apply_serial_interval <- function(cdf, cases) {
  
  # get a square matrix of contributions of each date to each other, and
  # matrix-multiply to disaggregate
  si_disaggregation <- disaggregation_matrix(cdf, nrow(cases))
  si_disaggregation %*% cases
  
}

# given the mean and standard deviation of a lognormal distribution, compute the
# parameters (mean and standard deviation of the normal distribution over the
# variable's log)
lognormal_prior <- function(mean, sd) {
  
  var <- sd ^ 2
  list(
    mean = log((mean ^ 2) / sqrt(var + mean ^ 2)),
    sd = sqrt(log(1 + var / (mean ^ 2)))
  )
  
}

R0_prior <- function() {
  # Visually estimated values of the posterior distributions  (means and lower
  # and upper 95% CIs) over R0 in 11 European countries from Flaxman et al.
  # (Imperial report 13). Rather than use the estimates in the main text, we
  # usee those fromm the SI under the shortest serial interval - a mean of 5
  # days, corresponding to a median of 4.49 days. Note, we assumed based on
  # Figure 7 that the SI gamma distribution parameter values given are the mean
  # and the rate parameter of the distribution), since no other combination of
  # mean, sd, shape, scale, or rate could be found to match the figure closely.
  # We use this lower estimate because it is closest to the estimate of Nishiura
  # et al. of a median serial interval of 4 days, or 4.6 days taking only the
  # pairs of cases with greatest confidence. This is also the most conseervative
  # eestimate of R0 and therefore of the impact of social distancing interventions
  prior_estimate <- tibble::tribble(
    ~country, ~lower, ~mean, ~upper,
    "Denmark", 2, 2.6, 3.3,
    "Italy", 2.4, 2.6, 2.8,
    "Germany", 2.5, 3, 3.75,
    "Spain", 2.7, 3.2, 3.6,
    "United Kingdom", 2.4, 2.6, 3.1,
    "France", 2.5, 2.8, 3.2,
    "Norway", 1.7, 2.4, 3.3,
    "Belgium", 2.4, 2.9, 3.7,
    "Austria", 2.3, 2.7, 3.7,
    "Sweden", 2.3, 2.8, 3.6,
    "Switzerland", 2.3, 2.7, 3.4
  ) %>%
    summarise(lower = mean(lower),
              mean = mean(mean),
              upper = mean(upper))
  
  # to find a lognormal prior that matches this, find the parameters of a
  # lognormal distribution that give the minimum mean squared error against
  # these estiamtes
  obj <- function(par) {
    meanlog <- par[1]
    sdlog <- exp(par[2])
    mean <- exp(meanlog + sdlog ^ 2 / 2)
    cis <- qlnorm(c(0.025, 0.975), meanlog, sdlog)
    mean_error <- mean - prior_estimate$mean
    ci_error <- cis - c(prior_estimate$lower, prior_estimate$upper)
    mse <- mean(mean_error ^ 2 + ci_error[1] ^ 2 + ci_error[2] ^ 2)
    mse
  }
  
  par <- optim(par = c(0, 0), obj)$par
  
  list(meanlog = par[1], sdlog = exp(par[2]))
  
}

plot_trend <- function(
  simulations,
  data,
  base_colour = grey(0.4),
  multistate = FALSE,
  hline_at = 1,
  ylim = c(0, 6),
  intervention_at = interventions(),
  projection_at = NA,
  keep_only_rows = NULL,
  max_date = data$dates$latest_mobility,
  min_date = as.Date("2020-03-01"),
  plot_voc = FALSE
) {
  
  if(is.na(min_date)){
    min_date <- max_date - months(6)
  }
  
  mean <- colMeans(simulations)
  ci_90 <- apply(simulations, 2, quantile, c(0.05, 0.95)) 
  ci_50 <- apply(simulations, 2, quantile, c(0.25, 0.75))
  
  if (multistate) {
    states <- rep(data$states, each = data$n_dates_project)
    dates <- rep(data$dates$infection_project, data$n_states)
  } else {
    dates <- data$dates$infection_project
    states <- NA
  }
  
  df <- tibble(date = dates,
               state = states,
               mean = mean,
               ci_50_lo = ci_50[1, ],
               ci_50_hi = ci_50[2, ],
               ci_90_lo = ci_90[1, ],
               ci_90_hi = ci_90[2, ])
  
  if (!is.null(keep_only_rows)) {
    df <- df[keep_only_rows, ]
  }
  
  df <- df %>%
    filter(
      date >= min_date,
      date <= max_date
    ) %>%
    mutate(type = "Nowcast")
  
  
  date_breaks <- ifelse(
    length(unique(df$date)) < 200,
    "1 month",
    "2 month"
  )
  
  date_minor_breaks <- ifelse(
    length(unique(df$date)) < 200,
    "2 weeks",
    "1 month"
  )
  
  #range(ylim)[2] - range(ylim)[1]
  
  x_text_size <- ifelse(length(unique(df$date)) < 200, 10, 9)
  
  
  if (is.null(ylim)) {
    ylim <- c(min(df$ci_90_lo), max(df$ci_90_hi)) 
  }
  
  if(range(ylim)[2] - range(ylim)[1] >= 4 & range(ylim)[2] - range(ylim)[1] <= 10){
    y_scale <- scale_y_continuous(position = "right", breaks = seq(from = ylim[1], to = ylim[2], by = 1))
  } else(
    y_scale <- scale_y_continuous(position = "right")
  )
  
  p <- ggplot(df) + 
    
    aes(date, mean, fill = type) +
    
    xlab(element_blank()) +
    
    coord_cartesian(ylim = ylim) +
    y_scale +
    scale_x_date(date_breaks = date_breaks, date_minor_breaks = date_minor_breaks, date_labels = "%e/%m") +
    scale_alpha(range = c(0, 0.5)) +
    scale_fill_manual(values = c("Nowcast" = base_colour)) +
    
    
    geom_vline(
      aes(xintercept = date),
      data = intervention_at,
      colour = "grey75"
    ) +
    
    geom_ribbon(aes(ymin = ci_90_lo,
                    ymax = ci_90_hi),
                alpha = 0.2) +
    geom_ribbon(aes(ymin = ci_50_lo,
                    ymax = ci_50_hi),
                alpha = 0.5) +
    geom_line(aes(y = ci_90_lo),
              colour = base_colour,
              alpha = 0.8) + 
    geom_line(aes(y = ci_90_hi),
              colour = base_colour,
              alpha = 0.8) + 
    
    geom_hline(yintercept = hline_at, linetype = "dotted") +
    
    cowplot::theme_cowplot() +
    cowplot::panel_border(remove = TRUE) +
    theme(legend.position = "none",
          strip.background = element_blank(),
          strip.text = element_text(hjust = 0, face = "bold"),
          axis.title.y.right = element_text(vjust = 0.5, angle = 90),
          panel.spacing = unit(1.2, "lines"),
          axis.text.x = element_text(size = x_text_size))
  
  if(plot_voc){
    p <- p + 
      geom_vline(
        #data = prop_voc_date_state(),
        data = prop_variant_dates(),
        aes(xintercept = date),
        colour = "firebrick1",
        linetype = 5
      ) # this may caus problems if plot_voc is TRUE but multistate is FALSE
  }
  
  if (multistate) {
    p <- p + facet_wrap(~ state, ncol = 2, scales = "free")
  }
  
  if (!is.na(projection_at)) {
    p <- p +
      geom_vline(xintercept = projection_at, linetype = "dashed", colour = "grey60") +
      annotate("rect",
               xmin = projection_at,
               xmax = max(df$date),
               ymin = -Inf,
               ymax = Inf,
               fill = grey(0.5), alpha = 0.1)
  }
  
  p    
  
}


# plot_trend_long is outdated at this point as plot_trend has been updated
# was intended as alternative to plot_trend with wider x-axis for longer timeseries
# still may be useful and need updating in that case to include voc
plot_trend_long <- function(simulations,
                       data,
                       base_colour = grey(0.4),
                       multistate = FALSE,
                       hline_at = 1,
                       ylim = c(0, 4),
                       intervention_at = interventions(),
                       projection_at = NA,
                       keep_only_rows = NULL,
                       max_date = data$dates$latest_mobility,
                       min_date = as.Date("2020-03-01")) {
  
  mean <- colMeans(simulations)
  ci_90 <- apply(simulations, 2, quantile, c(0.05, 0.95))
  ci_50 <- apply(simulations, 2, quantile, c(0.25, 0.75))
  
  if (multistate) {
    states <- rep(data$states, each = data$n_dates_project)
    dates <- rep(data$dates$infection_project, data$n_states)
  } else {
    dates <- data$dates$infection_project
    states <- NA
  }
  
  df <- tibble(date = dates,
               state = states,
               mean = mean,
               ci_50_lo = ci_50[1, ],
               ci_50_hi = ci_50[2, ],
               ci_90_lo = ci_90[1, ],
               ci_90_hi = ci_90[2, ])
  
  if (!is.null(keep_only_rows)) {
    df <- df[keep_only_rows, ]
  }
  
  df <- df %>%
    filter(
      date >= min_date,
      date <= max_date
    ) %>%
    mutate(type = "Nowcast")
  
  if (is.null(ylim)) {
    ylim <- c(min(df$ci_90_lo), max(df$ci_90_hi)) 
  }
  
  p <- ggplot(df) + 
    
    aes(date, mean, fill = type) +
    
    xlab(element_blank()) +
    
    coord_cartesian(ylim = ylim) +
    scale_y_continuous(position = "right") +
    scale_x_date(date_breaks = "1 month", date_labels = "%e/%m") +
    scale_alpha(range = c(0, 0.5)) +
    scale_fill_manual(values = c("Nowcast" = base_colour)) +
    
    geom_vline(
      aes(xintercept = date),
      data = intervention_at,
      colour = "grey80"
    ) +
    
    geom_ribbon(aes(ymin = ci_90_lo,
                    ymax = ci_90_hi),
                alpha = 0.2) +
    geom_ribbon(aes(ymin = ci_50_lo,
                    ymax = ci_50_hi),
                alpha = 0.5) +
    geom_line(aes(y = ci_90_lo),
              colour = base_colour,
              alpha = 0.8) + 
    geom_line(aes(y = ci_90_hi),
              colour = base_colour,
              alpha = 0.8) + 
    
    geom_hline(yintercept = hline_at, linetype = "dotted") +
    
    cowplot::theme_cowplot() +
    cowplot::panel_border(remove = TRUE) +
    theme(legend.position = "none",
          strip.background = element_blank(),
          strip.text = element_text(hjust = 0, face = "bold"),
          axis.title.y.right = element_text(vjust = 0.5, angle = 90),
          panel.spacing = unit(1.2, "lines"),
          axis.text.x = element_text(size = 8))
  
  if (multistate) {
    p <- p + facet_wrap(
      facets = ~ state,
      ncol = 2,
      scales = "free",
      strip.position = "left"
    ) +
      theme(
        strip.text.y.left = element_text(
          hjust = 0,
          vjust = 1,
          face = "bold",
          angle = 0
        )
      )
  }
  
  if (!is.na(projection_at)) {
    p <- p +
      geom_vline(xintercept = projection_at, linetype = "dashed", colour = "grey60") +
      annotate("rect",
               xmin = projection_at,
               xmax = max(df$date),
               ymin = -Inf,
               ymax = Inf,
               fill = grey(0.5), alpha = 0.1)
  }
  
  p    
  
}


# interpolate proportion of infectious cases that are imports
proportion_imported <- function (local_infectious, imported_infectious) {
  
  all_infectious <- local_infectious + imported_infectious
  prop_imported <- imported_infectious / all_infectious
  
  # transform to vector on unconstrained scale
  prop_imported <- pmax(0.01, prop_imported)
  prop_imported <- pmin(0.99, prop_imported)
  q_imported <- c(qlogis(prop_imported))
  
  n_dates <- nrow(all_infectious)
  n_states <- ncol(all_infectious)
  date_nums <- seq_len(n_dates)
  states <- factor(seq_len(n_states))
  # interpolate state-by-state  
  df <- data.frame(
    date = rep(date_nums, n_states),
    state = rep(states, each = n_dates)
  )
  model <- mgcv::gam(q_imported ~ s(date) +
                       s(date, by = state, k = 30),
                     data = df)
  q_imported <- predict(model, newdata = df)
  
  # transform back, clamp to numerically stable values, and return
  p_imported <- plogis(q_imported)
  eps <- sqrt(.Machine$double.eps)
  p_imported <- pmax(eps, p_imported)
  p_imported <- pmin(1 - eps, p_imported)
  dim(p_imported) <- dim(all_infectious)
  p_imported
  
}

app_downloads <- function () {
  tibble::tribble(
    ~date, ~number,
    "04-25", 0,
    "04-26", 1113000,
    "04-27", 2e6,
    "04-28", 2440000,
    "04-29", 3e6,
    "04-30", 3310000,
    "05-01", 3500000,
    "05-02", 4000000,
    "05-03", 4250000,
    "05-05", 4950000,
    "05-06", 5100000
  ) %>%
    mutate(
      date = paste0("2020-", date),
      date = as.Date(date),
      uptake = number / 16e6
    )
}

as.greta_array <- greta:::as.greta_array

# copy of greta.gp::gp with v manually passed in, enabling different
# (hierarchical) variance on each gp draw
multi_gp <- function (x, v, kernel, inducing = NULL, tol = 1e-04) {
  
  sparse <- !is.null(inducing)
  
  x <- as.greta_array(x)
  
  if (!sparse)
    inducing <- x
  else
    inducing <- as.greta_array(inducing)
  
  # calculate key objects
  Kmm <- kernel(inducing)
  
  m <- nrow(v)
  
  if (!identical(tol, 0))
    Kmm <- Kmm + diag(m) * tol
  
  Lm <- t(chol(Kmm))
  
  # evaluate gp at x
  if (sparse) {
    
    Kmn <- kernel(inducing, x)
    A <- forwardsolve(Lm, Kmn)
    f <- t(A) %*% v
    
  } else {
    
    f <- Lm %*% v
    
  }
  
  # add the info to the greta array
  attr(f, "gp_info") <- list(kernel = kernel,
                             inducing = inducing,
                             v = v,
                             Lm = Lm)
  f
}

get_tests <- function() {
  
  # scrape data on daily numbers of tests by state from covid19data.com.au
  url <- "https://e.infogram.com/_/3osqzRmYBiJsJafg79YC?parent_url=https%3A%2F%2Fwww-covid19data-com-au.filesusr.com%2Fhtml%2F2aed08_944ecbfd558f24812177bca5a8a74000.html&src=embed"
  
  text <- readLines(url) %>%
    paste(collapse = "\n")
  
  start <- str_locate(text, "window.infographicData=")[2] + 1
  text <- substr(text, start, 1000000L)
  end <- str_locate(text, ";</script>")[1]
  text <- substr(text, start = 1, stop = end)
  json <- rjson::fromJSON(text)
  
  data_lines <- json$elements[[2]]$data[[1]]
  states <- data_lines[[1]][-1]
  dates <- vapply(data_lines[-1], `[`, 1, FUN.VALUE = character(1))
  dates <- paste0(dates, "/2020")
  dates <- as.Date(dates, format = "%d/%m/%Y")
  
  values_text <- sapply(data_lines[-1], `[`, -1)
  values_text <- gsub(",", "", values_text)
  values <- as.numeric(values_text)
  values <- matrix(values, ncol = length(states), byrow = TRUE)
  colnames(values) <- states
  
  df <- values %>%
    as.data.frame() %>%
    cbind(date = dates) %>%
    pivot_longer(cols = -date,
                 names_to = "state",
                 values_to = "cumulative_tests") %>%
    group_by(state) %>%
    arrange(date) %>%
    mutate(daily_tests = c(NA, diff(cumulative_tests)),
           daily_tests = pmax(0, daily_tests))
  
  df
  
}

# weighted mean and standard error of the weighted mean, computed with a
# bootstrap
weighted_mean <- weighted.mean

weighted_se <- function(x, mu, w, na.rm = TRUE) {
  x_sims <- replicate(10000,
                      sample(x, length(x), replace = TRUE),
                      simplify = FALSE)
  means <- vapply(x_sims,
                  weighted_mean,
                  w,
                  na.rm = na.rm,
                  FUN.VALUE = numeric(1))
  sd(means)
}

# get a (mean and se) number of total contacts at baseline that is comparable to
# the numbers in Freya's survey from Prem/polymod (similar survey methodology)
baseline_total_contacts <- function() {
  
  # load Prem contact matrix for Australia
  f <- "data/contacts/contact_matrices_152_countries/MUestimates_all_locations_1.xlsx"
  all_aus <- readxl::read_xlsx(
    path = f,
    sheet = "Australia"
  ) %>%
    as.matrix()
  
  # load Australian population data
  pop <- read_csv(
    file = "data/contacts/ERP_QUARTERLY_20052020195358149.csv",
    col_types = cols(
      MEASURE = col_double(),
      Measure = col_character(),
      STATE = col_double(),
      State = col_character(),
      SEX_ABS = col_double(),
      Sex = col_character(),
      AGE = col_character(),
      Age = col_character(),
      FREQUENCY = col_character(),
      Frequency = col_character(),
      TIME = col_character(),
      Time = col_character(),
      Value = col_double(),
      `Flag Codes` = col_logical(),
      Flags = col_logical()
    )
  ) %>%
    filter(
      Measure == "Estimated Resident Population",
      Sex == "Persons",
      State == "Australia",
      Time == "Sep-2019",
      Age != "All ages"
    ) %>%
    select(age = Age, pop = Value) %>%
    mutate(age = as.numeric(age)) %>%
    arrange(age) %>%
    mutate(
      age_bin = cut(age, seq(0, 100, by = 5), include.lowest = TRUE, right = FALSE)
    ) %>%
    group_by(age_bin) %>%
    summarise(
      min = min(age),
      max = max(age),
      pop = sum(pop)
    )
  
  # get the age-binned population represented by the survey (of adults)
  survey_pop <- pop %>%
    group_by(age_bin) %>%
    mutate(
      fraction = mean((min:max) >= 18),
      weighted_pop = pop * fraction
    ) %>%
    ungroup() %>%
    filter(min < 80)
  
  mean_wt_aus <- weighted_mean(colSums(all_aus), survey_pop$weighted_pop)
  
  # get standard error of the total number of contacts in the UK arm of polymod,
  # as a measure of prior uncertainty in the Australian estimate of total
  # contacts
  download.file("https://raw.githubusercontent.com/goldingn/comix_covid-19-first_wave/master/data/polymod_contacts_part.rds",
                (f <- tempfile()), method='wget')
  
  standard_error_uk <- readRDS(f) %>%
    filter(part_age_group != "[0,18)") %>%
    group_by(part_id) %>%
    summarise(
      contacts = n_distinct(cont_id)
    ) %>%
    ungroup() %>%
    summarise(
      se = sd(contacts) / sqrt(n())
    ) %>%
    pull(se)
  
  tibble::tibble(
    mean = mean_wt_aus,
    se = standard_error_uk
  )
  
}

# get the numbers of household contacts and distribution among types of
# locations from Rolls et al.
rolls_contact_data <- function() {
  
  # load data on encounters of individuals and format
  individuals <- read_csv(
    file = "data/contacts/covid_full.csv",
    col_types = cols(
      participant_id = col_double(),
      contact_id = col_character(),
      contact_duration = col_double(),
      location_duration = col_double(),
      hh_member = col_double(),
      location_code = col_double(),
      location_type = col_character(),
      weight = col_double()
    ),
    na = c("NULL", "", "NA")
  ) %>%
    select(-location_duration, -location_code) %>%
    mutate(location = case_when(
      location_type == "Home" ~ "home",
      location_type == "Retail and hospitality (bars, cafes, shops, hair dressing, etc.)" ~ "retail",
      location_type == "Public spaces (parks, streets, stations, airports etc.)" ~ "public",
      location_type == "Public Transport (train, tram, bus or taxi)" ~ "transit",
      location_type == "Work" ~ "work",
      TRUE ~ "other",
    ))
  
  # convert encounters to numbers and total durations of unique contacts in each
  # locationand contact type
  contact_data <- individuals %>%
    # this encounter as a proportion of all encounters with this contact
    group_by(participant_id, contact_id) %>%
    mutate(proportion = 1 / n()) %>%
    # sum proportions and durations in each location, using the average duration with that contact where it's missing
    group_by(participant_id, contact_id, hh_member, location, weight) %>%
    summarise(
      proportion = sum(proportion),
      contact_duration = mean(contact_duration, na.rm = TRUE) / n(),
    ) %>%
    # count (proportional) unique contacts and average durations in household/non
    # household in each location for each participant
    group_by(participant_id, hh_member, location, weight) %>%
    summarise(
      contacts = sum(proportion),
      contact_duration = mean(contact_duration, na.rm = TRUE)
    ) %>%
    # convert duration to hours
    mutate(contact_duration = contact_duration / (60)) %>%
    ungroup() %>%
    mutate(
      hh_member = ifelse(hh_member == 1,
                         "household",
                         "non_household")
    ) %>%
    group_by(weight) %>%
    # expand out to include 0s for different categories, to averages are unbiased
    tidyr::complete(participant_id, hh_member, location, fill = list(contacts = 0)) %>%
    arrange(participant_id, hh_member, location)
  
  contact_data
  
}


baseline_contact_parameters <- function(gi_cdf) {
  
  # mean duration of infection in days
  infectious_days <- infectious_period(gi_cdf)
  
  # get the average number and duration contacts by household/non-household
  baseline_contact_params <- rolls_contact_data() %>%
    # contact duration in hours per day, multiply by infectious days *for
    # household contacts only* to get number of contact hours of entire
    # infectious period. For non-household contacts, we can assume they are
    # different individuals so transmission can exceed contacts, so multiply
    # infectious days by the daily R
    mutate(
      contact_duration = contact_duration * ifelse(hh_member == "household",
                                                   infectious_days,
                                                   1)
    ) %>%
    # summarise number and duration of household/non-household contacts per
    # respondent
    group_by(participant_id, hh_member, weight) %>%
    summarise(contacts = round(sum(contacts)),
              contact_duration = mean(contact_duration, na.rm = TRUE)) %>%
    ungroup() %>%
    # summarise over respondents
    group_by(hh_member) %>%
    summarise(
      mean_contacts = weighted_mean(
        contacts,
        w = weight,
        na.rm = TRUE
      ),
      se_contacts = weighted_se(
        contacts,
        mean_contacts,
        w = weight,
        na.rm = TRUE
      ),
      mean_duration = weighted_mean(
        contact_duration,
        w = weight,
        na.rm = TRUE
      ),
      se_duration = weighted_se(
        contact_duration,
        mean_duration,
        w = weight,
        na.rm = TRUE
      )
    )  
  
  # replace the prior over mean non-household contacts with a more comparable
  # estimate from Prem/Polymod
  TC_0_prior <- baseline_total_contacts()
  OC_0_prior <- tibble::tibble(
    mean = TC_0_prior$mean - baseline_contact_params$mean_contacts[1],
    se = sqrt(TC_0_prior$se ^ 2 + baseline_contact_params$se_contacts[1] ^ 2)
  )
  baseline_contact_params$mean_contacts[2] <- OC_0_prior$mean
  # increase the uncertainty on the baseline contact rate, since the survey
  # design of Polymod is not the same as those we have fielded, and the
  # demographic conversion in Prem may also make it less comparable
  baseline_contact_params$se_contacts[2] <- OC_0_prior$se * 5
  
  baseline_contact_params
  
}

# Results of Freya's survey
freya_survey_results <- function() {
  
  results <- tibble::tribble(
    ~date,        ~estimate, ~lower, ~upper,
    "2020-04-04",      2.78,   2.44,   3.17,
    "2020-05-02",      3.80,     NA,     NA,
  ) %>%
    mutate(
      date = as.Date(date),
      sd = mean(c(estimate[1] - lower[1], upper[1] - estimate[1])) / qnorm(0.95)
    )
  
  results
  
}

# mean infectious period in days
infectious_period <- function(cdf) {
  days <- 0:100
  gi_pmf <- gi_probability(cdf, days)
  infectious_days <- sum(days * gi_pmf)
  infectious_days
}

# find a prior over logit(p) that corresponds to the prior over R0, at the mean
# values of the baseline contact data, by moment matching
logit_p_prior <- function(params, gi_cdf) {
  
  infectious_days <- infectious_period(gi_cdf)
  
  transform <- function(free) {
    list(meanlogit = free[1],
         sdlogit = exp(free[2]))
  }
  
  R0 <- function(p, HC_0, HD_0, OC_0, OD_0) {
    HC_0 * (1 - p ^ HD_0) + OC_0 * infectious_days * (1 - p ^ OD_0)
  }
  
  R0_params <- function(logit_p_params, n = 1e5) {
    logit_p <- rnorm(n, logit_p_params$meanlogit, logit_p_params$sdlogit)
    R0_draws <- R0(
      p = plogis(logit_p),
      HC_0 = rnorm(n, params$mean_contacts[1], params$se_contacts[1]),
      OC_0 = rnorm(n, params$mean_contacts[2], params$se_contacts[2]),
      HD_0 = rnorm(n, params$mean_duration[1], params$se_duration[1]),
      OD_0 = rnorm(n, params$mean_duration[2], params$se_duration[2])
    )
    log_R0_draws <- log(R0_draws)
    list(meanlog = mean(log_R0_draws),
         sdlog = sd(log_R0_draws))
  }
  
  obj <- function(free) {
    logit_p_params <- transform(free)
    R0_params <- R0_params(logit_p_params)
    R0_expected <- R0_prior()
    (R0_expected$meanlog - R0_params$meanlog) ^ 2 + sqrt((R0_expected$sdlog - R0_params$sdlog) ^ 2)
  }
  
  set.seed(2020-05-18)
  o <- optim(c(5, -2), fn = obj, method = "BFGS")
  
  transform(o$par)
  
}


# get change in visits to locations - used as covariates for numbers of
# non-household contacts, and residential as proportional to household contact
# duration 
location_change <- function(dates = NULL) {
  
  google_change_trends <- readRDS("outputs/google_change_trends.RDS") 
  
  location_change_trends <- google_change_trends %>%
    mutate(location = case_when(
      datastream == "Google: time at residential" ~ "home",
      datastream == "Google: time at transit stations" ~ "transit",
      datastream == "Google: time at parks" ~ "public",
      datastream == "Google: time at workplaces" ~ "work",
      datastream == "Google: time at retail and recreation" ~ "retail",
      TRUE ~ "other"
    )) %>%
    filter(location != "other") %>%
    select(-state_datastream, -datastream) %>% 
    pivot_wider(names_from = location, values_from = change)
  
  # optionally add all dates and states, and pad missing values (prior to first
  # mobility data) with 1s
  if (!is.null(dates)) {
    location_change_trends <- location_change_trends %>%
      group_by_all() %>%
      old_right_join(
        expand_grid(
          state = unique(.$state),
          date = dates
        )
      ) %>%
      ungroup() %>%
      mutate_at(vars(-state, -date), replace_na, 1)
  }
  
  location_change_trends
  
}

# change in time at residential locations in each state
h_t_state <- function(dates) {
  
  location_change(dates) %>%
    select(state, date, home) %>%
    pivot_wider(names_from = state, values_from = home) %>%
    select(-date) %>%
    as.matrix()
  
}

# compute fractions of non-household (unique) contacts
# in each location 
location_contacts <- function() {
  
  rolls_contact_data() %>%
    filter(hh_member == "non_household") %>%
    group_by(participant_id, location, weight) %>%
    summarise(contacts = sum(contacts)) %>%
    group_by(location) %>%
    summarise(
      mean_contacts = weighted_mean(
        contacts,
        w = weight
      )
    ) %>%
    mutate(
      proportion_contacts = mean_contacts / sum(mean_contacts)
    )
  
}

# get the index of microdistancing in each state as a date-by-state matrix
trends_date_state <- function (file, dates = NULL, states = NULL) {
  
  trends <- readRDS(file)
  
  date_seq <- seq(min(trends$date), max(trends$date), by = 1)
  
  if (is.null(states)) {
    states <- unique(trends$state)
  }
  
  index <- trends %>%
    # expand to all required dates and states
    select(state, date, mean) %>%
    old_right_join(
      expand_grid(
        state = states,
        date = date_seq
      )
    ) %>%
    # turn into a date-by-state matrix
    pivot_wider(
      names_from = state,
      values_from = mean
    ) %>%
    select(-date) %>%
    as.matrix()
  
  # crop to specified dates, extending either end out flat if missing
  if (!is.null(dates)) {
    idx <- dates - min(date_seq) + 1
    idx <- pmax(idx, 1)
    idx <- pmin(idx, nrow(index))
    index <- index[idx, ]
  }
  
  index
  
}

# get the overall index of distancing (no waning) on the current dates and
# optionally add extra 1s at the end
social_distancing_national <- function(dates, n_extra = 0) {
  
  distancing_file <- "outputs/social_distancing_latent.RDS"
  distancing_index <- distancing_file %>%
    readRDS() %>%
    select(mean, date) %>%
    old_right_join(tibble(date = dates)) %>%
    replace_na(list(mean = 0)) %>%
    pull(mean)
  
  distancing_index <- c(distancing_index, rep(1, n_extra))
  distancing_index
  
}


# prop_voc_date_state <- function() {
#   tibble::tribble(
#     ~state,        ~date, ~prop_voc,
#      "ACT", "2021-01-27",         1,
#      "NSW", "2021-01-27",         1,
#       "NT", "2021-01-27",         1,
#      "QLD", "2021-01-27",         1,
#       "SA", "2021-01-27",         1,
#      "TAS", "2021-01-27",         1,
#      "VIC", "2021-01-27",         1,
#       "WA", "2021-01-27",         1
#   ) %>%
#     mutate(
#       date = as.Date(date)
#     )
# }
# 
# prop_voc_date_state_long <- function(dates) {
#   
#   df <- expand_grid(
#     date = dates,
#     state = c("ACT", "NSW", "NT", "QLD", "SA", "TAS", "VIC", "WA")
#   ) %>%
#     full_join(
#       prop_voc_date_state()
#     )%>%
#     pivot_wider(
#       names_from = state,
#       values_from = prop_voc
#     ) %>%
#     dplyr::select(-date) %>%
#     as.matrix
#   
#   
#   df[1,] <- apply(
#     X = df[1,] %>% as.matrix,
#     MARGIN = 2,
#     FUN = function(x){
#       ifelse(is.na(x), 0, x)
#     }
#   ) %>%
#     t
#   
#   df <- df %>%
#     as_tibble %>%
#     fill(everything()) %>%
#     as.matrix
#   
#   
#   return(df)
# }

prop_variant_dates <- function(){
  tibble::tribble(
    ~state,        ~date, ~prop_wt, ~prop_alpha, ~prop_delta,
    "ACT",  "2020-01-01",        1,           0,          0,
    "NSW",  "2020-01-01",        1,           0,          0,
    "NT",   "2020-01-01",        1,           0,          0,
    "QLD",  "2020-01-01",        1,           0,          0,
    "SA",   "2020-01-01",        1,           0,          0,
    "TAS",  "2020-01-01",        1,           0,          0,
    "VIC",  "2020-01-01",        1,           0,          0,
    "WA",   "2020-01-01",        1,           0,          0,
    
    "ACT",  "2021-01-27",        0,           1,          0,
    "NSW",  "2021-01-27",        0,           1,          0,
    "NT",   "2021-01-27",        0,           1,          0,
    "QLD",  "2021-01-27",        0,           1,          0,
    "SA",   "2021-01-27",        0,           1,          0,
    "TAS",  "2021-01-27",        0,           1,          0,
    "VIC",  "2021-01-27",        0,           1,          0,
    "WA",   "2021-01-27",        0,           1,          0,
    
    "ACT",  "2021-06-07",        0,           0,          1,
    "NSW",  "2021-06-07",        0,           0,          1,
    "NT",   "2021-06-07",        0,           0,          1,
    "QLD",  "2021-06-07",        0,           0,          1,
    "SA",   "2021-06-07",        0,           0,          1,
    "TAS",  "2021-06-07",        0,           0,          1,
    "VIC",  "2021-06-07",        0,           0,          1,
    "WA",   "2021-06-07",        0,           0,          1
  ) %>%
    mutate(
      date = as.Date(date)
    )
}

prop_variant <- function(dates){
  
  df <-  prop_variant_dates() %>%
    full_join(
      y = expand_grid(
        date = dates,
        state = c("ACT", "NSW", "NT", "QLD", "SA", "TAS", "VIC", "WA")
      )
    ) %>%
    arrange(state, date) %>%
    tidyr::fill(everything()) %>%
    filter(date %in% dates) # account for "2020-01-01" start date may not be in dates
    
  prop_wt <- df %>%
    dplyr::select(state, date, prop_wt) %>%
    pivot_wider(
      names_from = state,
      values_from = prop_wt
    ) %>%
    arrange(date) %>%
    dplyr::select(-date)%>%
    as.matrix
  
  prop_alpha <- df %>%
    dplyr::select(state, date, prop_alpha) %>%
    pivot_wider(
      names_from = state,
      values_from = prop_alpha
    ) %>%
    arrange(date) %>%
    dplyr::select(-date)%>%
    as.matrix
  
  prop_delta <- df %>%
    dplyr::select(state, date, prop_delta) %>%
    pivot_wider(
      names_from = state,
      values_from = prop_delta
    ) %>%
    arrange(date) %>%
    dplyr::select(-date)%>%
    as.matrix
  
  prop_variant <- list(
    "prop_wt"    = prop_wt,
    "prop_alpha" = prop_alpha,
    "prop_delta" = prop_delta
  )
  
  return(prop_variant)
}


# greta sub-model for the component R_eff due to macro- and micro-distancing
distancing_effect_model <- function(
  dates,
  gi_cdf,
  voc_mixture = c("all", "alpha", "delta", "wt")
) {
  
  voc_mixture <- match.arg(voc_mixture)
  
  # informative priors on variables for contacts at t = 0 (Hx = household, Ox =
  # non-household, Tx = total, xC = contacts. xD = duration)
  baseline_contact_params <- baseline_contact_parameters(gi_cdf)
  
  
  prop_var <- prop_variant(dates = dates)
  prop_alpha <- prop_var$prop_alpha
  prop_delta <- prop_var$prop_delta
  prop_wt    <- prop_var$prop_wt
  
  if(voc_mixture == "alpha") {
    prop_alpha <- prop_alpha * 0 + 1
    prop_delta <- prop_wt <- prop_delta * 0
  }
  
  if(voc_mixture == "delta") {
    prop_delta <- prop_delta * 0 + 1
    prop_alpha <- prop_wt <- prop_alpha * 0
  }
  
  if(voc_mixture == "wt") {
    prop_wt <- prop_wt * 0 + 1
    prop_alpha <- prop_alpha * 0 
    prop_delta <- prop_delta * 0
  }
  
  
  # prior on the probability of *not* transmitting, per hour of contact
  # (define to match moments of R0 prior)
  logit_p_params <- logit_p_prior(baseline_contact_params, gi_cdf)
  logit_p <- normal(logit_p_params$meanlogit, logit_p_params$sdlogit)
  p <- ilogit(logit_p)
  
  phi_alpha       <- normal(1.454, 0.055, truncation = c(0, Inf))
  phi_delta_alpha <- normal(1.421, 0.033, truncation = c(0, Inf))
  
  phi_delta <- phi_alpha * phi_delta_alpha
  
  phi_star <- prop_wt * 1 + prop_alpha * phi_alpha + prop_delta * phi_delta

  p_star <- p ^ phi_star
  
  
  infectious_days <- infectious_period(gi_cdf)
  
  HC_0 <- normal(baseline_contact_params$mean_contacts[1],
                 baseline_contact_params$se_contacts[1],
                 truncation = c(0, Inf))
  HD_0 <- normal(baseline_contact_params$mean_duration[1],
                 baseline_contact_params$se_duration[1],
                 truncation = c(0, Inf))
  OD_0 <- normal(baseline_contact_params$mean_duration[2],
                 baseline_contact_params$se_duration[2],
                 truncation = c(0, Inf))
  
  # get HD_t in each state
  h_t <- h_t_state(dates)
  HD_t <- HD_0 * h_t
  
  # trends in non-household contacts in each state over time
  OC_t_state <- trends_date_state(
    "outputs/macrodistancing_trends.RDS",
    dates
  )
  OC_0 <- OC_t_state[1, 1]
  
  # model gamma_t: reduction in duration and transmission probability of
  # non-household contacts over time, per state
  
  # load probability of microdistancing and divide by the maximum value to get
  # an index of per-contact transmission probability
  microdistancing_prob <- trends_date_state(
    "outputs/microdistancing_trends.RDS",
    dates
  )
  d_t_state <- microdistancing_prob / max(microdistancing_prob)
  
  beta <- uniform(0, 1)
  gamma_t_state <- 1 - beta * d_t_state
  
  # compute component of R_eff for local cases
  household_infections <- HC_0 * (1 - p_star ^ HD_t)
  non_household_infections <- OC_t_state * gamma_t_state *
    infectious_days * (1 - p_star ^ OD_0)
  R_t <- household_infections + non_household_infections
  
  # return greta arrays
  list(R_t = R_t, 
       gamma_t_state = gamma_t_state,
       OC_t_state = OC_t_state,
       p = p,
       p_star = p_star,
       beta = beta,
       HC_0 = HC_0,
       HD_0 = HD_0,
       OC_0 = OC_0,
       OD_0 = OD_0,
       dates = dates,
       phi_alpha = phi_alpha,
       phi_delta_alpha = phi_delta_alpha,
       phi_delta = phi_delta,
       phi_star = phi_star)
  
}

#distancing_effect_model <- function(dates, gi_cdf) {
distancing_effect_model_old <- function(dates, gi_cdf) {
  
  # informative priors on variables for contacts at t = 0 (Hx = household, Ox =
  # non-household, Tx = total, xC = contacts. xD = duration)
  baseline_contact_params <- baseline_contact_parameters(gi_cdf)
  
  # prior on the probability of *not* transmitting, per hour of contact
  # (define to match moments of R0 prior)
  logit_p_params <- logit_p_prior(baseline_contact_params, gi_cdf)
  logit_p <- normal(logit_p_params$meanlogit, logit_p_params$sdlogit)
  p <- ilogit(logit_p)
  
  infectious_days <- infectious_period(gi_cdf)
  
  HC_0 <- normal(baseline_contact_params$mean_contacts[1],
                 baseline_contact_params$se_contacts[1],
                 truncation = c(0, Inf))
  HD_0 <- normal(baseline_contact_params$mean_duration[1],
                 baseline_contact_params$se_duration[1],
                 truncation = c(0, Inf))
  OD_0 <- normal(baseline_contact_params$mean_duration[2],
                 baseline_contact_params$se_duration[2],
                 truncation = c(0, Inf))
  
  # get HD_t in each state
  h_t <- h_t_state(dates)
  HD_t <- HD_0 * h_t
  
  # trends in non-household contacts in each state over time
  OC_t_state <- trends_date_state(
    "outputs/macrodistancing_trends.RDS",
    dates
  )
  OC_0 <- OC_t_state[1, 1]
  
  # model gamma_t: reduction in duration and transmission probability of
  # non-household contacts over time, per state
  
  # load probability of microdistancing and divide by the maximum value to get
  # an index of per-contact transmission probability
  microdistancing_prob <- trends_date_state(
    "outputs/microdistancing_trends.RDS",
    dates
  )
  d_t_state <- microdistancing_prob / max(microdistancing_prob)
  
  beta <- uniform(0, 1)
  gamma_t_state <- 1 - beta * d_t_state
  
  # compute component of R_eff for local cases
  household_infections <- HC_0 * (1 - p ^ HD_t)
  non_household_infections <- OC_t_state * gamma_t_state *
    infectious_days * (1 - p ^ OD_0)
  R_t <- household_infections + non_household_infections
  
  # return greta arrays
  list(R_t = R_t, 
       gamma_t_state = gamma_t_state,
       OC_t_state = OC_t_state,
       p = p,
       beta = beta,
       HC_0 = HC_0,
       HD_0 = HD_0,
       OC_0 = OC_0,
       OD_0 = OD_0,
       dates = dates)
  
}


plot_fit <- function(observed_cases, cases_sim, data) {
  
  valid <- which(data$valid_mat, arr.ind = TRUE)
  dates <- data$dates$infection
  states <- data$data$states
  
  # compute quantiles and plot timeseries for each state
  quants <- apply(
    cases_sim,
    2,
    quantile,
    c(0.025, 0.25, 0.5, 0.75, 0.975)
  )
  
  # PPC check for each state
  par(mfrow = c(4, 2),
      mar = c(2, 3, 2, 0.5))
  for(i in 1:8) {
    idx <- valid[, 2] == i
    y <- observed_cases[idx]
    x <- dates[valid[idx, 1]]
    plot(y ~ x,
         type = "n",
         ylim = range(c(quants[, idx], y)),
         xlim = range(dates[valid[, 1]]),
         ylab = "",
         las = 1,
         xlab = "")
    mtext(states[i], side = 3, adj = 0, cex = 0.8)
    polygon(c(x, rev(x)),
            c(quants[1, idx], rev(quants[5, idx])),
            col = grey(0.9),
            lty = 0)
    polygon(c(x, rev(x)),
            c(quants[2, idx], rev(quants[4, idx])),
            col = grey(0.8),
            lty = 0)
    points(y ~ x, pch = 16, cex = 0.5)
    
  }
  
}

# read in the national survey raw data and create contact and microdistancing
# data in the same firmat as originally shared. I decided I can have a weird
# recursive function, as a treat.
format_raw_survey_data <- function(file = NULL, wave = NULL) {
  
  # in default mode, loop through all extra raw files and try to get them in order
  if (is.null(file)) {
    files <- list.files("data/survey_raw/", pattern = ".csv$", full.names = TRUE)
    lengths <- nchar(files)
    dates <- files %>%
      substr(lengths - 9, lengths - 4) %>%
      as.Date("%d%m%y")
    files <- files[order(dates)]
    waves <- seq_along(files) + 14
    mapply(format_raw_survey_data, files, waves)
    return(invisible(NULL))
  }
  
  raw <- read_csv(file)
  
  micro <- raw %>%
    select(
      state = S3,
      date = StartDate,
      "1.5m compliance" = Q65,
      "non-household contact" = Q109,
      "hand hygine" = Q110,
      "cough" = Q111
    ) %>%
    mutate(
      state = case_when(state == "ACT" ~ "Australian Capital Territory",
                        TRUE ~ state),
      state = abbreviate_states(state)
    ) %>%
    filter(!is.na(state)) %>%
    mutate(
      date = as.character(date),
      date = as.Date(date, format = "%Y%m%d"),
      date = min(date)
    ) %>%
    pivot_longer(
      cols = c(
        "1.5m compliance",
        "non-household contact",
        "hand hygine",
        "cough"
      ),
      names_to = "question",
      values_to = "response"
    ) %>%
    group_by(state, date, question) %>%
    mutate(respondents = n()) %>%
    group_by(state, date, question, respondents) %>%
    count(response) %>%
    select(date,
           state,
           question,
           response,
           count = n,
           respondents) %>%
    arrange(state, date, question, response)
  
  write_csv(micro,
            paste0(
              "data/microdistancing/Barometer wave ",
              wave,
              " compliance.csv"
            ))
  
  contacts <- raw %>%
    select(state = S3,
           num_contacts = Q138) %>%
    mutate(state = replace_na(state, "Other")) %>%
    group_by(num_contacts, state) %>%
    summarise(n = n()) %>%
    pivot_wider(
      names_from = state,
      values_from = n,
      values_fill = list(n = 0)
    )
  
  
  write_csv(contacts,
            paste0("data/contacts/barometer/contact numbers wave ", wave, ".csv"))
  
}

# parsing the file of microdistancing measures from BETA
barometer_results <- function() {
  
  files <- list.files("data/microdistancing/",
                      pattern = ".csv$",
                      full.names = TRUE)
  
  tibbles <- lapply(
    files,
    read_csv,
    col_types = cols(
      date = col_date(format = ""),
      state = col_character(),
      question = col_character(),
      response = col_character(),
      count = col_double(),
      respondents = col_double()
    )
  )
  
  do.call(bind_rows, tibbles) %>%
    filter(
      !state %in% c("Australia", "Other")
    ) %>%
    mutate(
      question = recode(
        question,
        "hand hygine" = "Hand washing",
        "cough" = "Cough etiquette",
        "non-household contact" = "Physical contact"
      ), 
      state = recode(
        state,
        "New South Wales" = "NSW",
        "Northern Territory" = "NT",
        "Queensland" = "QLD",
        "South Australia" = "SA",
        "Tasmania" = "TAS",
        "Victoria" = "VIC",
        "Western Australia" = "WA"
      )
    ) %>%
    arrange(state, date, question, response)
  
}

# move columns of this matrix one to the left, and replace the right-most column
# with 1s. Used to define a weights matrix for a piecewise linear curve.
next_column <- function(x) {
  cbind(x[, -1], rep(1, nrow(x)))
}

# model for the trend in microdistancing
# compared to the macrodistancing effect, this has a linear transform of the
# distancing coefficient on the logit scale - corresponding to a different
# spread in adoption of microdistancing behaviour - and different dates of peak
# microdistancing and subsequent inflections
microdistancing_model <- function(data, parameters) {
  
  # shapes of inflection effects
  # timing of inflections in each state
  n_states <- nrow(parameters$inflections)
  segment_ends <- cbind(parameters$inflections, ones(n_states))
  denoms <- next_column(parameters$inflections) - parameters$inflections
  
  inflections_long <- parameters$inflections[data$state_id, ]
  denominators <- next_column(inflections_long) - inflections_long
  numerators <- sweep(-inflections_long, 1, data$time, FUN = "+")
  inflection_shape <- cbind(ones(length(data$state)), numerators / denominators)

  # masking columns to create weights matrix
  null1 <- (sign(inflection_shape) + 1) / 2
  null2 <- next_column(1 - null1)
  null3 <- next_column(null2)
  
  # piecewise linear weights matrix
  weights_up <- inflection_shape * null1 * null2
  anti_weights <- next_column(weights_up)
  weights_down <- (1 - anti_weights) * (1 - null2) * null3
  weights <- weights_up + weights_down 
  
  # apply weights to get inflections, and apply to initial distancing effect
  heights_long <- parameters$heights[data$state_id, ]
  inflection <- rowSums(weights * heights_long)
  
  # apply the initial distancing period (shrunk to meet the first height)
  initial <- (1 - data$distancing) * heights_long[, 1] 
  inflection - initial
  
}

log_imultilogit <- function(x) {
  
  dim <- dim(x)
  
  # check it's a matrix
  if (length(dim) != 2) {
    stop("log_imultilogit expects a 2D greta array",
         call. = FALSE)
  }
  
  op("log_imultilogit", x,
     dim = dim + c(0, 1),
     tf_operation = "tf_log_imultilogit")
}

tf_log_imultilogit <- function(x) {
  batch_size <- tf$shape(x)[[0]]
  shape <- c(batch_size, dim(x)[[2]], 1L)
  zeros <- tf$zeros(shape, tf_float())
  latent <- tf$concat(list(x, zeros), 2L)
  tf$nn$log_softmax(latent)
}


# model for the trend in macrodistancing a weighted sum of time at location
# types, and an overall scaling coefficient, multiplied by a scalar baseline
# contact rate
macrodistancing_model <- function(data, parameters) {
  
  # format data into a date/state by location greta array of log ratios of
  # mobility on baseline
  log_location_change <- data$location_change_trends %>%
    mutate_at(
      vars(public, home, retail, transit, work),
      log
    ) %>%
    # flip log ratio for at home (regression coefficients are all positive)
    mutate(home = -home) %>%
    # turn into a matrix
    select(-state, -date) %>%
    as.matrix() %>%
    # and into a greta array
    as_data()
  
  # expected log change in contacts
  log_change_contacts <- log_location_change %*% parameters$mobility_coefs
  
  # average daily number of contacts
  log_mean_daily_contacts <- log(parameters$OC_0) + log_change_contacts
  mean_daily_contacts <- exp(log_mean_daily_contacts)

  # model the fraction of contacts falling on each day of the week via a multilogit model
  n <- nrow(log_mean_daily_contacts)
  X <- cbind(ones(n), log_mean_daily_contacts)
  eta <- X %*% parameters$weekday_coefs
  
  # imultilogit gives the value for every weekday against each date, so pull out only the
  # actual weekday for that date
  idx <- cbind(
    seq_len(n),
    lubridate::wday(data$location_change_trends$date)
  )
  
  log_fraction_weekly_contacts <- log_imultilogit(eta)[idx]
  
  # this equivalent to:
  #   log_fraction_weekly_contacts_wday <- eta[idx] - log_sum_exp(eta)
  # but shouldn't be much different speed-wise

  # use this to apply the weekday effect
  log_mean_weekly_contacts <- log_mean_daily_contacts + log(7)
  log_mean_daily_contacts_wday <- log_mean_weekly_contacts + log_fraction_weekly_contacts
  
  # convert both to wide (date by state) format for lookups
  mean_daily_contacts_wide <- greta_long_to_date_state(
    mean_daily_contacts,
    data$location_change_trends$date,
    data$location_change_trends$state
  )
  
  log_mean_daily_contacts_wday_wide <- greta_long_to_date_state(
    log_mean_daily_contacts_wday,
    data$location_change_trends$date,
    data$location_change_trends$state
  )
  
  log_fraction_weekly_contacts_wide <- greta_long_to_date_state(
    log_fraction_weekly_contacts,
    data$location_change_trends$date,
    data$location_change_trends$state
  )
  
  # return wide version of all these
  list(
    mean_daily_contacts = mean_daily_contacts_wide,
    log_mean_daily_contacts_wday = log_mean_daily_contacts_wday_wide,
    log_fraction_weekly_contacts = log_fraction_weekly_contacts_wide
  )
  
}

# a sort of null model (assuming a different rate of contacts per survey/state) for plotting the data 
macrodistancing_null <- function(data, log_fraction_weekly_contacts_mean) {
  
  # extract indices to the survey waves and states for each observation
  
  # add numeric ids for wave dates and states
  wave_dates <- sort(unique(data$contacts$wave_date))
  states <- sort(unique(data$contacts$state))
  dates <- sort(unique(data$location_change_trends$date))
  
  idx <- data$contacts %>%
    mutate(
      wave_id = match(wave_date, wave_dates),
      state_id = match(state, states)
    ) %>%
    select(wave_id, state_id) %>%
    as.matrix()
    
  n_waves <- length(wave_dates)
  n_states <- length(states)

  # hierarchical model for the average number of contacts per survey/state
  log_contacts_mean <- normal(0, 10)
  log_contacts_sd <- normal(0, 1, truncation = c(0, Inf))
  log_contacts_raw <- normal(0, 1, dim = c(n_waves, n_states))
  log_contacts_wide <- log_contacts_mean + log_contacts_raw * log_contacts_raw
  
  # expand these out to match the data
  log_mean_daily_contacts <- log_contacts_wide[idx]
  
  # this has the fractions for each date and state, so pull out the relevant
  # entries
  idx <- data$contacts %>%
    mutate(
      date_id = match(date, dates),
      state_id = match(state, states)
    ) %>%
    select(date_id, state_id) %>%
    as.matrix()
  
  # use this to apply the weekday effect
  log_mean_weekly_contacts <- log_mean_daily_contacts + log(7)
  log_predicted_contacts <- log_mean_weekly_contacts + log_fraction_weekly_contacts_mean[idx]
  
  # get lognormal parameters from mean and standard deviation
  sdlog <- normal(0, 5, truncation = c(0, Inf))
  
  # because mean = exp(meanlog + (sdlog ^ 2) / 2)
  meanlog <- log_predicted_contacts - (sdlog ^ 2) / 2

  distribution(data$contacts$contact_num) <- discrete_lognormal(
    meanlog = meanlog,
    sdlog = sdlog,
    breaks = data$breaks
  )
  
  # return greta arrays to fit model  
  module(
    avg_daily_contacts_wide = exp(log_contacts_wide),
    sdlog,
    wave_dates,
    states
  )
  
}

# take a vector greta array correpsonding to dates and states and convert to
# date-by-state wide format
greta_long_to_date_state <- function(long, dates, states) {
  wide_dim <- c(n_distinct(dates), n_distinct(states))
  greta_array(long, dim = wide_dim)
}

# contruct multiple GPs for epsilons in the Reff model
epsilon_gp <- function(
  date_nums,
  n_states,
  kernel,
  inducing_date_nums = date_nums,
  sigma_state = normal(0, 0.5, truncation = c(0, Inf), dim = n_states),
  tol = 1e-6) {
  
  # whitened representation of GP
  n_inducing <- length(inducing_date_nums)
  v_raw <- normal(0, 1, dim = c(n_inducing, n_states))
  v <- sweep(v_raw, 2, sigma_state, FUN = "*")
  
  # GP  
  epsilon <- multi_gp(
    x = date_nums,
    v = v,
    kernel = kernel,
    inducing = inducing_date_nums,
    tol = tol
  )
  
  epsilon
  
}

op <- greta::.internals$nodes$constructors$op

# Given an date-by-state matrix of 'infectiousness' due to imports and previous
# locally-acquired cases, and a corresponding matrix 'R_local' of Reff for
# locally-acquired infections, compute the expected numbers of locally-acquired
# cases into the future. Also requires 'disaggregation_probs' giving the density
# of the serial interval over a sequence of days.
project_local_cases <- function(
  infectiousness,
  R_local,
  disaggregation_probs
) {
  
  if (!identical(dim(infectiousness), dim(R_local))) {
    stop ("infectiousness and R_local must have the same dimensions")
  }
  
  op("project_local_cases",
     infectiousness,
     R_local,
     disaggregation_probs,
     operation_args = list(
       T = nrow(infectiousness),
       K = length(disaggregation_probs)
     ),
     tf_operation = "tf_project_local_cases",
     dim = dim(infectiousness))
  
}

# tensorflow function to project the expected number of locally-acquired cases
# into the future
tf_project_local_cases <- function(infectiousness, R_local, disaggregation_probabilities, T, K) {
  
  # continuing condition of TF while loop
  cond <- function(C, I, R, p, t, T, K, sequence) {
    tf$less(t, T)
  }
  
  # body of TF while loop
  body <- function(C, I, R, p, t, T, K, sequence) {
    
    # increase the expected infectiousness on subsequent days due to cases
    # infected on this day
    new_C <- R[, t, ] * I[, t, ]
    new_C <- tf$expand_dims(new_C, 1L)
    
    # distribute infectiousness of these cases across their infectious profile
    new_I <- new_C * p
    
    # add to cases and cumulative infectiousness
    perm_to <- c(1L, 2L, 0L)
    perm_from <- c(2L, 0L, 1L)
    
    I_t <- tf$transpose(I, perm_to)
    new_I_t <- tf$transpose(new_I, perm_to)
    I_t <- tf$tensor_scatter_nd_add(I_t,
                                    indices = t + sequence,
                                    updates = new_I_t)
    I <- tf$transpose(I_t, perm_from)
    
    C_t <- tf$transpose(C, perm_to)
    new_C_t <- tf$transpose(new_C, perm_to)
    C_t <- tf$tensor_scatter_nd_add(C_t,
                                    indices = tf$reshape(t, shape = c(1L, 1L)),
                                    updates = new_C_t)
    C <- tf$transpose(C_t, perm_from)
    
    list(C, I, R, p, t + 1L, T, K, sequence)
    
  }
  
  # pad I and R with K zeros so we don't need to mess with indexing inside the loop
  batch_size <- greta:::get_batch_size()
  n_locations <- dim(infectiousness)[[3]]
  pad <- tf$zeros(
    shape = tf$stack(list(batch_size, K + 1L, n_locations)),
    dtype = greta:::tf_float()
  )
  I <- tf$concat(list(infectiousness, pad), axis = 1L)
  R <- tf$concat(list(R_local, pad), axis = 1L)
  C <- tf$zeros(
    shape = tf$stack(list(batch_size, T, n_locations)),
    dtype = greta:::tf_float()
  )
  
  # initial variables for loop
  values <- list(
    C,
    I,
    R,
    disaggregation_probabilities,
    0L,
    T,
    K,
    as.matrix((1:K) - 1L)
  )
  
  # iterate to compute infectiousness
  result <- tf$while_loop(cond, body, values)
  
  # return expected numbers of new local cases
  result[[1]]
  
}

# check fit of projected cases against national epi curve
check_projection <- function(fitted_model, start_date = as.Date("2020-02-28")) {
  
  R_eff_local <- fitted_model$greta_arrays$R_eff_loc_12
  R_eff_imported <- fitted_model$greta_arrays$R_eff_imp_12
  gi_vec <- gi_vector(gi_cdf, fitted_model$data$dates$latest, state = "ACT")
  local_infectiousness <- fitted_model$data$local$infectiousness
  imported_infectiousness <- fitted_model$data$imported$infectiousnes
  local_cases <- fitted_model$data$local$cases
  dates <- fitted_model$data$dates$infection
  n_states <- fitted_model$data$n_states
  n_dates <- fitted_model$data$n_dates
  
  # national-level Reff - no clusters and weighted by state populations
  local_weights <- sweep(
    local_infectiousness,
    1,
    rowSums(local_infectiousness),
    FUN = "/"
  )
  local_weights[is.na(local_weights)] <- 1 / n_states
  
  import_weights <- sweep(
    imported_infectiousness,
    1,
    rowSums(imported_infectiousness),
    FUN = "/"
  )
  import_weights[is.na(import_weights)] <- 1 / n_states
  
  R_eff_loc_ntnl <- rowSums(R_eff_local[seq_len(n_dates), ] * local_weights)
  R_eff_imp_ntnl <- rowSums(R_eff_imported[seq_len(n_dates), ] * import_weights)
  
  # subset to from the first of March, when transmission became established (the
  # model is not designed to work with the stochastic extinctions we saw at the beginning of the outbreak)
  start <- which(dates == start_date)
  sub_idx <- start:n_dates
  
  # simulate local-local transmission dynamics, at national level; forced using
  # (observed) case importation and local cases prior to the start of the
  # simulation
  
  # locally-acquired infections present prior to the start of the simulation
  previous_local_cases <- local_cases
  previous_local_cases[sub_idx, ] <- 0
  previous_local_infectiousness <- gi_convolution(
    cases = previous_local_cases,
    dates = dates,
    states = data$states,
    gi_cdf = gi_cdf
  )
  # previous_local_infectiousness <- rowSums(previous_local_infectiousness)
  
  # compute infectious forcing from local cases emerging during this period that
  # were directly infected by imported cases (can't just include the import
  # infectiousness, since they are subject to a different Reff). Get expected
  # number of new local cases from imports, then disaggregate according to their
  # infectiousness profile to get force of local infection
  
  # expected number of new locally-acquired cases during the simulation period due
  # to infection from imports
  import_local_cases <- sweep(imported_infectiousness, 1, R_eff_imp_ntnl[seq_len(n_dates)], FUN = "*")
  import_local_cases_ntnl <- rowSums(import_local_cases)
  import_local_infectiousness <- gi_convolution(
    cases = import_local_cases,
    dates = dates,
    states = data$states,
    gi_cdf = gi_cdf
  )
  
  # combine these to get forcing from existing and import-associated local cases,
  # and disaggregate to get infectiousness of these
  local_infectiousness <- previous_local_infectiousness + import_local_infectiousness
  
  # Given this basic force of infection, R for locally-acquired cases (mean trend,
  # no clusters), and the infectiousness profile, iterate the dynamics to compute
  # the numbers of local cases
  secondary_locals <- project_local_cases(
    infectiousness = rowSums(local_infectiousness)[sub_idx],
    R_local = R_eff_loc_ntnl[sub_idx],
    disaggregation_probs = gi_vec
  )
  
  # compute locally-acquired cases
  local_cases_project_ntnl <- import_local_cases_ntnl[sub_idx] + secondary_locals
  local_cases_project_ntnl_sim <- calculate(local_cases_project_ntnl,
                                            values = fitted_model$draws,
                                            nsim = 1000)[[1]]
  
  data <- fitted_model$data
  data$dates$infection_project <- dates[sub_idx]
  data$n_dates_project <- length(sub_idx)
  
  local_cases_ntnl <- rowSums(local_cases[sub_idx, ])
  plot_trend(local_cases_project_ntnl_sim,
             data = data,
             multistate = FALSE,
             ylim = c(0, 2 * max(local_cases_ntnl)),
             hline_at = NULL,
             min_date = start_date,
             base_colour = green) +
    ggtitle("Projected national locally-acquired cases") +
    ylab("daily infections") +
    geom_line(data = data.frame(mean = local_cases_ntnl,
                                date = dates[sub_idx],
                                type = "Nowcast"))
  
}

# load a csv of numbers of non-household contacts by state (where the first
# column gives numberts of contacts and the rest of each row gives counts of
# respondents with that number of contacts), tidy up the names, add a date and
# convert to long format (one row per response)
load_contacts_by_state <- function(csv, date) {
  
  contacts <- read_csv(
    csv,
    col_types = list(
      .default = col_double()
    )
  ) %>%
    rename_all(
      recode,
      contact_num = "contacts",
      num_contacts = "contacts",
      "Australian Capital Territory" = "ACT",
      "New South Wales" = "NSW",
      "Northern Territory" = "NT",
      "Queensland" = "QLD",
      "South Australia" = "SA",
      "Tasmania" = "TAS",
      "Victoria" = "VIC",
      "Western Australia" = "WA"
    ) %>%
    pivot_longer(cols = -contacts,
                 names_to = "state",
                 values_to = "respondents") %>%
    mutate(date = date) %>%
    filter(state != "Other") %>%
    uncount(respondents) 
  
  contacts
  
}

# load in data from the contact surveys
contact_survey_data <- function() {
  
  # load the first set of awkward datasets and their dates
  contact_data_list <- list(
    
    # Freya's survey waves
    load_contacts_by_state(
      "data/contacts/freya_survey/contact_counts_NG_wave1.csv",
      as.Date("2020-04-04")
    ),
    
    load_contacts_by_state(
      "data/contacts/freya_survey/contact_counts_NG_wave2.csv",
      as.Date("2020-05-02")
    ),
    
    # first barometer wave
    load_contacts_by_state(
      "data/contacts/barometer/contacts_by_state.csv",
      as.Date("2020-05-27")
    )
    
  )
  
  # add on barometer survey data that comes in waves
  contact_wave_files <- list.files("data/contacts/barometer",
                                   pattern = "\\d.csv",
                                   full.names = TRUE)
  
  # find survey dates from corresponding microdistancing data
  wave_dates <- contact_wave_files %>%
    # pull out wave number
    basename() %>%
    strsplit(" ") %>%
    vapply(`[`, 4, FUN.VALUE = character(1)) %>%
    gsub(".csv", "", x = .) %>%
    # pull in file
    paste("Barometer wave", ., "compliance.csv") %>%
    file.path("data/microdistancing/", .) %>%
    lapply(read_csv) %>%
    lapply(pull, "date") %>%
    lapply(first) %>%
    do.call(c, .)
  
  barometer_data_list <- mapply(
    load_contacts_by_state,
    csv = contact_wave_files,
    date = wave_dates,
    SIMPLIFY = FALSE
  )
  
  # combine these
  do.call(
    bind_rows,
    c(contact_data_list, barometer_data_list)
  )
  
}

# load the data needed for the macrodistancing model
macrodistancing_data <- function(dates = NULL, breaks = c(0:10, 20, 50, Inf)) {
  
  # modelled change (after/before ratio) in time at types of locations from Google
  location_change_trends <- location_change(dates) %>%
    mutate_at(
      vars(public, home, retail, transit, work),
      ~replace_na(., 1)
    ) %>%
    mutate(state = abbreviate_states(state))
  
  # state-level numbers of non-household contacts by state and date from Freya's
  # survey and the BETA barometer. Remove implausible responses, from the
  # reporting clump at 999 and above (short conversation with 999 or more people
  # in a day is implausible, and probably an entry/reporting/understanding error)
  contact_data <- parse_all_surveys() %>%
    filter(
      !is.na(contact_num),
      !is.na(state)
    ) %>%
    select(
      state,
      date,
      wave,
      wave_date,
      wave_duration,
      weekend_fraction,
      age_groups,
      city,
      employment,
      contact_num,
      starts_with("contacts_")
    ) %>%
    filter(
      contact_num <= 999,
      date %in% location_change_trends$date
    )
  
  list(
    breaks = breaks,
    contacts = contact_data,
    location_change_trends = location_change_trends
  )
  
}

macrodistancing_params <- function(baseline_contact_params) {
  
  # baseline number of non-household contacts, from Prem and Rolls
  OC_0 <- normal(baseline_contact_params$mean_contacts[2],
                 baseline_contact_params$se_contacts[2],
                 truncation = c(0, Inf))
  
  # coefficients for change in average contacts as a function of mobility
  # indices (all set to be negative in first lockdown)
  mobility_coefs <- normal(0, 1, dim = 5, truncation = c(0, Inf))
  
  # coefficients for the fraction of weekly contacts that are on weekends, as a
  # function of the log diffference in contacts
  weekday_coefs <- normal(0, 1, dim = c(2, 6))

  list(
    OC_0 = OC_0,
    mobility_coefs = mobility_coefs,
    weekday_coefs = weekday_coefs
  )
  
}

# greta distribution object for the grouped negative binomial distribution
right_aggregated_negative_binomial_distribution <- R6Class(
  "right_aggregated_negative_binomial_distribution",
  inherit = greta:::distribution_node,
  public = list(
    
    max_count = NA,
    
    initialize = function(size, prob, max_count, dim) {
      
      size <- as.greta_array(size)
      prob <- as.greta_array(prob)
      
      self$max_count <- max_count
      
      # add the nodes as parents and parameters
      dim <- greta:::check_dims(size, prob, target_dim = dim)
      super$initialize(
        "right_aggregated_negative_binomial",
        dim,
        discrete = TRUE
      )
      self$add_parameter(size, "size")
      self$add_parameter(prob, "prob")
      
    },
    
    tf_distrib = function(parameters, dag) {
      
      size <- parameters$size
      prob <- parameters$prob
      
      log_prob <- function(x) {
        
        # define the unadjusted density
        distribution <- tfp$distributions$NegativeBinomial(
          total_count = size,
          probs = fl(1) - prob
        )
        
        
        # Get the adjustment to account for truncation need the integral from 0
        # to max_count. We can't use the CDF as there is no gradient w.r.t. the
        # size parameter, so we sum over the PMF in the uncensored range instead
        integers <- tf$range(fl(0), fl(self$max_count + 1))
        integer_log_densities <- distribution$log_prob(integers)
    
        # compute the log-probability of *not* exceeding max_count
        # do reduce_log_sum_exp to get log of sum over PMFs
        log_p_not_excess <- tf$reduce_logsumexp(
          integer_log_densities,
          axis = 2L,
          keepdims = TRUE
        )
        
        # compute the log-probability of being in excess
        # log(1 - exp(log_p_not_excess))
        log_p_excess <- log(-tf$math$expm1(log_p_not_excess))
          
        # get the point densities of all counts        
        log_density_integers <- distribution$log_prob(x)
        
        # bonus: extract the log probs from integer_log_densities instead of
        # recomputing
        # set the excess counts to 0 for now
        # x_clean <- tf$where(excess_count, fl(0), x)

        # find the counts exceeding max_count
        excess_count <- x > fl(self$max_count)
        
        # for these, replace the log prob with the probabiity of being in excess
        # of max_count
        log_density <- tf$where(excess_count, log_p_excess, log_density_integers)
        
        log_density
        
      }
      
      sample <- function(seed) {
        
        d <- tfp$distributions$NegativeBinomial(
          total_count = size,
          probs = fl(1) - prob
        )
        d$sample(seed = seed)
        
      }
      
      list(log_prob = log_prob, sample = sample)
      
    }
    
  )
)

right_aggregated_negative_binomial <- function(size, prob, max_count, dim = NULL) {
  greta:::distrib("right_aggregated_negative_binomial", size, prob, max_count, dim)
}

# CDF of the provided distribution, handling 0s and Infs
tf_safe_cdf <- function(x, distribution) {
  
  # prepare to handle values outside the supported range
  too_low <- tf$less(x, greta:::fl(0))
  too_high <- tf$equal(x, greta:::fl(Inf))
  supported <- !too_low & !too_high
  ones <- tf$ones_like(x)
  zeros <- tf$zeros_like(x)
  
  # run cdf on supported values, and fill others in with the appropriate value
  x_clean <- tf$where(supported, x, ones)
  cdf_clean <- distribution$cdf(x_clean)
  mask <- tf$where(supported, ones, zeros)
  add <- tf$where(too_high, ones, zeros)
  cdf_clean * mask + add
  
}

# greta distribution object for the grouped negative binomial distribution
discrete_lognormal_distribution <- R6Class(
  "discrete_lognormal_distribution",
  inherit = greta:::distribution_node,
  public = list(
    
    breaks = NA,
    lower_bounds = NA,
    upper_bounds = NA,
    
    initialize = function(meanlog, sdlog, breaks, dim) {
      
      meanlog <- as.greta_array(meanlog)
      sdlog <- as.greta_array(sdlog)
      
      # handle gradient issue between sdlog and 0s
      breaks <- pmax(breaks, .Machine$double.eps)
      self$breaks <- breaks
      self$lower_bounds <- breaks[-length(breaks)]
      self$upper_bounds <- breaks[-1]
      
      # add the nodes as parents and parameters
      dim <- greta:::check_dims(meanlog, sdlog, target_dim = dim)
      super$initialize("discrete_lognormal", dim, discrete = TRUE)
      self$add_parameter(meanlog, "meanlog")
      self$add_parameter(sdlog, "sdlog")
      
    },
    
    tf_distrib = function(parameters, dag) {
      
      meanlog <- parameters$meanlog
      sdlog <- parameters$sdlog
      
      tf_breaks <- fl(self$breaks)
      tf_lower_bounds <- fl(self$lower_bounds)
      tf_upper_bounds <- fl(self$upper_bounds)
      
      log_prob <- function(x) {
        
        # build distribution object
        d <- tfp$distributions$LogNormal(
          loc = meanlog,
          scale = sdlog
        )
        
        # for those lumped into groups, compute the bounds of the observed groups and get tensors for the
        # bounds in the format expected by TFP
        x_safe <- tf$math$maximum(x, fl(.Machine$double.eps))
        tf_idx <- tfp$stats$find_bins(x_safe, tf_breaks)
        tf_idx_int <- greta:::tf_as_integer(tf_idx)
        tf_lower_vec <- tf$gather(tf_lower_bounds, tf_idx_int)
        tf_upper_vec <- tf$gather(tf_upper_bounds, tf_idx_int)
        
        # compute the density over the observed groups
        low <- tf_safe_cdf(tf_lower_vec, d)
        up <- tf_safe_cdf(tf_upper_vec, d)
        log_density <- log(up - low)
        
      }
      
      sample <- function(seed) {
        
        d <- tfp$distributions$LogNormal(
          loc = meanlog,
          scale = sdlog
        )
        continuous <- d$sample(seed = seed)
        tf$floor(continuous)
        
      }
      
      list(log_prob = log_prob, sample = sample)
      
    }
    
  )
)

# a discretised lognormal distribution (i.e. samplesd by applying the floor
# operation to samples from a lognormal). Due to the numerical instability of
# integrating across the distribution, a vector of breaks must be defined and
# the observations will be treated as censored within those breaks
discrete_lognormal <- function(meanlog, sdlog, breaks, dim = NULL) {
  greta:::distrib("discrete_lognormal", meanlog, sdlog, breaks, dim)
}

lognormal_params <- function(mean, sd) {
  var <- sd ^ 2
  list(
       meanlog = log((mean ^ 2) / sqrt(var + mean ^ 2)),
       sdlog = sqrt(log(1 + var / (mean ^ 2)))
  )
}

# define the likelihood for the macrodistancing model
macrodistancing_likelihood <- function(predictions, data) {
  
  # pull out the expected number of non-household contacts by state and date
  all_dates <- unique(data$location_change_trends$date)
  all_states <- unique(data$location_change_trends$state)
  date_idx <- match(data$contacts$date, all_dates)
  state_idx <- match(data$contacts$state, all_states)
  idx <- cbind(date_idx, state_idx)
  
  # get expected number of contacts per respondent based on their date and state
  # (accounting for day of the week effects)
  log_predicted_contacts <- predictions$log_mean_daily_contacts_wday[idx]
  
  # get lognormal parameters from mean and standard deviation
  sdlog <- normal(0, 5, truncation = c(0, Inf))
  
  # because mean = exp(meanlog + (sdlog ^ 2) / 2)
  meanlog <- log_predicted_contacts - (sdlog ^ 2) / 2

  distribution(data$contacts$contact_num) <- discrete_lognormal(
    meanlog = meanlog,
    sdlog = sdlog,
    breaks = data$breaks
  )
  
  result <- list(
    predictions = exp(log_predicted_contacts),
    sdlog = sdlog
  )
  
  invisible(result)
  
}

# check convergence
convergence <- function(draws) {
  
  r_hats <- coda::gelman.diag(draws, autoburnin = FALSE, multivariate = FALSE)$psrf[, 1]
  n_eff <- coda::effectiveSize(draws)
  
  # sometimes n_eff underflows to 0 simply because the values beinng traced are
  # very small, so remove these (exactly 0 is not possible)
  n_eff <- n_eff[n_eff != 0]
  
  cat(sprintf("maximum R-hat: %.2f\nminimum n effective: %.2f",
              max(r_hats),
              min(n_eff)))
  
  result <- list(r_hats = r_hats,
                 n_eff = n_eff)
  
  invisible(result)
  
}

# has the sampler converged to our standards?
converged <- function(draws, max_r_hat = 1.1, min_n_eff = 1000) {
  stats <- convergence(draws)
  all(stats$r_hats < max_r_hat) &
    all(stats$n_eff >= min_n_eff)
}

# define a zero-mean hierarchical normal prior over a vector of length n
hierarchical_normal <- function(n, index = NULL, mean_sd = 10, sd_sd = 0.5) {
  
  indexed <- !is.null(index)
  hyper_n <- ifelse(indexed, max(index), 1)
  
  mean <- normal(0, mean_sd, dim = hyper_n)
  sd <- normal(0, sd_sd, truncation = c(0, Inf), dim = hyper_n)
  raw <- normal(0, 1, dim = n)
  
  if (indexed) {
    mean <- mean[index]
    sd <- sd[index]
  }
  
  mean + raw * sd
  
}

microdistancing_params <- function(n_locations = 8, n_inflections = 1, inflection_max = 1) {
  
  # share information between peaks on both timing and amplitude
  inflection_means <- normal(0, 10, dim = n_inflections)
  inflection_sds <- normal(0, 0.5, truncation = c(0, Inf), dim = n_inflections)
  inflection_raw <- normal(0, 1, dim = c(n_locations, n_inflections))
  inflection_scaled <- sweep(inflection_raw, 2, inflection_sds, FUN = "*")
  inflection_centred <- sweep(inflection_raw, 2, inflection_means, FUN = "+")
  inflections <- ilogit(inflection_centred)

  height_means <- normal(0, 10, dim = n_inflections + 1)
  height_sds <- normal(0, 0.5, truncation = c(0, Inf), dim = n_inflections + 1)
  height_raw <- normal(0, 1, dim = c(n_locations, n_inflections + 1))
  height_scaled <- sweep(height_raw, 2, height_sds, FUN = "*")
  height_centred <- sweep(height_raw, 2, height_means, FUN = "+")
  heights <- ilogit(height_centred)
  
  # order inflections between 0 and 1 and constrain to earlier than a maximum value
  inflections <- apply(inflections, 1, "cumprod")
  inflections <- 1 - t(inflections)
  inflections <- inflections * inflection_max
  
  list(
    inflections = inflections,
    heights = heights
  )
  
}

# load  all hygiene/microdistancing survey data
hygiene_data <- function () {
  parse_all_surveys()  %>%
    select(
      -starts_with("contact")
    ) %>%
    mutate(
      phys_contact = ifelse(phys_contact, "Yes", "No")
    ) %>%
    pivot_longer(
      cols = c(phys_contact, phys_distance, wash_hands, cough, face_covering),
      names_to = "question",
      values_to = "response"
    ) %>%
    mutate(
      question = recode(
        question,
        `phys_distance` = "1.5m compliance",
        `wash_hands` = "Hand washing",
        `cough` = "Cough etiquette",
        `face_covering` = "Face covering",
        `phys_contact` = "Physical contact"
      )
    ) %>%
    mutate(
      response = case_when(
        question == "1.5m compliance" &
          response %in% c("Always") ~ "yes",
        question == "1.5m compliance" &
          response %in% c("Often", "Sometimes", "Rarely", "No") ~ "no",
        TRUE ~ response) 
    ) %>%
    # recode hand washing to yes/no (whether they did it immediately afterwards)
    mutate(
      response = case_when(
        question == "Hand washing" & response != "No" ~ "yes",
        question == "Hand washing" & response == "No" ~ "no",
        TRUE ~ response) 
    ) %>%
    # recode cough etiquette to yes/no (whether they covered their mouth with anything)
    mutate(
      response = case_when(
        question == "Cough etiquette" & response != "Nothing" ~ "yes",
        question == "Cough etiquette" & response == "Nothing" ~ "no",
        TRUE ~ response) 
    ) %>%
    # recode face covering as Always or not
    mutate(
      response = case_when(
        question == "Face covering" &
          response %in% c("Always") ~ "yes",
        question == "Face covering" &
          response %in% c("Often", "Sometimes", "Rarely", "No") ~ "no",
        TRUE ~ response) 
    ) %>%
    # recode physical contact to the opposite, to reflect avoidance
    mutate(
      response = case_when(
        question == "Physical contact" & response == "No" ~ "yes",
        question == "Physical contact" & response == "Yes" ~ "no",
        TRUE ~ response) 
    ) %>%
    filter(
      !is.na(response),
      !is.na(state)
    ) %>%
    # collate responses into respondents and yeses
    group_by(state, wave_date, date, question, response) %>%
    summarise(count = n())%>%
    ungroup() %>%
    pivot_wider(
      names_from = "response",
      values_from  = "count",
    ) %>%
    mutate(
      yes = replace_na(yes, 0),
      no = replace_na(no, 0),
      respondents = yes + no
    ) %>%
    rename(count = yes) %>%
    select(-no) %>%
    mutate(proportion = count / respondents) %>%
    arrange(state, question, date)
  
}

# get data for fitting and predicting from microdistancing model
microdistancing_data <- function(dates = NULL) {
  
  # assume adoption of microdistancing follows the same trend as macrodistancing,
  # and that waning starts at the same time, but don't assume it wanes at the same
  # rate
  distancing <- readRDS("outputs/social_distancing_latent.RDS")
  
  # use dates from start of distancing to present if no others are specified
  if (is.null(dates)) {
    dates <- seq(
      min(distancing$date),
      Sys.Date(),
      by = 1
    )
  }
  
  survey <- hygiene_data() %>%
    filter(
      date %in% dates
    )
  
  # clip distancing to non-degenerate values
  range <- range(distancing$mean[!distancing$mean %in% c(0,  1)])
  
  # get data to predict to
  pred_data <- distancing %>%
    rename(distancing = mean) %>%
    mutate(
      distancing = pmax(distancing, range[1]),
      distancing = pmin(distancing, range[2])
    ) %>%
    select(date, distancing) %>%
    old_right_join(
      expand_grid(
        date = dates,
        state = unique(survey$state)
      )
    ) %>%
    mutate(
      distancing = case_when(
        is.na(distancing) & date < min(dates) ~ 0,
        is.na(distancing) & date >= min(dates) ~ 1,
        TRUE ~ distancing
      )
    ) %>%
    mutate(
      state_id = match(state, unique(state)),
      time = as.numeric(date - interventions("national")$date[3]),
      time = time / max(time)
    ) %>%
    arrange(state, date)
  
  # subset to 1.5m question and add data for modelling
  survey_distance <- survey %>%
    filter(question == "1.5m compliance") %>%
    left_join(pred_data)
  
  result <- list(survey_distance = survey_distance,
                 prediction_data = pred_data)
  
  result
  
}

# given vectors of dates and numbers of days post infection, and a single state,
# return the fraction of cases *not* being detected by that point
ttd_survival <- function(days, dates, target_state, cdfs = NULL) {
  
  # filter to this state,
  # loop through dates running ecdfs on days (accounting for change of dates from onset to infection!)
  
  # will need to line up dates, but shouldn't need to line up days_idx (ecdf()
  # will take care of it)

  # load empirical CDFs of delay from onset to notificiation (aggregated from
  # date of onset) over time
  if (is.null(cdfs)) {
    cdfs <- readRDS("outputs/delay_from_onset_cdfs.RDS")
  }
  
  # subset to this state
  cdfs <- cdfs %>%
    filter(state == target_state)
  
  # line up dates
  dates <- pmin(dates, max(cdfs$date))
  dates <- pmax(dates, min(cdfs$date))
  dates_idx <- match(dates, cdfs$date)
  
  # convert from days post infection to days post onset (can be up to 5 days
  # negative)
  days_onset <- days - 5
  days_onset_list <- lapply(days_onset, list)

  # apply relevant CDF to each number of days
  ecdfs <- cdfs$ecdf[dates_idx]
  cdf_vec <- mapply(do.call, ecdfs, days_onset_list)
  
  # return probability of not being detected by this point
  1 - cdf_vec
  
}

# returna date-by-state matrix of reduction in R due to faster detection of cases
surveillance_effect <- function(dates, states, cdf,
                                gi_bounds = c(0, 20),
                                ttd_cdfs = NULL) {
  
  n_dates <- length(dates)
  n_states <- length(states)
  gi_range <- diff(gi_bounds) + 1
  day_vec <- seq_len(gi_range) - 1 + gi_bounds[1]
  day_mat <- col(matrix(0, n_dates, gi_range)) - 1
  
  # generation interval probability on each day post-infection
  gi_days <- gi_probability(cdf, day_vec, bounds = gi_bounds)
  
  date_state_mat <- matrix(1, n_dates, n_states)
  
  for (i in seq_along(states)) {
    
    # times to detection for each date in this states
    ttd_days <- day_mat
    ttd_days[] <- ttd_survival(
      c(day_mat),
      rep(dates, gi_range),
      target_state = states[i],
      cdfs = ttd_cdfs
    )
    
    # weighted sum to get reduction due to impeded transmission
    date_state_mat[, i] <- c(ttd_days %*% gi_days)
    
  }
  
  date_state_mat
  
}


# get the mean date of symptom onset give a date of detection (using the
# time-varying time to detection distribution)
impute_one_onset <- function(confirmation_date,
                             state,
                             notification_delay_cdf,
                             method = c("expected", "random"),
                             min_days = -10,
                             max_days = 40) {
  
  method <- match.arg(method)
  
  # get possible dates of onset
  delays <- seq(min_days, max_days) 
  possible_onset_dates <- confirmation_date - delays
  
  # probability of being detected this many days later (probability of detection
  # by this day, minus probability of detection by the previous day)
  surv_from <- notification_delay_cdf(delays - 1, possible_onset_dates, state)
  surv_to <- notification_delay_cdf(delays, possible_onset_dates, state)
  prob <- surv_from - surv_to
  
  # normalise to get probabilities of different delays
  prob <- prob / sum(prob)
  
  # compute either the expected time since onset, or draw a random one
  delay <- switch(method,
                  expected = round(sum(delays * prob)),
                  random = sample(delays, 1, prob = prob))
  
  # subtract to get expected date of symptom onset
  onset_date <- confirmation_date - delay
  onset_date
  
}

impute_onsets <- function(confirmation_dates,
                          states,
                          notification_delay_cdf,
                          method = c("expected", "random"),
                          min_days = -10,
                          max_days = 40) {
  
  method <- match.arg(method)
  onset_dates <- mapply(
    impute_one_onset,
    confirmation_date = confirmation_dates,
    state = states,
    MoreArgs = list(
      notification_delay_cdf = notification_delay_cdf,
      method = method,
      min_days = min_days,
      max_days = max_days
    ),
    SIMPLIFY = FALSE
  )
  do.call(c, onset_dates)
  
}

# clean up some weird date encoding in the linelist
clean_date <- function (date, min_date = as.Date("2019-12-01"), max_date = Sys.Date()) {
  # don't use ifelse as it converts to a numeric
  # date <- original_date
  # date[weird] <- corrected_date[weird]
  
  # remove any that are out of bounds
  early <- as.Date(date) < min_date
  late <- as.Date(date) > max_date
  date[early | late] <- NA
  
  date
}

# convert a date-by-region matrix into long format 
lengthen <- function(matrix, dates, region_name, value_name) {
  matrix %>%
    as.data.frame() %>%
    bind_cols(date = dates) %>%
    pivot_longer(-date,
                 names_to = region_name,
                 values_to = value_name)
  
}

# fetch the state corresponding to each postcode
postcode_to_state <- function(postcode) {
  
  state <- case_when(
    grepl("^26", postcode) ~ "ACT",
    grepl("^2", postcode) ~ "NSW",
    grepl("^3", postcode) ~ "VIC",
    grepl("^4", postcode) ~ "QLD",
    grepl("^5", postcode) ~ "SA",
    grepl("^6", postcode) ~ "WA",
    grepl("^7", postcode) ~ "TAS",
    grepl("^08", postcode) ~ "NT",
    # QLD seems to be recording some locations as 93xx (QLD po-box or LVR?) 9399
    # is in NIR list as QLD, but 9301 is not)
    grepl("^93", postcode) ~ "QLD",
    TRUE ~ "NA"
  )
  
  state[state == "NA"] <- NA
  state
  
}

lga_to_state <- function (lga) {
  
  "data/spatial/abs/LGA19_to_STATE16.csv" %>%
    read_csv(
      col_types = cols(
        LGA_CODE_2019 = col_double(),
        LGA_NAME_2019 = col_character(),
        STATE_CODE_2016 = col_double(),
        STATE_NAME_2016 = col_character()
      )
    ) %>%
    select(
      lga = LGA_NAME_2019,
      state = STATE_NAME_2016
    ) %>%
    mutate(
      state = abbreviate_states(state)
    ) %>%
    old_right_join(
      tibble(lga = lga)
    ) %>%
    pull(state)
  
}

linelist_date_times <- function(
  dir,
  name_pattern = "^COVID-19 UoM "
) {
  # find the files
  files <- list.files(dir, pattern = ".xlsx$", full.names = TRUE)
  # pull out the date time stamp
  date_time_text <- gsub(name_pattern, "", basename(files)) 
  date_time_text <- gsub(".xlsx$", "", date_time_text)
  date_times <- as.POSIXct(date_time_text, format = "%d%b%Y %H%M")
  # return as a dataframe
  tibble::tibble(
    file = files,
    date_time = date_times
  )
}

# copy over all new NNDSS linelist files from the shared drive to an unsynced local store
sync_nndss <- function(mount_dir = "~/Mounts/nndss", storage_dir = "~/not_synced/nndss") {
  
  # mount the drive
  system("mount_nndss", ignore.stderr = TRUE)
  Sys.sleep(5)
  
  from_files <- list.files(mount_dir, full.names = TRUE)
  existing_files <- list.files(storage_dir)
  new <- !(basename(from_files) %in% existing_files)
  files_to_read <- from_files[new]
  for (new_file in files_to_read) {
    file.copy(new_file, file.path(storage_dir, basename(new_file)), )
  }
}

# read in the latest linelist and format for analysis
get_nndss_linelist <- function(
  date = NULL,
  dir = "~/not_synced/nndss",
  strict = TRUE,
  #missing_location_assumption = "imported"
  missing_location_assumption = "local"
  #missing_location_assumption = "missing"
) {
  
  data <- linelist_date_times(dir)
  
  # subset to this date
  if (!is.null(date)) {
    data <- data %>%
      filter(as.Date(date_time) == date)
  }
  
  # get the latest linelist
  data <- data %>%
    filter(date_time == max(date_time, na.rm = TRUE))
  
  col_types <- NULL
  if (strict) {
    col_types_1 <- c(
      STATE = "text",
      POSTCODE = "numeric",
      CONFIRMATION_STATUS = "numeric",
      TRUE_ONSET_DATE = "date",
      SPECIMEN_DATE = "date",
      NOTIFICATION_DATE = "date",
      NOTIFICATION_RECEIVE_DATE = "date",
      Diagnosis_Date = "date",
      AGE_AT_ONSET = "numeric",
      SEX = "numeric",
      DIED = "numeric",
      PLACE_OF_ACQUISITION = "text",
      HOSPITALISED = "numeric",
      CV_ICU = "numeric",
      CV_VENTILATED = "numeric",
      OUTBREAK_REF = "text",
      CASE_FOUND_BY = "numeric",
      CV_SYMPTOMS = "text",
      CV_OTHER_SYMPTOMS = "text",
      CV_COMORBIDITIES = "text",
      CV_OTHER_COMORBIDITIES = "text",
      CV_GESTATION = "numeric",
      CV_CLOSE_CONTACT = "numeric"
      #CV_EXPOSURE_SETTING = "numeric",
      #CV_SOURCE_INFECTION = "numeric"
    )
    
    
    col_types_2 <- c(
      STATE = "text",
      POSTCODE = "numeric",
      CONFIRMATION_STATUS = "numeric",
      TRUE_ONSET_DATE = "date",
      SPECIMEN_DATE = "date",
      NOTIFICATION_DATE = "date",
      NOTIFICATION_RECEIVE_DATE = "date",
      Diagnosis_Date = "date",
      AGE_AT_ONSET = "numeric",
      SEX = "numeric",
      DIED = "numeric",
      PLACE_OF_ACQUISITION = "text",
      HOSPITALISED = "numeric",
      CV_ICU = "numeric",
      CV_VENTILATED = "numeric",
      OUTBREAK_REF = "text",
      CASE_FOUND_BY = "numeric",
      CV_SYMPTOMS = "text",
      CV_OTHER_SYMPTOMS = "text",
      CV_COMORBIDITIES = "text",
      CV_OTHER_COMORBIDITIES = "text",
      CV_GESTATION = "numeric",
      #CV_CLOSE_CONTACT = "numeric"
      CV_EXPOSURE_SETTING = "numeric",
      CV_SOURCE_INFECTION = "numeric"
    )
  }
  
  
  ll_date <- data$date_time[[1]]
  
  
  if(ll_date < "2021-03-08"){
    col_types <- col_types_1
  } else {
    col_types <- col_types_2
  }
  
  dat <- readxl::read_xlsx(
    data$file,
    col_types = col_types
  )
  
  if(ll_date < "2021-03-08"){
    dat <- dat %>%
      mutate(
        CV_SOURCE_INFECTION = NA_real_
      )
  }
  
  
  if (is.numeric(dat$POSTCODE)) {
    dat <- dat %>%
      mutate(
        POSTCODE = sprintf("%04d", dat$POSTCODE),
        POSTCODE = ifelse(POSTCODE == "00NA", NA, POSTCODE) 
      )
  } else {
    dat <- dat %>%
      mutate(POSTCODE = NA)
  }
  
  # Remove cases without a state
  dat <- dat %>%
    filter(!is.na(STATE))
  
  # tidy up dates and parse place of acquisition to local (Australia) vs. overseas
  dat <- dat %>%
    mutate(
      TRUE_ONSET_DATE = clean_date(TRUE_ONSET_DATE),
      NOTIFICATION_RECEIVE_DATE = clean_date(NOTIFICATION_RECEIVE_DATE),
      SPECIMEN_DATE = clean_date(SPECIMEN_DATE)
    ) %>%
    mutate(
      import_status = case_when(
        # return "ERROR" if place of acquisition and cv_source_infection
        # indicate opposite import statuses
        grepl("^1101", PLACE_OF_ACQUISITION) & CV_SOURCE_INFECTION == 1 ~ "ERROR",
        !is.na(PLACE_OF_ACQUISITION) & 
          !grepl("^1101|^00038888", PLACE_OF_ACQUISITION) &
          CV_SOURCE_INFECTION == 2 ~ "ERROR",
        !is.na(PLACE_OF_ACQUISITION) & 
          !grepl("^1101|^00038888", PLACE_OF_ACQUISITION) &
          CV_SOURCE_INFECTION == 3 ~ "ERROR",
        !is.na(PLACE_OF_ACQUISITION) & 
          !grepl("^1101|^00038888", PLACE_OF_ACQUISITION) &
          CV_SOURCE_INFECTION == 4 ~ "ERROR",
        # where source known us that
        grepl("^1101", PLACE_OF_ACQUISITION) ~ "local",
        CV_SOURCE_INFECTION == 1 ~ "imported",
        CV_SOURCE_INFECTION == 2 ~ "local",
        CV_SOURCE_INFECTION == 3 ~ "local",
        CV_SOURCE_INFECTION == 4 ~ "local",
        CV_SOURCE_INFECTION == 6 ~ "local",
        CV_SOURCE_INFECTION == 7 ~ "local",
        # otherwise impute it
        CV_SOURCE_INFECTION == 5 ~ missing_location_assumption,
        grepl("^00038888", PLACE_OF_ACQUISITION) ~ missing_location_assumption,
        !is.na(PLACE_OF_ACQUISITION) ~ "imported",
        is.na(PLACE_OF_ACQUISITION) ~ missing_location_assumption,
        is.na(CV_SOURCE_INFECTION) ~ missing_location_assumption
      )#,
      # import_status = case_when(
      #   import_status == "missing" & STATE == "WA" ~ "local",
      #   import_status == "missing" & STATE != "WA" ~ "imported",
      #   TRUE ~ import_status
      # )
    ) #%>%
    # mutate(
    #   import_status = ifelse(import_status == "ERROR", "imported", import_status)
    # )
  
  # record state of acquisition, and residence
  dat <- dat %>%
    # fill in missing places of acquisition with correct code
    mutate(
      PLACE_OF_ACQUISITION = ifelse(
        is.na(PLACE_OF_ACQUISITION),
        "00038888",
        PLACE_OF_ACQUISITION)
    ) %>%
    mutate(
      postcode_of_acquisition = substr(PLACE_OF_ACQUISITION, 5, 8),
      postcode_of_residence = replace_na(POSTCODE, "8888"),
      state_of_acquisition = postcode_to_state(postcode_of_acquisition),
      state_of_residence = postcode_to_state(postcode_of_residence)
    )
  

  
  # Generate linelist data
  linelist <- dat %>%
    # notification receive date seems buggy, and is sometimes before the
    # notification date and specimen collection date
    mutate(
      date_confirmation = pmax(NOTIFICATION_RECEIVE_DATE,
                               NOTIFICATION_DATE,
                               na.rm = TRUE),
    ) %>%
    select(
      date_onset = TRUE_ONSET_DATE,
      date_detection = SPECIMEN_DATE,
      date_confirmation,
      state = STATE,
      import_status,
      postcode_of_acquisition,
      postcode_of_residence,
      state_of_acquisition,
      state_of_residence
    ) %>%
    mutate(
      report_delay = as.numeric(date_confirmation - date_onset),
      date_linelist = as.Date(data$date_time, tz = "Australia/Canberra"),
      state = as.factor(state)
    ) %>%
    
    # Remove those with onset date after confirmation date
    # only keep individuals with date of confirmation after onset date if less than 2 days (inclusive) because
    # we assume some individuals tested via contact tracing will test positive before symptom onset and therefore plausible
    # (noting that reporting delay distribution only calculated from positive differences)
    # also remove any individuals with NA for both notification and symptom onset dates
    # filter(
    #   date_confirmation >= (date_onset - 2) | is.na(date_confirmation) | is.na(date_onset)
    # ) %>%
    filter(
      !(is.na(date_confirmation) & is.na(date_onset))
    ) %>%
    mutate_at(
      vars(starts_with("date_")),
      ~as.Date(.)
    ) %>%
    # for cases missing a date of detection, assume it's the day before the date
    # of confirmation (1 days is the median and mode of this delay distribution)
    mutate(
      date_detection = case_when(
        is.na(date_detection) ~ date_confirmation - 1,
        TRUE ~ date_detection
      )
    )
  
  linelist
  
}

# replace VIC elements with VIC linelist
get_vic_linelist <- function(file) {
  
  linelist_date <- file %>%
    basename() %>%
    substr(1, 8) %>%
    as.Date(format = "%Y%m%d")
  

  file %>%
      read_csv(
        col_types = cols(
          CaseNumber = col_double(),
          DiagnosisDate = col_date(format = ""),
          SymptomsOnsetDate = col_date(format = ""),
          LGA = col_character(),
          Acquired = col_character(),
          FirstSpecimenPositiveDate = col_date(format = "")
        ),
        na = "NA"
      ) %>%
    # read_excel(
    #   col_types = c("numeric", "date", "date", "text", "text", "date"),
    #   na = "NA"
    # ) %>%
    mutate(
      date_onset = as.Date(SymptomsOnsetDate),
      date_confirmation = as.Date(DiagnosisDate),
      date_detection = clean_date(as.Date(FirstSpecimenPositiveDate)),
      state = "VIC",
      import_status = case_when(
        Acquired == "Travel overseas" ~ "imported",
        TRUE ~ "local"
      ),
      postcode_of_acquisition = "8888",
      postcode_of_residence = "8888",
      state_of_acquisition = NA,
      state_of_residence = NA,
      report_delay = NA,
      date_linelist = linelist_date
    ) %>%
    mutate(
      date_detection = case_when(
        is.na(date_detection) ~ date_confirmation - 1,
        TRUE ~ date_detection
      )
    ) %>%
    select(
      date_onset,
      date_detection,
      date_confirmation,
      state,
      import_status,
      postcode_of_acquisition,
      postcode_of_residence,
      state_of_acquisition,
      state_of_residence,
      report_delay,
      date_linelist
    )
  
  
  # 
  # file %>%
  #   read_csv(
  #     col_types = cols(
  #       PHESSID = col_double(),
  #       diagnosis_date = col_datetime(format = ""),
  #       ss_onset = col_datetime(format = ""),
  #       Localgovernmentarea = col_character(),
  #       acquired = col_character(),
  #       SPECIMEN_DATE = col_date(format = "%d/%m/%Y")
  #     ),
  #     na = "NULL"
  #   ) %>%
  #   mutate(
  #     date_onset = as.Date(ss_onset),
  #     date_confirmation = as.Date(diagnosis_date),
  #     date_detection = clean_date(SPECIMEN_DATE),
  #     state = "VIC",
  #     import_status = case_when(
  #       acquired == "Travel overseas" ~ "imported",
  #       TRUE ~ "local"
  #     ),
  #     postcode_of_acquisition = "8888",
  #     postcode_of_residence = "8888",
  #     state_of_acquisition = NA,
  #     state_of_residence = NA,
  #     report_delay = NA,
  #     date_linelist = linelist_date
  #   ) %>%
  #   # the mode and mean of the delay from testing to confirmation in VIC is around 3 days at the moment
  #   mutate(
  #     date_detection = case_when(
  #       is.na(date_detection) ~ date_confirmation - 3,
  #       TRUE ~ date_detection
  #     )
  #   ) %>%
  #   select(
  #     date_onset,
  #     date_detection,
  #     date_confirmation,
  #     state,
  #     import_status,
  #     postcode_of_acquisition,
  #     postcode_of_residence,
  #     state_of_acquisition,
  #     state_of_residence,
  #     report_delay,
  #     date_linelist
  #   )
  
  
}


get_vic_linelist_wollert <- function(file = "~/not_synced/vic/Whittlesea outbreak dates 30 May 2021.xlsx"){

  file %>%
    read_excel(
      col_names = c(
        "ID",
        "CRM",
        "infectious_date",
        "inferred_onset",
        "positive"
      ),
      col_types = c(
        "text",
        "numeric",
        "date",
        "date",
        "date"
      ),
      skip = 1
    ) %>%
    mutate(
      date_onset = as.Date(inferred_onset),
      date_confirmation = as.Date(positive),
      state = "VIC",
      import_status = "local",
      postcode_of_acquisition = "8888",
      postcode_of_residence = "8888",
      state_of_acquisition = NA,
      state_of_residence = NA,
      report_delay = NA,
      date_linelist = NA,
      date_detection = date_confirmation - 1
    ) %>%
    dplyr::select(
      date_onset,
      date_detection,
      date_confirmation,
      state,
      import_status,
      postcode_of_acquisition,
      postcode_of_residence,
      state_of_acquisition,
      state_of_residence,
      report_delay,
      date_linelist
    )

}


get_sa_linelist <- function(file = "~/not_synced/sa/SA_linelist_25July2021.csv") {
  
  # We need to time travel as SA health has given us an outdated linelist (by 1 day)
  # We'll undo this on the EpiFX end.
  t_offset <- lubridate::days(1)
  
  file %>%
    read_csv() %>%
    mutate(
      symptom_onset = lubridate::dmy(symptom_onset) + t_offset,
      notification_date = lubridate::dmy(notification_date) + t_offset,
      likely_infection_date = lubridate::dmy(likely_infection_date) + t_offset) %>%
  
  mutate(
      date_onset = case_when(!is.na(likely_infection_date) ~ likely_infection_date + 5,
                             T                             ~ symptom_onset),
      date_detection = NA,
      date_confirmation = notification_date,
      state = "SA",
      postcode_of_acquisition = "8888",
      postcode_of_residence = NA,
      state_of_acquisition = ifelse(
        import_status == "local",
        "SA",
        NA
      ),
      state_of_residence = NA,
      report_delay = NA,
      date_linelist = as.Date("2021-07-25"),
      interstate_import = FALSE
    ) %>%
    select(
      date_onset,
      date_detection,
      date_confirmation,
      state,
      import_status,
      postcode_of_acquisition,
      postcode_of_residence,
      state_of_acquisition,
      state_of_residence,
      report_delay,
      date_linelist,
      interstate_import
    ) %>%
    filter(
      import_status == "local"
    )
}


col_nsw_date <- function(type = c("short", "long")) {
  type <- match.arg(type)
  switch(
    type,
    short = col_date(format = "%Y-%m-%d"),
    long = col_date(format = "%d/%m/%Y %H:%M:%S AM")
  )
}

# get latest NSW linelist
get_nsw_linelist <- function () {
  
  files <- list.files(
    "~/not_synced/nsw",
    pattern = ".csv",
    full.names = TRUE
  )

  dates <- files %>%
    basename() %>%
    substr(1, 8) %>%
    as.Date(format = "%Y%m%d")

  latest <- which.max(dates)
  file <- files[latest]
  date <- dates[latest]


  file %>%
    read_csv(
      col_types = cols(
        .default = col_character(),
        CASE_ID = col_double(),
        EARLIEST_CONFIRMED_OR_PROBABLE = col_nsw_date(),
        SYMPTOM_ONSET_DATE = col_nsw_date(),
        CALCULATED_ONSET_DATE = col_nsw_date(),
        AGE_AT_EVENT_YEARS = col_double(),
        DATE_ISOLATION_BEGAN = col_nsw_date(),
        SETTING_OF_TRANSMISSION_DATE = col_nsw_date("long"),
        INTERVIEWED_DATE = col_nsw_date()
      )
    ) %>%
    # remove some bogus dates
    mutate(across(
      all_of(c(
        "EARLIEST_CONFIRMED_OR_PROBABLE",
        "SYMPTOM_ONSET_DATE",
        "SETTING_OF_TRANSMISSION_DATE",
        "CALCULATED_ONSET_DATE",
        "DATE_ISOLATION_BEGAN",
        "SETTING_OF_TRANSMISSION_DATE",
        "INTERVIEWED_DATE"
      )),
      clean_date
    )
    ) %>%
    # if any infection dates are after onset, or on/after confirmation, set the infection date to NA
    mutate(
      SETTING_OF_TRANSMISSION_DATE = case_when(
        SETTING_OF_TRANSMISSION_DATE > SYMPTOM_ONSET_DATE ~ as.Date(NA),
        SETTING_OF_TRANSMISSION_DATE >= EARLIEST_CONFIRMED_OR_PROBABLE ~ as.Date(NA),
        TRUE ~ SETTING_OF_TRANSMISSION_DATE
      )
    ) %>%
    mutate(
      date_onset = case_when(
        !is.na(SETTING_OF_TRANSMISSION_DATE) ~ SETTING_OF_TRANSMISSION_DATE + 5,
        TRUE ~ CALCULATED_ONSET_DATE
      ),
      date_detection = NA,
      date_confirmation = EARLIEST_CONFIRMED_OR_PROBABLE,
      state = "NSW",
      import_status = ifelse(
        PLACE_ACQUISITION == "Acquired in NSW",
        "local",
        "imported"
      ),
      postcode_of_acquisition = NA,
      postcode_of_residence = NA,
      state_of_acquisition = "NSW",
      state_of_residence = NA,
      report_delay = NA,
      date_linelist = date,
      interstate_import = FALSE
    ) %>%
    select(
      date_onset,
      date_detection,
      date_confirmation = EARLIEST_CONFIRMED_OR_PROBABLE,
      state,
      import_status,
      postcode_of_acquisition,
      postcode_of_residence,
      state_of_acquisition,
      state_of_residence,
      report_delay,
      date_linelist,
      interstate_import
    ) %>%
    arrange(
      desc(date_onset)
    )
  
  # edit for excel file  
  # file <- "~/not_synced/nsw/cases_freya.xlsx"
  # date <- as.Date("2021-07-21")
  # 
  # file %>%
  #   readxl::read_xlsx(
  #     col_types = c(
  #       "text",
  #       "date",
  #       "date",
  #       "text",
  #       "text",
  #       "text",
  #       "text",
  #       "text",
  #       "date",
  #       "text"
  #     )
  #   ) %>%
  #   distinct(.keep_all = TRUE) %>% # some duplicated rows can be id'd by project_recid
  #   mutate(
  #     EARLIEST_CONFIRMED_OR_PROBABLE = as.Date(EARLIEST_CONFIRMED_OR_PROBABLE),
  #     CALCULATED_ONSET_DATE = as.Date(CALCULATED_ONSET_DATE),
  #     SETTING_OF_TRANSMISSION_DATE = as.Date(SETTING_OF_TRANSMISSION_DATE)
  #   ) %>%
  # # if any infection dates are after onset, or on/after confirmation, set the infection date to NA
  # mutate(
  #   SETTING_OF_TRANSMISSION_DATE = case_when(
  #     SETTING_OF_TRANSMISSION_DATE >= EARLIEST_CONFIRMED_OR_PROBABLE ~ as.Date(NA),
  #     TRUE ~ SETTING_OF_TRANSMISSION_DATE
  #   )
  # ) %>%
  #   mutate(
  #     date_onset = case_when(
  #       !is.na(SETTING_OF_TRANSMISSION_DATE) ~ SETTING_OF_TRANSMISSION_DATE + 5,
  #       TRUE ~ CALCULATED_ONSET_DATE
  #     ),
  #     date_detection = NA,
  #     date_confirmation = EARLIEST_CONFIRMED_OR_PROBABLE,
  #     state = "NSW",
  #     import_status = ifelse(
  #       PLACE_ACQUISITION == "Acquired in NSW",# rethink this if goes to prime time
  #       "local",
  #       "imported"
  #     ),
  #     postcode_of_acquisition = NA,
  #     postcode_of_residence = NA,
  #     state_of_acquisition = "NSW",
  #     state_of_residence = NA,
  #     report_delay = NA,
  #     date_linelist = date#,
  #     #interstate_import = FALSE # rethink this if goes to prime time
  #   ) %>%
  #   select(
  #     date_onset,
  #     date_detection,
  #     date_confirmation = EARLIEST_CONFIRMED_OR_PROBABLE,
  #     state,
  #     import_status,
  #     postcode_of_acquisition,
  #     postcode_of_residence,
  #     state_of_acquisition,
  #     state_of_residence,
  #     report_delay,
  #     date_linelist#,
  #     #interstate_import
  #   ) %>%
  #   arrange(
  #     desc(date_onset)
  #   )
  
}

# given a date-by-state matrix of case counts by date of infection,
# corresponding vectors of dates and states, and a function got the CDF of the
# continous version of a generation interval distribution, adjust the GI
# distribution by surveillance effectiveness (fraction of cases detected and
# isolated by each day post infection) and convolve the cases to get the
# combined infectiousness in each date and state.
gi_convolution <- function(cases, dates, states, gi_cdf, gi_bounds = c(0, 20)) {
  
  n_dates <- length(dates)
  n_states <- length(states)
  if (!identical(dim(cases), c(n_dates, n_states))) {
    stop ("cases does not match dates and states", call. = FALSE)
  }
  
  convolved <- cases * 0
  for (i in seq_len(n_states)) {
    # Circulant matrices of generation interval discrete probabilities
    # use Nishiura's serial interval as a generation interval
    # gi_cdf <- nishiura_cdf()
    gi_mat <- gi_matrix(
        gi_cdf = gi_cdf,
        dates = dates,
        state = states[i],
        gi_bounds = gi_bounds
      )
    
    convolved[, i] <- gi_mat %*% cases[, i]
    
  }
  
  convolved
  
  
}

# cases of spillover (import-local transmission) in during mandatory hotel quarantine
hotel_quarantine_spillover_data <- function() {

  tibble::tribble(
    ~earliest_date, ~latest_date, ~state, ~infectee, ~information_source,
    "2020-05-01", "2020-05-14", "VIC", "quarantine security guard (Rydges Hotel)",
    "https://www.dhhs.vic.gov.au/tracking-coronavirus-victoria",
    "2020-08-03", "2020-08-08", "NSW", "quarantine security guard (Sydney Harbour Marriott Hotel)",
    "https://www.health.nsw.gov.au/news/Pages/20200818_01.aspx",
    "2020-11-01", "2020-11-14", "SA", "quarantine security guard (Peppers Waymouth Hotel)",
    "https://www.sahealth.sa.gov.au/wps/wcm/connect/public+content/sa+health+internet/about+us/news+and+media/all+media+releases/covid-19+update+15+november",
    "2020-11-27", "2020-11-30", "NSW", "quarantine domestic worker (Novotel Darling Harbour)",
    "https://www.health.nsw.gov.au/news/Pages/20201204_01.aspx",
    "2020-12-07", "2020-12-12", "NSW", "quarantine airport driver (Sydney Ground Transport, Alexandria)",
    "https://www.abc.net.au/news/2020-12-16/nsw-confirms-new-locally-acquired-coronavirus-case/12988866",
    "2020-12-01", "2020-12-13", "NSW", "unknown (link from quarantine hotel to Northern Beaches)",
    "https://www.health.nsw.gov.au/news/Pages/20201216_03.aspx",
    "2020-12-31", "2021-01-02", "QLD", "quarantine cleaner (Hotel Grand Chancellor)",
    "https://www.health.qld.gov.au/news-events/doh-media-releases/releases/public-health-alert-brisbane",
  ) %>%
    mutate_at(
      c("earliest_date", "latest_date"),
      as.Date
    )
    
}

# given a raw (unimputed) linelist, prepare all the data needed for modelling
reff_model_data <- function(
  linelist_raw = load_linelist(),
  n_weeks_ahead = 6,
  inducing_gap = 3
) {
  
  linelist_date <- max(linelist_raw$date_linelist)
  
  # load modelled google mobility data 
  mobility_data <- readRDS("outputs/google_change_trends.RDS")
  
  # compute delays from symptom onset to detection for each state over time
  notification_delay_cdf <- get_notification_delay_cdf(linelist_raw)
  
  # impute onset dates and infection dates using this
  linelist <- linelist_raw %>%
    impute_linelist(notification_delay_cdf = notification_delay_cdf)
  
  # truncate mobility data to no later than the day before the linelist (needed
  # for modelling on historic linelists) and then get the most recent date
  latest_mobility_date <- mobility_data %>%
    filter(date < linelist_date) %>%
    pull(date) %>%
    max()
  
  # get linelist date and state information
  earliest_date <- min(linelist$date)
  latest_date <- max(linelist$date)
  
  states <- sort(unique(linelist$state))
  dates <- seq(earliest_date, latest_date, by = 1)
  mobility_dates <- seq(earliest_date, latest_mobility_date, by = 1)
  
  n_states <- length(states)
  n_dates <- length(dates)
  n_extra <- as.numeric(Sys.Date() - max(dates)) + 7 * n_weeks_ahead
  date_nums <- seq_len(n_dates + n_extra)
  dates_project <- earliest_date + date_nums - 1
  n_dates_project <- n_date_nums <- length(date_nums)
  
  # build a vector of inducing points, regularly spaced over time but with one on
  # the most recent date
  inducing_date_nums <- rev(seq(n_date_nums, 1, by = -inducing_gap))
  n_inducing <- length(inducing_date_nums)
  
  # get detection probabilities for these dates and states
  detection_prob_mat <- detection_probability_matrix(
    latest_date = linelist_date - 1,
    infection_dates = dates,
    states = states,
    notification_delay_cdf = notification_delay_cdf
  )
  
  # subset to dates with reasonably high detection probabilities in some states
  detectable <- detection_prob_mat >= 0.5
  
  # the last date with infection data we include
  last_detectable_idx <- which(!apply(detectable, 1, any))[1]
  latest_infection_date <- dates[ifelse(is.na(last_detectable_idx), length(dates), last_detectable_idx)]
  
  # those infected in the state
  local_cases <- linelist %>%
    filter(!interstate_import) %>%
    infections_by_region(
      region_type = "state",
      case_type = "local"
    )
  
  # and those infected in any state, but infectious in this one
  local_cases_infectious <- linelist %>%
    infections_by_region(
      region_type = "state",
      case_type = "local"
    )

  # those imported (only considered infectious, but with a different Reff)
  imported_cases <- linelist %>%
    infections_by_region(
      region_type = "state",
      case_type = "imported"
    )
  
  # correct Reff denominator for right-truncation (infectors not yet detected) by
  # expectation (resolving divide-by-zero error)
  detection_prob_mat[] <- pmax(detection_prob_mat, 1e-6)
  local_cases_infectious_corrected <- local_cases_infectious /  detection_prob_mat
  imported_cases_corrected <- imported_cases / detection_prob_mat
  
  # disaggregate imported and local cases according to the generation interval
  # probabilities to get the expected number of infectious people in each state
  # and time
  tti_cdfs <- readRDS("outputs/isolation_cdfs.RDS")
  
  local_infectiousness <- gi_convolution(
    local_cases_infectious_corrected,
    dates = dates,
    states = states,
    gi_cdf = gi_cdf,
    ttd_cdfs = tti_cdfs
  )
  
  imported_infectiousness <- gi_convolution(
    imported_cases_corrected,
    dates = dates,
    states = states,
    gi_cdf = gi_cdf,
    ttd_cdfs = tti_cdfs
  )

  # elements to exclude due to a lack of infectiousness
  local_valid <- is.finite(local_infectiousness) & local_infectiousness > 0
  import_valid <- is.finite(imported_infectiousness) & imported_infectiousness > 0
  valid_mat <- (local_valid | import_valid) & detectable
  
  # data on quarantine spillovers (import-local infections) and imported cases
  # since mandatory hotel quarantine was implemented hotel quarantine
  n_hotel_spillovers <- nrow(hotel_quarantine_spillover_data())
  hotel_quarantine_start_date <- max(quarantine_dates()$date)
  n_hotel_cases <- sum(imported_cases[dates >= hotel_quarantine_start_date, ])
  
  # return a named, nested list of these objects
  list(
    local = list(
      cases = local_cases,
      cases_infectious = local_cases_infectious,
      infectiousness = local_infectiousness
    ),
    imported = list(
      cases = imported_cases,
      infectiousness = imported_infectiousness,
      total_hotel_cases = n_hotel_cases,
      total_hotel_spillovers = n_hotel_spillovers
    ),
    detection_prob_mat = detection_prob_mat,
    valid_mat = valid_mat,
    states = states,
    dates = list(
      infection = dates,
      infection_project = dates_project,
      onset = dates + 5,
      date_nums = date_nums,
      inducing_date_nums = inducing_date_nums,
      mobility = mobility_dates,
      earliest = earliest_date,
      latest = latest_date,
      latest_mobility = latest_mobility_date,
      latest_infection = latest_infection_date,
      latest_project = max(dates_project),
      linelist = linelist_date
    ),
    n_dates = n_dates,
    n_states = n_states,
    n_date_nums = n_date_nums,
    n_dates_project = n_dates_project,
    n_inducing =  n_inducing
  )
  
}

reff_model <- function(data) {
  
  # reduction in R due to surveillance detecting and isolating infectious people
  surveillance_reff_local_reduction <- surveillance_effect(
    dates = data$dates$infection_project,
    cdf = gi_cdf,
    states = data$states
  )
  
  extra_isolation_local_reduction <- extra_isolation_effect(
    dates = data$dates$infection_project,
    cdf = gi_cdf,
    states = data$states
  )
  
  # the reduction from R0 down to R_eff for imported cases due to different
  # quarantine measures each measure applied during a different period. Q_t is
  # R_eff_t / R0 for each time t, modelled as a monotone decreasing step function
  # over three periods with increasingly strict policies
  q_index <- case_when(
    data$dates$infection < quarantine_dates()$date[1] ~ 1,
    data$dates$infection < quarantine_dates()$date[2] ~ 2,
    TRUE ~ 3,
  )
  q_index <- c(q_index, rep(3, data$n_date_nums - data$n_dates))
  
  # q_raw <- uniform(0, 1, dim = 3)
  log_q_raw <- -exponential(1, dim = 3)
  log_q <- cumsum(log_q_raw)
  log_Qt <- log_q[q_index]
  
  # add likelihood for hotel quarantine spillovers - assume Poisson since
  # there's no reason to expect clustering with these rare events, and we'd
  # never be able to determine the number infected in each incident anyway
  expected_hotel_spillovers <- exp(log_q[3] + log(data$imported$total_hotel_cases))
  distribution(data$imported$total_hotel_spillovers) <- poisson(expected_hotel_spillovers)
  
  # The change in R_t for locally-acquired cases due to social distancing
  # behaviour, modelled as a sum of household R_t and non-household R_t
  # Non-household Reff is modelled as a function of the number of non-household
  # contacts per 24h (itself modelled from mobility data, calibrated against
  # contact surveys) and the relative transmission probability per contact,
  # inferred from surveys on micro-distancing behaviour.
  distancing_effect <- distancing_effect_model(data$dates$mobility, gi_cdf)
  
  # pull out R_t component due to distancing for locally-acquired cases, and
  # extend to correct length
  R_eff_loc_1_no_surv <- extend(distancing_effect$R_t, data$n_dates_project)
  
  # multiply by the surveillance effect
  R_eff_loc_1 <- R_eff_loc_1_no_surv * surveillance_reff_local_reduction *
    extra_isolation_local_reduction
  log_R_eff_loc_1 <- log(R_eff_loc_1)
  
  # extract R0 from this model and estimate R_t component due to quarantine for
  # overseas-acquired cases
  log_R0 <- log_R_eff_loc_1[1, 1]
  log_R_eff_imp_1 <- log_R0 + log_Qt
  R_eff_imp_1 <- exp(log_R_eff_imp_1)
  
  # hierarchical (marginal) prior sd on log(Reff12) by state 
  sigma <- normal(0, 0.5, truncation = c(0, Inf))
  sigma_state <- sigma * ones(data$n_states)
  var <- sigma ^ 2

  # hierarchical prior mean on log(Reff12) by state
  mu_prior <- log_R_eff_loc_1 - var

  # temporally correlated errors in R_eff for local cases - representing all the
  # stochastic transmission dynamics in the community, such as outbreaks in
  # communities with higher or lower tranmission rates
  # fixing the kernel variance at 1, and introducing the variance in v
  kernel_L <- rational_quadratic(
    lengthscales = lognormal(3, 1),
    variance = 1,
    alpha = lognormal(3, 1)
  )
  
  # de-centred temporally-correlated log Reff12 GP prior
  epsilon_L <- epsilon_gp(
    date_nums = data$dates$date_nums,
    n_states = data$n_states,
    inducing_date_nums = data$dates$inducing_date_nums,
    sigma_state = sigma_state,
    kernel = kernel_L
  )

  # add the prior mean back on to re-centre the posterior  
  log_R_eff_loc <- mu_prior + epsilon_L

  # expand out the Reff for locals
  log_R_eff_imp <- sweep(
    zeros(data$n_date_nums, data$n_states),
    1,
    log_R_eff_imp_1,
    FUN = "+"
  )
  
  R_eff_loc_12 <- exp(log_R_eff_loc)
  R_eff_imp_12 <- exp(log_R_eff_imp)
  
  # work out which elements to exclude (because there were no infectious people)
  valid <- which(data$valid_mat, arr.ind = TRUE)
  
  # combine everything as vectors, excluding invalid datapoints (remove invalid
  # elements here, otherwise it causes a gradient issue)
  R_eff_loc <- exp(log_R_eff_loc[1:data$n_dates, ])
  R_eff_imp <- exp(log_R_eff_imp[1:data$n_dates, ])
  new_from_loc_vec <- data$local$infectiousness[valid] * R_eff_loc[valid]
  new_from_imp_vec <- data$imported$infectiousness[valid] * R_eff_imp[valid]
  expected_infections_vec <- new_from_loc_vec + new_from_imp_vec
  
  # negative binomial likelihood for number of cases
  sqrt_inv_size <- normal(0, 0.5, truncation = c(0, Inf), dim = data$n_states)
  size <- 1 / sqrt(sqrt_inv_size[valid[, 2]])
  prob <- 1 / (1 + expected_infections_vec / size)
  
  # Account for right truncation; underreporting of recent infections which have
  # had less time to be detected. Given the number of cases N_t infected on day t
  # (that will ever be detected), the number of cases N^*_t infected on that day
  # that are known about so far is drawn from a binomial sample with probability
  # p, from the time-to-detection distribution. Since N_t is drawn from a negative
  # binomial,  N^*_t is drawn from a compound binomial/negative binomial mixture
  # distribution. Fortunately that turns out to be a negative binomial with
  # modified probability parameter (NB is poisson-gamma, so binomial-NB is
  # binomial-poisson-gamma, but binomial-poisson is poisson with rate lambda * p and gamma times a constant is gamma,
  # so it's a poisson-gamma, which is NB).
  
  # There is an average of one day from specimen collection to confirmation, and
  # the linelist covers the previous day, so the date by which they need to have
  # been detected two days prior to the linelist date.
  detection_prob_vec <- data$detection_prob_mat[valid]
  
  # Modify the probability to account for truncation. When detection_prob_vec = 1,
  # this collapses to prob
  prob_trunc <- 1 / (1 + detection_prob_vec * (1 - prob) / prob)
  
  distribution(data$local$cases[valid]) <- negative_binomial(size, prob_trunc)
  
  m <- model(expected_infections_vec)

  list(
    greta_model = m,
    greta_arrays = module(
      expected_infections_vec,
      size,
      prob_trunc,
      R_eff_loc_1,
      R_eff_imp_1,
      R_eff_loc_12,
      R_eff_imp_12,
      log_R0,
      log_q,
      distancing_effect,
      surveillance_reff_local_reduction,
      extra_isolation_local_reduction,
      log_R_eff_loc,
      log_R_eff_imp,
      epsilon_L
    )
  )
  
  # p <- distancing_effect$p
  # phi <- distancing_effect$phi
  # phi_wt <- distancing_effect$phi_wt
  # 
  # 
  # m <- model(expected_infections_vec,
  #            p)
  # 
  # list(
  #   greta_model = m,
  #   greta_arrays = module(
  #     expected_infections_vec,
  #     size,
  #     prob_trunc,
  #     R_eff_loc_1,
  #     R_eff_imp_1,
  #     R_eff_loc_12,
  #     R_eff_imp_12,
  #     log_R0,
  #     log_q,
  #     distancing_effect,
  #     surveillance_reff_local_reduction,
  #     log_R_eff_loc,
  #     log_R_eff_imp,
  #     epsilon_L
  #   ),
  #   phi,
  #   phi_wt
  # )
  
  
}

# extend (or truncate, or optionally clamp) values in rows of a matrix
# greta_array. I.e. given a matrix `x`, return another matrix with `n_rows` rows
# (by default the same as `x`), and with rows after `clamp_from` taking the
# value of row `clamp_from`. This can be used to extend a matrix, propagating
# the last value (if `n_rows` is increased), shorten a matrix (if n_rows is
# decreased), and simultaneously clamp subsequent values in the matrix at a
# fixed value.
extend <- function(x, n_rows = nrow(x), clamp_from = nrow(x)) {
  if (clamp_from > nrow(x)) {
    stop ("clamp_from must not be higher than the umber of rows in the matrix",
          call. = FALSE)
  }
  index <- seq_len(n_rows)
  clamped_index <- pmin(index, clamp_from)
  x[clamped_index, ]
}

# reff component 1 under only surveillance changes
reff_1_only_surveillance <- function(fitted_model) {
  ga <- fitted_model$greta_arrays
  log_R0 <- ga$log_R0
  reduction <- ga$surveillance_reff_local_reduction
  exp(log_R0 + log(reduction))
}

reff_1_only_extra_isolation <- function(fitted_model) {
  ga <- fitted_model$greta_arrays
  log_R0 <- ga$log_R0
  reduction <- ga$extra_isolation_local_reduction
  exp(log_R0 + log(reduction))
}

# reff component 1 if only macrodistancing had changed
reff_1_only_macro <- function(fitted_model) {
  ga <- fitted_model$greta_arrays
  baseline_surveillance_effect <- ga$surveillance_reff_local_reduction[1]
  de <- ga$distancing_effect
  infectious_days <- infectious_period(gi_cdf)
  h_t <- h_t_state(fitted_model$data$dates$mobility)
  HD_t <- de$HD_0 * h_t
  household_infections_macro <- de$HC_0 * (1 - de$p_star ^ HD_t)
  non_household_infections_macro <- de$OC_t_state * infectious_days * (1 - de$p_star ^ de$OD_0)
  hourly_infections_macro <- household_infections_macro + non_household_infections_macro
  hourly_infections_macro_extended <- extend(
    hourly_infections_macro,
    fitted_model$data$n_dates_project
  )
  hourly_infections_macro_extended * baseline_surveillance_effect
}

# reff component 1 if only macrodistancing had changed
reff_1_only_micro <- function(fitted_model) {
  ga <- fitted_model$greta_arrays
  baseline_surveillance_effect <- ga$surveillance_reff_local_reduction[1]
  de <- ga$distancing_effect
  infectious_days <- infectious_period(gi_cdf)
  household_infections_micro <- de$HC_0 * (1 - de$p_star ^ de$HD_0)
  non_household_infections_micro <- de$OC_0 * infectious_days *
    (1 - de$p_star ^ de$OD_0) * de$gamma_t_state
  hourly_infections_micro <- household_infections_micro +
    non_household_infections_micro
  hourly_infections_micro_extended <- extend(
    hourly_infections_micro,
    fitted_model$data$n_dates_project
  )
  hourly_infections_micro_extended * baseline_surveillance_effect
}


# reff component 1 if only vaccination
reff_1_vaccine_effect <- function(fitted_model, timeseries){
  
  ga <- fitted_model$greta_arrays
  
  dates <- fitted_model$data$dates$infection_project
  
  df <- full_join(
    tibble(date = dates),
    timeseries
  ) %>%
    tidyr::fill(
      overall_transmission_effect,
      .direction = "updown"
    )
  
  ote <- df$overall_transmission_effect
  
  
  baseline_surveillance_effect <- ga$surveillance_reff_local_reduction[1]
  de <- ga$distancing_effect
  infectious_days <- infectious_period(gi_cdf)
  household_infections_vacc <- de$HC_0 * (1 - de$p_star ^ de$HD_0)
  non_household_infections_vacc <- de$OC_0 * infectious_days *
    (1 - de$p_star ^ de$OD_0)
  hourly_infections_vacc <- household_infections_vacc +
    non_household_infections_vacc
  hourly_infections_vacc_extended <- extend(
    hourly_infections_vacc,
    fitted_model$data$n_dates_project
  )
  
  reff_non_vac <- hourly_infections_vacc_extended * baseline_surveillance_effect 
  
  sweep(reff_non_vac, 1, ote, "*")
  
}


vaccination_dates <- function() {
  expand_grid(
    date = c("2021-02-22"),
    state = c("ACT", "NSW", "NT", "QLD", "SA", "TAS", 
              "VIC", "WA")
  ) %>%
    mutate(
      date = as.Date(date),
      state = factor(state)
    )
}


# outputting Reff trajectories for Rob M
reff_sims <- function(fitted_model, nsim = 2000, which = "R_eff_loc_12") {
  
  ga <- fitted_model$greta_arrays[[which]]
  ga_vec <- c(ga)
  sim <- calculate(ga_vec, values = fitted_model$draws, nsim = nsim)
  
  samples <- t(sim[[1]][, , 1])
  colnames(samples) <- paste0("sim", 1:2000)
  
  tibble(
    date = rep(fitted_model$data$dates$infection_project, fitted_model$data$n_states),
    state = rep(fitted_model$data$states, each = fitted_model$data$n_dates_project),
  ) %>%
    mutate(date_onset = date + 5) %>%
    cbind(samples)
  
}

# given a vector of discrete values (e.g. logicals) representing switches
# between states, constrain those switches so that the state may not switch to
# another state unless thge run of the same value must have been at least
# 'min_run_length' long.
constrain_run_length <- function(x, min_run_length = 7) {
  
  # create an empty vector to fill and then iterate (this must be recursive, so
  # can't use e.g. slider::slide())
  result <- rep(FALSE, length(x))
  for (end in seq_along(result)) {
    
    # check whether there was a change recently
    start <- max(1, end - min_run_length) 
    idx <-  seq(start, end - 1)
    previous <- result[idx]
    recently_flipped <- !all(previous == previous[1])
    
    # if there was, then keep the same state as the last iteration. If there
    # wasn't, use the ideal value (the value in x)
    if (recently_flipped) {
      most_recent <- previous[length(previous)]
      result[end] <- most_recent
    } else {
      result[end] <- x[end]
    }
    
  }
  
  result
  
}

# function to calculate, plot, and save all the outputs (with flags for plot
# types) - pass in an optional maximum date argument
reff_plotting <- function(
  fitted_model,
  dir = "outputs",
  subdir = "figures",
  min_date = as.Date("2020-03-01"),
  #min_date = NA,
  max_date = fitted_model$data$dates$latest_mobility,
  mobility_extrapolation_rectangle = TRUE,
  projection_date = NA,
  washout_cutoff = 0,
  vaccine_timeseries = timeseries
) {
  
  if(is.na(min_date)){
    min_date <- max_date - months(6)
  }
  
  # reformat case data for plotting (C1 and C12)
  local_cases_long <- fitted_model$data$local$cases %>%
    as_tibble() %>%
    mutate(
      date = fitted_model$data$dates$infection,
    ) %>%
    pivot_longer(
      cols = -date,
      names_to = "state",
      values_to = "cases"
    )
  
  # rugplot of case counts
  case_data <- local_cases_long %>%
    filter(date >= min_date) %>%
    mutate(
      type = "Nowcast",
      height = 1
    ) %>%
    uncount(cases)
  
  case_rug <- geom_rug(
    aes(date, height),
    data = case_data,
    sides = "b",
    alpha = 0.5,
    size = 0.5,
    colour = grey(0.7)
  )
  
  # a washout for whether there are few cases (<20 in past 5 days) with a run of
  # at least 7 days (to prevent rapidly flip-flopping)
  few_case_data <- local_cases_long %>%
    full_join(
      expand_grid(
        state = fitted_model$data$states,
        date = seq(min_date, max_date, by = 1),
      )
    ) %>%
    mutate(
      cases = replace_na(cases, 0)
    ) %>%
    group_by(state)  %>%
    mutate(
      recent_count = slider::slide_int(cases, sum, .before = 13),
    ) %>%
    ungroup() %>%
    filter(date >= min_date) %>%
    mutate(
      few_cases = recent_count < washout_cutoff,
      type = "Nowcast",
      state = factor(state),
      mean = 1
    ) %>%
    # don't let the washout state change until there have been at least the
    # specified number of days in the same state
    arrange(state, date) %>%
    group_by(state) %>%
    mutate(
      washout = constrain_run_length(few_cases, 7)
    ) %>%
    ungroup()
  
  few_case_washout <- geom_ribbon(
    aes(ymin = -10, ymax = washout * 100 - 10),
    data = few_case_data,
    fill = grey(1),
    alpha = 0.5,
    colour = grey(0.9),
    linetype = 3
  )
  
  
  
  # add counterfactuals to the model object:
  # add fitted_model_extended obect because fitted_model is modified
  fitted_model_extended <- fitted_model
  # Reff for locals component 1 under
  # only micro/macro/surveillance improvements
  fitted_model_extended$greta_arrays <- c(
    fitted_model$greta_arrays,
    list(
      R_eff_loc_1_macro = reff_1_only_macro(fitted_model_extended),
      R_eff_loc_1_micro = reff_1_only_micro(fitted_model_extended),
      R_eff_loc_1_surv = reff_1_only_surveillance(fitted_model_extended),
      R_eff_loc_1_vaccine_effect = reff_1_vaccine_effect(fitted_model_extended, vaccine_timeseries),
      R_eff_loc_1_iso = reff_1_only_extra_isolation(fitted_model_extended)
    ) 
  )
  
  # flatten all relevant greta array matrices to vectors before calculating
  trajectory_types <- c(
    "R_eff_loc_1",
    "R_eff_imp_1",
    "R_eff_loc_12",
    "R_eff_imp_12",
    "epsilon_L",
    "R_eff_loc_1_micro",
    "R_eff_loc_1_macro",
    "R_eff_loc_1_surv",
    "R_eff_loc_1_vaccine_effect",
    "R_eff_loc_1_iso"
  )
  vector_list <- lapply(fitted_model_extended$greta_arrays[trajectory_types], c)
  
  # simulate from posterior for these quantities of interest
  args <- c(vector_list, list(values = fitted_model_extended$draws, nsim = 10000))
  sims <- do.call(calculate, args)
  
  # vaccine effect only
  plot_trend(sims$R_eff_loc_1_vaccine_effect[,1:fitted_model_extended$data$n_date_nums,], # clunky fix
             data = fitted_model_extended$data,
             min_date = min_date,
             max_date = max_date,
             multistate = FALSE,
             base_colour = fifo,
             projection_at = projection_date,
             ylim = c(0, 6),
             intervention_at = vaccination_dates(),
             plot_voc = TRUE
  ) + 
    ggtitle(label = "Impact of vaccination",
            subtitle = expression(R["eff"]~"if"~only~vaccination~had~occurred)) +
    ylab(expression(R["eff"]~component))
  
  save_ggplot("R_eff_1_local_vaccine_effect.png", dir, subdir, multi = FALSE)
  
  
  # microdistancing only
  plot_trend(sims$R_eff_loc_1_micro,
             data = fitted_model_extended$data,
             min_date = min_date,
             max_date = max_date,
             multistate = TRUE,
             base_colour = purple,
             projection_at = projection_date,
             plot_voc = TRUE) + 
    ggtitle(label = "Impact of micro-distancing",
            subtitle = expression(R["eff"]~"if"~only~"micro-distancing"~behaviour~had~changed)) +
    ylab(expression(R["eff"]~component))
  
  save_ggplot("R_eff_1_local_micro.png", dir, subdir)
  
  # macrodistancing only
  plot_trend(sims$R_eff_loc_1_macro,
             data = fitted_model_extended$data,
             min_date = min_date,
             max_date = max_date,
             multistate = TRUE,
             base_colour = blue,
             projection_at = projection_date,
             plot_voc = TRUE) + 
    ggtitle(label = "Impact of macro-distancing",
            subtitle = expression(R["eff"]~"if"~only~"macro-distancing"~behaviour~had~changed)) +
    ylab(expression(R["eff"]~component))
  
  save_ggplot("R_eff_1_local_macro.png", dir, subdir)
  
  # improved surveilance only
  plot_trend(sims$R_eff_loc_1_surv,
             data = fitted_model_extended$data,
             max_date = max_date,
             multistate = TRUE,
             base_colour = yellow,
             projection_at = projection_date,
             plot_voc = TRUE) + 
    ggtitle(label = "Impact of improved surveillance",
            subtitle = expression(R["eff"]~"if"~only~surveillance~effectiveness~had~changed)) +
    ylab(expression(R["eff"]~component))
  
  save_ggplot("R_eff_1_local_surv.png", dir, subdir)
  
  # extra isolation effect only
  plot_trend(sims$R_eff_loc_1_iso,
             data = fitted_model_extended$data,
             min_date = min_date,
             max_date = max_date,
             multistate = TRUE,
             base_colour = ,
             projection_at = projection_date,
             plot_voc = TRUE) + 
    ggtitle(label = "Impact contract tracing isolation",
            subtitle = expression(R["eff"]~"if"~due~to~social~distancing)) +
    ylab(expression(R["eff"]~component))
  
  save_ggplot("R_eff_1_ttiq.png", dir, subdir)
  
  # Component 1 for national / state populations
  plot_trend(sims$R_eff_loc_1,
             data = fitted_model_extended$data,
             min_date = min_date,
             max_date = max_date,
             multistate = TRUE,
             base_colour = green,
             projection_at = projection_date,
             plot_voc = TRUE) + 
    ggtitle(label = "Impact of social distancing",
            subtitle = expression(Component~of~R["eff"]~due~to~social~distancing)) +
    ylab(expression(R["eff"]~component))
  
  save_ggplot("R_eff_1_local.png", dir, subdir)
  
  plot_trend(sims$R_eff_imp_1,
             data = fitted_model_extended$data,
             min_date = min_date,
             max_date = max_date,
             multistate = FALSE,
             base_colour = orange,
             ylim = c(0, 0.4),
             intervention_at = quarantine_dates(),
             projection_at = projection_date,
             plot_voc = TRUE) + 
    ggtitle(label = "Impact of quarantine of overseas arrivals",
            subtitle = expression(Component~of~R["eff"]~due~to~quarantine~of~overseas~arrivals)) +
    ylab(expression(R["eff"]~component))
  
  save_ggplot("R_eff_1_import.png", dir, subdir, multi = FALSE)
  
  # Reff for active cases
  p <- plot_trend(sims$R_eff_loc_12,
                  data = fitted_model_extended$data,
                  min_date = min_date,
                  max_date = max_date,
                  multistate = TRUE,
                  base_colour = green,
                  ylim = c(0, 6),
                  projection_at = projection_date,
                  plot_voc = TRUE) +
    ggtitle(label = "Local to local transmission potential",
            subtitle = "Average across active cases") +
    ylab(expression(R["eff"]~from~"locally-acquired"~cases))
  
  if (mobility_extrapolation_rectangle) {
    p <- p + annotate("rect",
                      xmin = fitted_model_extended$data$dates$latest_infection,
                      xmax = fitted_model_extended$data$dates$latest_mobility,
                      ymin = -Inf,
                      ymax = Inf,
                      fill = grey(0.5), alpha = 0.1)
    
  }
  
  # add case rug plot and washout
  p <- p + few_case_washout + case_rug
  p
  
  save_ggplot("R_eff_12_local.png", dir, subdir)
  
  # component 2 (noisy error trends)
  p <- plot_trend(sims$epsilon_L,
                  data = fitted_model_extended$data,
                  min_date = min_date,
                  max_date = max_date,
                  multistate = TRUE,
                  base_colour = pink,
                  hline_at = 0,
                  projection_at = projection_date,
                  ylim = NULL,
                  plot_voc = TRUE) + 
    ggtitle(label = "Short-term variation in local to local transmission rates",
            subtitle = expression(Deviation~from~log(R["eff"])~of~"local-local"~transmission)) +
    ylab("Deviation")
  
  if (mobility_extrapolation_rectangle) {
    p <- p + annotate("rect",
                      xmin = fitted_model_extended$data$dates$latest_infection,
                      xmax = fitted_model_extended$data$dates$latest_mobility,
                      ymin = -Inf,
                      ymax = Inf,
                      fill = grey(0.5), alpha = 0.1)
    
  }
  
  # add case rug plot and washout
  p <- p + case_rug + few_case_washout
  
  p
  
  save_ggplot("R_eff_2_local.png", dir, subdir)
  
}


# given a dataframe of Reff trajectory samples for Rob M, 'soft-clamp' the Reff
# trajectories so that the log-mean of Reff is constant into the future from the target date, but the
# trajectories still vary over time (rather than hard-clamping them so the
# trajectory for each Reff is flat into the future).
soft_clamp <- function(local_samples, target_date) {
  local_samples %>%
    pivot_longer(
      cols = starts_with("sim"),
      names_to = "sim",
      values_to = "reff"
    ) %>%
    group_by(
      date,
      state
    ) %>%
    mutate(
      log_reff = log(reff),
      log_mean = mean(log_reff)
    ) %>%
    group_by(
      state
    ) %>%
    mutate(
      latest_log_mean = mean(log_reff[date == target_date]),
      adjust = ifelse(date > target_date, latest_log_mean - log_mean, 0),
      log_reff = log_reff + adjust,
      reff = exp(log_reff)
    ) %>%
    ungroup() %>%
    select(-log_reff, -log_mean, -latest_log_mean, -adjust) %>%
    pivot_wider(
      names_from = sim,
      values_from = reff
    ) 
}

# given a dataframe of Reff trajectory samples for Rob M, 'hard-clamp' the Reff
# trajectories so that the trajectory for each Reff is costant into the future
# from the target date.
hard_clamp <- function(local_samples, target_date) {
  local_samples %>%
    pivot_longer(
      cols = starts_with("sim"),
      names_to = "sim",
      values_to = "reff"
    ) %>%
    group_by(
      state,
      sim
    ) %>%
    mutate(
      target_reff = reff[date == target_date],
      reff = ifelse(date > target_date, reff[date == target_date], reff),
    ) %>%
    ungroup() %>%
    select(-target_reff) %>%
    pivot_wider(
      names_from = sim,
      values_from = reff
    )
}

# output simulations
write_reff_sims <- function(fitted_model,
                            dir = "outputs/projection",
                            write_reff_1 = TRUE,
                            write_reff_12 = TRUE) {
  
  if (write_reff_1) {
    
    reff_1 <- reff_sims(fitted_model, which = "R_eff_loc_1")
    
    reff_1 %>%
      write_csv(
        file.path(dir, "r_eff_1_local_samples.csv")
      )
    
  }
  
  if (write_reff_12) {
    
    # find the dates for clamping into the future (where 50%/95% cases so far detected)
    clip_idx_50 <- (fitted_model$data$detection_prob_mat > 0.5) %>%
      apply(1, all) %>%
      which() %>%
      max()
    
    clip_idx_95 <- (fitted_model$data$detection_prob_mat > 0.95) %>%
      apply(1, all) %>%
      which() %>%
      max()
    
    date_50 <- fitted_model$data$dates$infection[clip_idx_50]
    date_95 <- fitted_model$data$dates$infection[clip_idx_95]
    
    reff_12 <- reff_sims(fitted_model, which = "R_eff_loc_12")
    
    reff_12 %>%
      write_csv(
        file.path(dir, "r_eff_12_local_samples.csv")
      )
    
    reff_12 %>%
      soft_clamp(date_50) %>%
      write_csv(
        file.path(dir, "r_eff_12_local_samples_soft_clamped_50.csv")
      )
    
    reff_12 %>%
      soft_clamp(date_95) %>%
      write_csv(
        file.path(dir, "r_eff_12_local_samples_soft_clamped_95.csv")
      )
  }
  
}

fit_reff_model <- function(data, max_tries = 3, iterations_per_step = 2000) {
  
  # build the greta model
  model_output <- reff_model(data)
  greta_arrays <- model_output$greta_arrays
  greta_model <- model_output$greta_model
  
  # first pass at model fitting  
  draws <- mcmc(
    greta_model,
    sampler = hmc(Lmin = 25, Lmax = 30),
    chains = 10,
    warmup = 500,
    n_samples = 2000,
    one_by_one = TRUE
  )
  
  # if it did not converge, try extending it a bunch more times
  finished <- converged(draws)
  tries <- 0
  while(!finished & tries < max_tries) {
    draws <- extra_samples(
      draws,
      iterations_per_step,
      one_by_one = TRUE
    )
    tries <- tries + 1
    finished <- converged(draws)
  }
  
  # warn if we timed out before converging successfully
  if (tries == max_tries) {
    warning("sampling did not converge according to benchmarks")
  }
  
  # return a fitted model object
  module(greta_model, greta_arrays, data, draws)
  
}

write_reff_key_dates <- function(model_data, dir = "outputs/") {
  # save these dates for Freya and Rob to check
  tibble(
    linelist_date = model_data$dates$linelist,
    latest_infection_date = model_data$dates$latest_infection,
    latest_reff_date = model_data$dates$latest_mobility,
    forecast_reff_change_date = model_data$dates$latest_mobility + 1
  ) %>%
    write_csv(
      file.path(dir, "output_dates.csv")
    )
}

# save local case data, dates, and detection probabilities for Robs
write_local_cases <- function(model_data, dir = "outputs") {
  
  tibble::tibble(
    date_onset = rep(model_data$dates$onset, model_data$n_states),
    detection_probability = as.vector(model_data$detection_prob_mat),
    state = rep(model_data$states, each = model_data$n_dates),
    count = as.vector(model_data$local$cases_infectious),
    acquired_in_state = as.vector(model_data$local$cases)
  ) %>%
    write.csv(
      file.path(dir, "local_cases_input.csv"),
      row.names = FALSE
    )
  
}

# convert any new linelist files into formatted case data in past_cases
update_past_cases <- function(past_cases_dir = "outputs/past_cases") {
  
  # linelists already processed
  past_cases_files <- list.files(past_cases_dir, pattern = ".csv$") %>%
    tibble(file = .) %>%
    mutate(
      date = gsub("local_cases_input_", "", file),
      date = gsub(".csv", "", date),
      date = as.Date(date)
    )
  
  # find all nndss linelists that haven't yet been processed (using only the latest on each date)
  linelist_files <- linelist_date_times("~/not_synced/nndss") %>%
    mutate(
      date = as.Date(date_time),
      file = basename(file)
    ) %>%
    group_by(date) %>%
    filter(
      date_time == max(date_time)
    ) %>%
    ungroup() %>%
    anti_join(
      past_cases_files,
      by = "date"
    )
  
  # subset to only Wednesdays
  # mutate(wday = lubridate::wday(date, label = TRUE)) %>%
  # filter(wday == "Wed")
  
  linelists <- linelist_files %>%
    pull(date) %>%
    lapply(load_linelist)
  
  # format and write these out
  for (linelist in linelists) {
    
    model_data <- reff_model_data(linelist)
    
    linelist_date <- model_data$dates$linelist
    
    message("processing linelist: ", linelist_date)
    
    tibble::tibble(
      date_onset = rep(model_data$dates$onset, model_data$n_states),
      detection_probability = as.vector(model_data$detection_prob_mat),
      state = rep(model_data$states, each = model_data$n_dates),
      count = as.vector(model_data$local$cases_infectious),
      acquired_in_state = as.vector(model_data$local$cases)
    ) %>%
      write.csv(
        paste0(past_cases_dir, "/local_cases_input_",
               format(linelist_date, format = "%Y-%m-%d"), ".csv"),
        row.names = FALSE)
    
  }
  
}

# plot visual checks of model posterior calibration against observed data
plot_reff_checks <- function(fitted_model, nsim = 10000) {
  
  cases <- negative_binomial(
    fitted_model$greta_arrays$size,
    fitted_model$greta_arrays$prob_trunc
  )
  cases_sim <- calculate(cases, values = fitted_model$draws, nsim = nsim)[[1]][, , 1]
  
  valid <- which(fitted_model$data$valid_mat, arr.ind = TRUE)
  observed <- fitted_model$data$local$cases[valid]
  
  # overall PPC check
  bayesplot::ppc_ecdf_overlay(
    observed,
    cases_sim[1:1000, ],
    discrete = TRUE
  )
  
  # check by state and time
  plot_fit(observed, cases_sim, fitted_model$data)
  
  # check simulation fit
  check_projection(fitted_model)
  
}

# split the dates and states into periods  with similar notification delay distributions
notification_delay_group <- function(date_confirmation, state) {
  
  stage <- case_when(
    date_confirmation < as.Date("2020-06-14") ~ 1,
    date_confirmation < as.Date("2020-08-01") ~ 2,
    date_confirmation < as.Date("2020-08-21") ~ 3,
    TRUE ~ 4,
  )
  
  group <- case_when(
    stage == 1 ~ "all states (start-Jun13)",
    stage == 2 & state == "VIC" ~ "VIC 1 (Jun14-Jul31)",
    stage == 3 & state == "VIC" ~ "VIC 2 (Aug1-Aug20)",
    stage == 4 & state == "VIC" ~ "VIC 3 (Aug21-now)",
    TRUE ~ "other states Jun14-now"
  )
  
  group
  
}

# return a function to get the CDf of the notification delay distribution for a
# given date and state
get_notification_delay_cdf <- function(linelist) {
  
  delay_data <- linelist %>%
    filter(
      !is.na(date_onset),
      date_confirmation <= (date_linelist - 3)
    ) %>%
    select(
      date_onset,
      date_confirmation,
      state,
      import_status
    ) %>%
    mutate(
      delay = as.numeric(date_confirmation - date_onset),
      group = notification_delay_group(date_confirmation, state)
    ) %>%
    filter(
      delay <= 6 * 7
    ) %>%
    group_by(group) %>%
    mutate(
      lower = quantile(delay, 0.005),
      upper = quantile(delay, 0.995)
    ) %>%
    filter(
      delay >= lower,
      delay <= upper
    )
  
  # get an ecdf for each group
  ecdfs <- delay_data %>%
    mutate(
      ecdf = list(ecdf(delay)),
      id = row_number()
    ) %>%
    filter(id == 1) %>%
    select(group, ecdf)
  
  # return a function to compute the CDF of the delay distribution for that
  # state and those delays and dates
  function(delays, possible_onset_dates, states) {
    
    group <- notification_delay_group(possible_onset_dates, states)
    idx <- match(group, ecdfs$group)
    idx <- replace_na(idx, 1)
    
    cdfs <- ecdfs$ecdf[idx]
    probs <- rep(0, length(group))
    
    for(i in seq_along(group)) {
      probs[i] <- cdfs[[i]](delays[i])
    }
    
    probs
    
  }
  
}

# return a date-by-state matrix of detection probabilities
detection_probability_matrix <- function(latest_date, infection_dates, states, notification_delay_cdf) {
  
  n_dates <- length(infection_dates)
  n_states <- length(states)
  onset_dates <- infection_dates + 5
  delays <- latest_date - onset_dates
  
  onset_dates_mat <- matrix(
    onset_dates, 
    nrow = n_dates,
    ncol = n_states
  )
  
  delays_mat <- matrix(
    delays, 
    nrow = n_dates,
    ncol = n_states
  )
  
  states_mat <- matrix(
    states,
    nrow = n_dates,
    ncol = n_states,
    byrow = TRUE
  )
  
  # get the detection probability matrix
  detection_prob_mat <- delays_mat * 0
  detection_prob_mat[] <- notification_delay_cdf(
    delays = delays_mat,
    possible_onset_dates = onset_dates_mat,
    states = states_mat
  )
  
  detection_prob_mat
  
}

impute_linelist <- function(linelist, notification_delay_cdf) {
  
  # impute the onset dates (only 0.6% of cases) using expected value from time to
  # detection distribution. Do this outside dplyr to avoid duplicating slow computations
  missing_onset <- is.na(linelist$date_onset)
  imputed_onsets <- impute_onsets(
    linelist$date_confirmation[missing_onset],
    linelist$state[missing_onset],
    notification_delay_cdf,
    method = "random"
  )
  linelist$date_onset[missing_onset] <- imputed_onsets
  
  linelist %>%
    mutate(date = date_onset - 5)
  
}

load_nndss <- function () {
  get_nndss_linelist() %>%
    impute_linelist()
} # possibly deprecated?

load_vic <- function (file) {
  get_vic_linelist(file) %>%
    impute_linelist()
} # possibly deprecated?

load_linelist <- function(date = NULL,
                          use_vic = FALSE,
                          use_sa = TRUE,
                          use_nsw = TRUE) {
  
  # load the latest NNDSS linelist (either the latest or specified file)
  linelist <- get_nndss_linelist(date = date)
  
  # optionally replace VIC data with DHHS direct upload
  if (use_vic) {
    
    vic_linelist <- linelist$date_linelist[1] %>%
      format(format = "%Y%m%d") %>%
      paste0("~/not_synced/vic/", ., "_linelist_reff.csv") %>%
      get_vic_linelist()
    
    linelist <- linelist %>%
      filter(state != "VIC") %>%
      bind_rows(vic_linelist)
    
    # ll_date <- max(linelist$date_linelist)
    # 
    # vic_linelist <- get_vic_linelist() %>%
    #   mutate(date_linelist = ll_date) %>%
    #   dplyr::select(
    #     -postcode_of_acquisition,
    #     -postcode_of_residence,
    #     -state_of_acquisition,
    #     -state_of_residence,
    #     - report_delay
    #   )
    # 
    # vic_nndss <- linelist %>%
    #   filter(
    #     (
    #       state == "VIC" &
    #         date_confirmation > as.Date("2021-05-20") &
    #         import_status == "local"
    #     )
    #   ) #%>%
    #   # dplyr::select(
    #   #   -postcode_of_acquisition,
    #   #   -postcode_of_residence,
    #   #   -state_of_acquisition,
    #   #   -state_of_residence,
    #   #   - report_delay
    #   # )
    # 
    # 
    # nndss_match <- semi_join(
    #   x = vic_nndss %>%
    #     group_by(
    #       date_onset,
    #       date_confirmation
    #     ) %>%
    #     mutate(id = row_number()),
    #   y = vic_linelist %>%
    #     group_by(
    #       date_onset,
    #       date_confirmation
    #     ) %>%
    #     mutate(id = row_number()),
    #   by = c(
    #     "date_onset",
    #     "date_confirmation",
    #     "id"
    #   )
    # )
    # 
    # vic_match <- semi_join(
    #   x = vic_linelist %>%
    #     group_by(
    #       date_onset,
    #       date_confirmation
    #     ) %>%
    #     mutate(id = row_number()),
    #   y = vic_nndss %>%
    #     group_by(
    #       date_onset,
    #       date_confirmation
    #     ) %>%
    #     mutate(id = row_number()),
    #   by = c(
    #     "date_onset",
    #     "date_confirmation",
    #     "id"
    #   )
    # )
    # 
    # 
    # nndss_anti <- anti_join(
    #   x = vic_nndss %>%
    #     group_by(
    #       date_onset,
    #       date_confirmation
    #     ) %>%
    #     mutate(id = row_number()),
    #   y = vic_linelist %>%
    #     group_by(
    #       date_onset,
    #       date_confirmation
    #     ) %>%
    #     mutate(id = row_number()),
    #   by = c(
    #     "date_onset",
    #     "date_confirmation",
    #     "id"
    #   )
    # )
    # 
    # nndss_anti_onset <- nndss_anti %>%
    #   filter(!is.na(date_onset)) %>%
    #   ungroup
    # 
    # nndss_anti_na <- nndss_anti %>%
    #   filter(is.na(date_onset)) %>%
    #   ungroup %>%
    #   group_by(date_confirmation) %>%
    #   mutate(id = row_number())
    #   
    # 
    # vic_anti <- anti_join(
    #   x = vic_linelist %>%
    #     group_by(
    #       date_onset,
    #       date_confirmation
    #     ) %>%
    #     mutate(id = row_number()),
    #   y = vic_nndss %>%
    #     group_by(
    #       date_onset,
    #       date_confirmation
    #     ) %>%
    #     mutate(id = row_number()),
    #   by = c(
    #     "date_onset",
    #     "date_confirmation",
    #     "id"
    #   )
    # ) %>%
    #   ungroup %>%
    #   group_by(date_confirmation) %>%
    #   arrange(date_detection, date_onset) %>%
    #   mutate(id = row_number()) %>%
    #   ungroup
    # 
    # nndss_splice <- left_join(
    #   nndss_anti_na,
    #   vic_anti %>%
    #     select(date_onset, date_confirmation, id),
    #   by = c(
    #     "date_confirmation",
    #     "id"
    #   )
    # ) %>%
    #   mutate(
    #     date_onset = date_onset.y
    #   )%>%
    #   select(
    #     date_onset,
    #     date_detection,
    #     date_confirmation,
    #     state,
    #     import_status,
    #     postcode_of_acquisition,
    #     postcode_of_residence,
    #     state_of_acquisition,
    #     state_of_residence,
    #     report_delay,
    #     date_linelist
    #   )
    # 
    # 
    # vic_check <- anti_join(
    #   vic_anti %>%
    #     select(date_onset, date_confirmation, id),
    #   nndss_anti_na,
    #   by = c(
    #     "date_confirmation",
    #     "id"
    #   )
    # ) # leftover from vic dataset that are not matchable to nndss
    # 
    # vic_spliced <- bind_rows(
    #   nndss_match,
    #   nndss_anti_onset,
    #   nndss_splice
    # )
    # 
    # 
    # linelist <- linelist %>%
    #   filter(
    #     !(state == "VIC" &
    #         date_confirmation > as.Date("2021-05-20") &
    #         import_status == "local")
    #   ) %>%
    #   bind_rows(
    #     vic_spliced
    #   )
    
  }
  
  if (use_sa) {
    
    linelist <- linelist %>%
      filter(
        !(state == "SA" &
            import_status == "local" &
            date_detection >= as.Date("2021-07-19") & 
            date_detection < as.Date("2021-07-26")
          )
      ) %>%
      
      bind_rows(
        get_sa_linelist()
      )
     
  }
  
  if (use_nsw) {
    
    nsw_ll <- get_nsw_linelist()
    nsw_ll_date <- nsw_ll$date_linelist[1]
    nsw_ll_start <- min(nsw_ll$date_confirmation)
    
    
    linelist <- linelist %>%
      filter(
        !(state == "NSW" &
            import_status == "local" &
            date_detection >= nsw_ll_start & 
            date_detection <= nsw_ll_date
        )
      ) %>%
      
      bind_rows(
        nsw_ll
      )
  }
  
  # flag whether each case is an interstate import
  linelist <- linelist %>%
    mutate(
      interstate_import = case_when(
        state != state_of_acquisition ~ TRUE,
        TRUE ~ FALSE
      )
    )
  
  linelist
  
}

write_linelist <- function(dir = "outputs"){
  
  linelist <- load_linelist()
  
  ll_date <- linelist$date_linelist[1]
  
  write.csv(
    x = linelist,
    file = sprintf(
      "%s/interim_linelist_%s.csv",
      dir,
      ll_date
    ),
    row.names = FALSE
  )
  
}

# convert imputed linelist into matrix of new infections by date and state
infections_by_region <- function(linelist,
                                 region_type = c("state", "postcode_of_residence"),
                                 case_type = c("local", "imported", "both"),
                                 from = min(linelist$date),
                                 to = max(linelist$date)) {
  
  region_type <- match.arg(region_type)
  case_type <- match.arg(case_type)
  
  # get full range of dates (do this before dropping rows)
  dates <- seq(from, to, by = 1)
  
  # drop unneeded rows (and regions)
  if (case_type != "both") {
    linelist <- linelist %>%
      filter(import_status == case_type)
  }
  
  regions <- unique(linelist[[region_type]])
  
  # pad this with full set of dates, states, and import statuses
  grid <- expand_grid(
    date = dates,
    region = regions
  )
  
  # widen into matrices of date by state
  new_infections <- linelist %>%
    mutate(cases = 1) %>%
    rename(region = !!region_type) %>%
    old_right_join(grid) %>%
    group_by(region, date) %>%
    summarise(cases = sum(cases, na.rm = TRUE)) %>%
    ungroup() %>%
    pivot_wider(names_from = region, values_from = cases) %>%
    select(-date) %>%
    as.matrix()
  
  new_infections
  
}

# aggregate infections and infectiousness for a given case type at lga level
lga_infections <- function(linelist, dates, gi_mat, case_type = c("local", "imported")) {
  
  case_type <- match.arg(case_type)
  
  # aggregate locally-acquired cases by postcode and date
  postcode_matrix <- linelist %>%
    infections_by_region(
      region_type = "postcode",
      case_type = case_type,
      from = min(dates),
      to = max(dates)
    )
  
  postcodes <- colnames(postcode_matrix)
  
  # read in postcode-lga lookup and weights
  weights_tbl <- read_xlsx(
    "data/spatial/abs/CA_POSTCODE_2018_LGA_2018.xlsx",
    sheet = 4,
    skip = 5
  ) %>%
    filter(
      row_number() > 1,
      !is.na(RATIO)
    ) %>%
    select(
      postcode = POSTCODE_2018...1,
      lga = LGA_NAME_2018,
      lga_code = LGA_CODE_2018,
      weight = RATIO
    ) %>%
    # subset to observed postcodes
    old_right_join(
      tibble(
        postcode = postcodes,
      )
    ) %>%
    # assign unrecognised/unknown/overseas postcodes to a separate class
    mutate(
      lga = replace_na(lga, "other"),
      weight = replace_na(weight, 1)
    )
  
  # convert to matrix for weighting
  weights_matrix <- weights_tbl %>%
    select(-lga_code) %>%
    pivot_wider(
      names_from = lga,
      values_from = "weight",
      values_fill = list(weight = 0)
    ) %>%
    select(-postcode) %>%
    as.matrix() %>%
    `rownames<-`(postcodes)
  
  # normalise so we don't lose any cases
  weights_matrix <- sweep(weights_matrix, 1, rowSums(weights_matrix), FUN = "/")
  
  # aggregate cases to lga level
  lga_matrix <- postcode_matrix %*% weights_matrix
  lga <- lengthen(lga_matrix,
                  dates,
                  "lga",
                  "infections")
  
  # get infectiousness of these cases
  postcode_infectious_matrix <- gi_mat %*% postcode_matrix
  lga_infectious_matrix <- postcode_infectious_matrix %*% weights_matrix
  lga_infectious <- lengthen(lga_infectious_matrix,
                             dates,
                             "lga",
                             "infectiousness")
  
  # convert both to long form and combine
  lga_long <- lga %>%
    left_join(
      lga_infectious
    ) %>%
    # remove empty entries
    filter(
      infections > 0 | infectiousness > 0
    ) %>%
    # add LGA codes
    left_join(
      weights_tbl %>%
        select(lga, lga_code) %>%
        filter(!duplicated(.))
    ) %>%
    mutate(
      state = lga_to_state(lga)
    ) %>%
    arrange(state, lga, date)
  
  lga_long
  
}


# function to get a greta array forecasting numbers of locally-acquired cases
# in each state into the future.

# local cases and imported cases should be matrices containing integer (or
# fractional) numbers of observed or assumed cases infected on each date.
# Reff_locals and Reff_imports should be either matrices or 2D greta arrays of
# transmission potential for locally-acquired and imported cases. All four of
# these arguments must have the same number of columns. Reff_locals and
# Reff_imports must have the same number of rows, which should be greater than
# or equal to the numbers of rows in local_cases and imported_cases. Where
# local_cases and imported_cases have fewer rows than the other matrices, they
# will be padded with zeros to represent an assumption of no imported cases or
# other local cases injected into the local population. dates must be a vector
# of dates with as many elements as rows in the Reff matrices, and gi_cdf must
# be a function returning the continuous version of the generation interval
# distribution.

# the function first computes the number of *primary* local cases - those
# infected by imported cases - and then uses a discrete convolution to compute
# the expected number of secondary local infections (local-local) into the
# future. Note this a deterministic simulation of real-valued case counts, so it
# is impossible for a simulated outbreak to go extinct, and a case count of
# close to 0 cases will inevitably lead to a large outbreak if Reff exceeds 1.
forecast_locals <- function (local_cases, imported_cases,
                             Reff_locals, Reff_imports,
                             dates, gi_cdf,
                             simulation_start = dates[nrow(local_cases)],
                             gi_bounds = c(0, 20)) {
  
  n_dates <- length(dates)
  n_states <- ncol(Reff_locals)
  states <- c("ACT", "NSW", "NT", "QLD", "SA", "TAS", "VIC", "WA")
  
  # check inputs
  if (nrow(Reff_locals) != n_dates |
      nrow(Reff_imports) != n_dates) {
    stop("Reff_locals and Reff_imports must have the same number of rows ",
         "as there are elements in dates")
  }
  
  if (ncol(Reff_imports) != n_states |
      ncol(local_cases) != n_states |
      ncol(imported_cases) != n_states) {
    stop("all input matrices must have the same number of columns")
  }
  
  if (nrow(local_cases) > n_dates |
      nrow(imported_cases) > n_dates) {
    stop("local_cases and imported_cases must fewer or equal numbers ",
         "of rows to the Reff matrices")
  }
  
  # pad the cases data if needed
  local_cases <- pad_cases_matrix(local_cases, n_dates, which = "after")
  imported_cases <- pad_cases_matrix(imported_cases, n_dates, which = "after")
  
  # create the generation interval matrix and vector
  # gi_mat <- gi_matrix(gi_cdf, dates, gi_bounds = gi_bounds)
  gi_vec <- gi_vector(gi_cdf, max(dates), gi_bounds = gi_bounds)
  
  # infectiousness of imported cases over time
  imported_infectious <- gi_convolution(
    cases = imported_cases,
    dates = dates,
    states = states,
    gi_cdf = gi_cdf,
    gi_bounds = gi_bounds
  )
  
  # expected number of primary (import-local) locally-acquired cases
  primary_local_cases <- imported_infectious * Reff_imports
  
  # infectiousness of primary locally-acquired cases
  primary_local_infectiousness <- gi_convolution(
    cases = primary_local_cases,
    dates = dates,
    states = states,
    gi_cdf = gi_cdf,
    gi_bounds = gi_bounds
  )
  
  # infectiousness of observed (or assumed) locally-acquired cases
  existing_local_infectiousness <- gi_convolution(
    cases = local_cases,
    dates = dates,
    states = states,
    gi_cdf = gi_cdf,
    gi_bounds = gi_bounds
  )
  
  # sum get local infectiousness not caused by dynamic cases
  local_infectiousness <- existing_local_infectiousness +
    primary_local_infectiousness
  
  # work out where to simulate from
  start_idx <- match(simulation_start, dates)
  sim_idx <- seq(start_idx, n_dates, by = 1)
  
  # simulate the expected numbers of secondary local cases
  secondary_local_cases <- project_local_cases(
    infectiousness = local_infectiousness[sim_idx, ],
    R_local = Reff_locals[sim_idx, ],
    disaggregation_probs = gi_vec
  )
  
  # pad the result with 0s to represent simulated cases
  secondary_local_cases <- pad_cases_matrix(secondary_local_cases,
                                            n_dates,
                                            "before")
  
  # matrix of just the primary local cases after the observed cases
  new_primary_local_cases <- zeros(n_dates, n_states)
  new_primary_local_cases[sim_idx, ] <- primary_local_cases[sim_idx, ]
  
  # combine with primary local cases to get the total number of new
  # locally-acquired cases
  forecast_local_cases <- local_cases + new_primary_local_cases + secondary_local_cases
  
  # get the probability (poisson assumption) of one for more new
  # locally-acquired cases
  forecast_local_infectious <- gi_convolution(
    cases = forecast_local_cases,
    dates = dates,
    states = states,
    gi_cdf = gi_cdf,
    gi_bounds = gi_bounds
  )
  
  expected_transmission <- forecast_local_infectious * Reff_locals +
    primary_local_cases
  p_cases <- 1 - exp(-expected_transmission)
  
  list(
    local_cases = forecast_local_cases,
    secondary_local_cases = secondary_local_cases,
    probability_of_cases = p_cases
  )
  
}

pad_cases_matrix <- function(cases, n_dates, which = c("after", "before")) {
  
  which <- match.arg(which)
  
  if (nrow(cases) < n_dates) {
    
    pad <- matrix(0,
                  nrow = n_dates - nrow(cases),
                  ncol = ncol(cases))
    
    if (inherits(cases, "greta_array")) {
      pad <- as_data(pad)
    }
    
    cases <- switch(which,
                    before = rbind(pad, cases),
                    after = rbind(cases, pad))
  }
  
  cases
  
}

# build a convolution matrix for the discrete generation interval for a single
# state, applying the effect of improving surveillance and normalising to
# integrate to 1
gi_matrix <- function(gi_cdf, dates, state,
                      gi_bounds = c(0, 20),
                      ttd_cdfs = NULL) {
  
  n_dates <- length(dates)
  
  # baseline GI matrix, without effects of improved surveillance
  day_diff <- time_difference_matrix(n_dates)
  gi_mat_naive <- gi_probability(gi_cdf, day_diff)
  
  # compute fraction surviving without detection in circulant matrix format,
  # multiply by GI matrix and rescale to get new GI distribution on each day
  ttd_mat <- day_diff
  ttd_mat[] <- ttd_survival(
    days = c(day_diff),
    dates = rep(dates, each = n_dates),
    target_state = state,
    cdfs = ttd_cdfs
  )
  scaling <- surveillance_effect(
    dates = dates,
    cdf = gi_cdf,
    state = state,
    gi_bounds = gi_bounds,
    ttd_cdfs = ttd_cdfs
  )
  rel_gi_mat <- gi_mat_naive * ttd_mat
  gi_mat <- sweep(rel_gi_mat, 2, scaling, FUN = "/")
  
  gi_mat
  
}

# build a vector of discrete generation interval probability masses for a given
# date, applying the effect of improving surveillance and normalising to
# integrate to 1
gi_vector <- function(gi_cdf, date, state,
                      gi_bounds = c(0, 20),
                      ttd_cdfs = NULL) {
  
  # baseline GI vector, without effects of improved surveillance
  days <- seq(gi_bounds[1], gi_bounds[2])
  gi_vec_naive <- gi_probability(gi_cdf, days = days, bounds = gi_bounds)
  
  # compute fraction surviving without detection in circulant matrix format,
  # multiply by GI matrix and rescale to get new GI distribution on each day
  ttd_vec <- ttd_survival(
    days = days,
    dates = rep(date, each = length(days)),
    target_state = state,
    cdfs = ttd_cdfs
  )
  scaling <- surveillance_effect(
    dates = date,
    cdf = gi_cdf,
    state = state,
    gi_bounds = gi_bounds,
    ttd_cdfs = ttd_cdfs
  )
  rel_gi_vec <- gi_vec_naive * ttd_vec
  gi_vec <- rel_gi_vec / scaling
  
  gi_vec
  
}

save_ggplot <- function (filename,
                         dir = "outputs",
                         subdir = "figures",
                         multi = TRUE,
                         width = 11.69 / 2,
                         height = 8.27 / 3,
                         scale = 1,
                         dpi = 150) {
  
  # david does 8.27 x 11.69 (landscape A4) for 3x2 panels
  # aspect ratio of 0.707:1 h:w
  # want A4 *portrait* width (8.27) with same aspect ratio
  
  if (multi) {
    
    # work out dimensions for 4x2 panels for reports
    ratio <- height / width
    mfrow <- c(4, 2)
    width <- height * 3
    height <- (width / mfrow[2]) * ratio * mfrow[1] * 1.2
    scale <- 0.95 * scale
    
  } else {
    
    height <- height * 1.25
    
  }
  
  path <- file.path(dir, subdir, filename)
  ggsave(path,
         width = width,
         height = height,
         scale = scale,
         dpi = dpi,
         bg = 'white')
  
}

# prep a spatial layer with Victorian LGAs and their populations
prep_state_lgas <- function(
  state = "Victoria",
  out_dir = "data/spatial"
) {
  
  state_short <- abbreviate_states(state)
  filepath <- file.path(out_dir,
                        paste0(
                          tolower(state_short),
                          "_lga.RDS"
                        ))
  library(sf)
  
  # load populations of all meshblocks
  mesh_pop <- read_csv(
    "data/spatial/abs/2016 census mesh block counts.csv",
    col_types = cols(
      MB_CODE_2016 = col_character(),
      MB_CATEGORY_NAME_2016 = col_character(),
      AREA_ALBERS_SQKM = col_double(),
      Dwelling = col_double(),
      Person = col_double(),
      State = col_double()
    )
  ) %>%
    rename(
      MB_CODE16 = MB_CODE_2016
    )
  
  # add populations onto shapefile
  state_mesh <- paste0(
    "data/spatial/abs/MB_2016_",
    state_short,
    ".shp"
  ) %>%
    st_read(
      stringsAsFactors = FALSE
    ) %>%
    left_join(mesh_pop)
  
  # get LGAs in VIC, join with mesh blocks, and sum populations
  st_read("data/spatial/abs/LGA_2016_AUST.shp",
          stringsAsFactors = FALSE) %>%
    filter(STE_NAME16 == state) %>%
    select(lga_code = LGA_CODE16,
           lga = LGA_NAME16,
           area = AREASQKM16) %>%
    st_join(state_mesh) %>%
    group_by(lga_code, lga, area) %>%
    summarise(pop = sum(Person)) %>%
    filter(area > 0) %>%
    mutate(
      pop_dens = pop / area
    ) %>%
    saveRDS(filepath)
  
}

# dplyr introduced a breaking change whereby the order of entries returned by a
# right_join was changed from the order of y to the order of x. This may work to
# reverse it.
old_right_join <- function(x, y, ...) {
  left_join(y, x, ...)
}

# get the survey wave number from a raw survey filename
wave_from_file <- function(filename, previous_waves = 14) {
  
  files <- list.files("data/survey_raw/", pattern = ".csv$", full.names = TRUE)
  lengths <- nchar(files)
  dates <- files %>%
    substr(lengths - 9, lengths - 4) %>%
    as.Date("%d%m%y")
  files <- files[order(dates)]
  waves <- seq_along(files) + previous_waves
  idx <- match(
    basename(filename),
    basename(files)
  )
  
  waves[idx]
  
}

# which household types correspond to the respondent being a parent (guessing at
# BETA's 'parent' field)
parenty_households <- c(
  "Couple with non-dependent child(ren)",
  "Couple with dependent child(ren)",
  "Couple with dependent and non-dependent children",
  "Single parent with non-dependent child(ren)",
  "Single parent with dependent child(ren)",
  "Single parent with dependent and non-dependent children"
)

parse_all_doh_surveys <- function(dir = "data/survey_raw") {
  
  dir %>%
    list.files(
      pattern = ".csv$",
      full.names = TRUE
    ) %>%
    lapply(parse_doh_survey) %>%
    bind_rows() %>%
    remove_doh_duplicates()
  
}

# read in and parse raw DoH survey files in a similar way to BETA
parse_doh_survey <- function(filename) {
  full <- filename %>%
    read_csv(
      col_types = cols(
        .default = col_character(),
        S1 = col_double(),
        Q138 = col_double(),
        Q166_1 = col_double(),
        Q166_4 = col_double(),
        Q167_1 = col_double(),
        Q168_2 = col_double(),
        Q169_1 = col_double()
      )
    ) %>%
    mutate(
      wave = wave_from_file(filename),
      state = abbreviate_states(S3),
      city = Location == "Major city",
      parent = Q39 %in% parenty_households,
      distancing_any = NA,
      cough_any = NA,
      mask = NA,
      survey = "doh",
      date = as.Date(StartDate, format = "%Y%m%d"),
    )
  
  # add face covering data if it is there
  if ("Q222" %in% names(full)) {
    full <- full %>%
      mutate(face_covering = Q222)
  } else {
    full <- full %>%
      mutate(face_covering = NA)
  }
  
  full %>%
    select(
      wave,
      state,
      gender = S2,
      age = S1,
      vulnerable = Q75,
      age_groups = AgeBracket,
      city,
      location = Location,
      postcode = Q37,
      household = Q39,
      income = Q42,
      parent,
      employment = Q38,
      phys_contact = Q109,
      phys_distance = Q65,
      wash_hands = Q110,
      cough_any,
      cough = Q111,
      mask,
      face_covering,
      contact_num = Q138,
      contacts_under18 = Q166_1,
      contacts_18to39 = Q166_2,
      contacts_40to59 = Q166_3,
      contacts_60plus = Q166_4,
      contacts_ageunsure = Q166_5,
      contacts_phys = Q167_1,
      contacts_close = Q167_2,
      contacts_notclose = Q167_3,
      contacts_physdontknow = Q167_4,
      contacts_home = Q168_1,
      contacts_work = Q168_2,
      contacts_worship = Q168_3,
      contacts_school = Q168_4,
      contacts_shop = Q168_5,
      contacts_cafe = Q168_6,
      contacts_sport = Q168_7,
      contacts_public = Q168_8,
      contacts_other = Q168_9,
      contacts_cantremember = Q168_10,
      contacts_5min = Q169_1,
      contacts_5to30min = Q169_2,
      contacts_30to90min = Q169_3,
      contacts_90minto3hrs = Q169_4,
      contacts_3hrsplus = Q169_5,
      contacts_timedontknow = Q169_6,
      date,
      survey
    ) %>%
    mutate_at(
      vars(vulnerable, phys_contact),
      ~yesno_to_logical(.)
    ) %>%
    mutate_at(
      vars(starts_with("contacts_")),
      ~as.numeric(.)
    )
  
}

# plot apparent duplication of records in doh surveys by wave and state to spot
# bot entries
plot_age_duplication <- function(doh_surveys, max_fraction = 0.12) {
  doh_surveys %>%
    # remove any state with fewer than 50 respondents per week, on average
    group_by(state, wave) %>%
    mutate(respondents = n()) %>%
    group_by(state) %>%
    mutate(mean_respondents = mean(respondents)) %>%
    filter(mean_respondents > 50) %>%
    # count fraction of respondents by age in each wave/state
    group_by(wave, state, age) %>%
    count() %>%
    group_by(state, wave) %>%
    mutate(
      fraction = n / sum(n)
    ) %>%
    ungroup() %>%
    select(-n) %>%
    complete(
      wave, state, age,
      fill = list(fraction = 0)
    ) %>%
    ggplot() +
    aes(state, age, fill = fraction) +
    geom_tile() +
    facet_wrap(~wave) +
    scale_fill_viridis_c(
      na.value = grey(0.6),
      limits = c(0, max_fraction)
    ) +
    theme_minimal()
}

# remove duplicated data in DoH surveys caused by bots
remove_doh_duplicates <- function(doh_surveys) {
  
  # find clusters of more than responses of the same age gender and postcode in a given wave
  duplicates <-
    doh_surveys %>%
    filter(!is.na(postcode) & postcode != -99 & wave >= 22) %>%
    group_by(wave, age, gender, postcode) %>%
    summarise(count = n()) %>%
    filter(count > 3) %>%
    arrange(wave, postcode, gender, age)
  
  # print out the detected duplicates
  message("duplicates detected:")
  print(duplicates, n = Inf)
  
  # return the data with them removed
  doh_surveys %>%
    anti_join(duplicates)
  
}

# read in and parse a uom survey in a similar way to the Barometer data
parse_uom_survey <- function(filename, wave = NA) {
  
  major_cities <- c(
    "Sydney",
    "Melbourne",
    "Brisbane",
    "Perth",
    "Adelaide",
    "Hobart",
    "ACT"
  )
  
  # read the file and skip the second row (full questions)
  read_xlsx(filename, col_types = "text")[-1, ] %>%
    map_df(~parse_guess(.)) %>%
    mutate(
      wave = wave,
      state = abbreviate_states(state),
      city = location %in% major_cities,
      parent = case_when(
        parent_yn_1 == "Yes" | parent_yn_2 == "Yes" ~ TRUE,
        parent_yn_3 == "Yes" ~ FALSE,
        TRUE ~ NA
      ),
      ATSI = case_when(
        Q2 == "No" ~ FALSE,
        Q2 == "Prefer not to say" ~ NA,
        TRUE ~ TRUE
      ),
      poor_health = Q3 == "poor",
      vulnerable = age > 70 | ATSI | poor_health,
      age_groups = case_when(
        age >= 75 ~ "75+",
        age > 64 ~ "65-74",
        age > 54 ~ "55-64",
        age > 44 ~ "45-54",
        age > 34 ~ "35-44",
        age > 24 ~ "25-34",
        age >= 18 ~ "18-24",
        TRUE ~ "NA"
      ),
      location = ifelse(city, "Major city", "Regional"),
      postcode = as.character(postcode),
      household = NA,
      income = NA,
      employment = NA,
      phys_contact = NA,
      phys_distance = NA,
      distancing_any = yesno_to_logical(Q9_7),
      wash_hands = NA,
      cough_any = yesno_to_logical(Q9_2),
      cough = NA,
      mask = yesno_to_logical(Q9_20),
      face_covering = NA,
      contacts_under18 = NA,
      contacts_18to39 = NA,
      contacts_40to59 = NA,
      contacts_60plus = NA,
      contacts_ageunsure = NA,
      contacts_phys = NA,
      contacts_close = NA,
      contacts_notclose = NA,
      contacts_physdontknow = NA,
      contacts_home = NA,
      contacts_work = NA,
      contacts_worship = NA,
      contacts_school = NA,
      contacts_shop = NA,
      contacts_cafe = NA,
      contacts_sport = NA,
      contacts_public = NA,
      contacts_other = NA,
      contacts_cantremember = NA,
      contacts_5min = NA,
      contacts_5to30min = NA,
      contacts_30to90min = NA,
      contacts_90minto3hrs = NA,
      contacts_3hrsplus = NA,
      contacts_timedontknow = NA,
      date = as.Date(`Date of`, origin = "1900-01-01"),
      survey = "uom"
    ) %>%
    select(
      wave,
      state,
      gender = Q1,
      vulnerable,
      age_groups,
      city,
      location,
      postcode,
      household,
      income,
      parent,
      employment,
      phys_contact,
      distancing_any,
      phys_distance,
      wash_hands,
      cough_any,
      cough,
      mask,
      face_covering,
      contact_num = Q11,
      contacts_under18,
      contacts_18to39,
      contacts_40to59,
      contacts_60plus,
      contacts_ageunsure,
      contacts_phys,
      contacts_close,
      contacts_notclose,
      contacts_physdontknow,
      contacts_home,
      contacts_work,
      contacts_worship,
      contacts_school,
      contacts_shop,
      contacts_cafe,
      contacts_sport,
      contacts_public,
      contacts_other,
      contacts_cantremember,
      contacts_5min,
      contacts_5to30min,
      contacts_30to90min,
      contacts_90minto3hrs,
      contacts_3hrsplus,
      contacts_timedontknow,
      date,
      survey
    )
}

yesno_to_logical <- function(x) {
  case_when(
    x == "Yes" ~ TRUE,
    x == "No" ~ FALSE,
    TRUE ~ NA
  )
}

# read in and parse the BETA barometer data dump
parse_barometer <- function(filename = "data/barometer_all/Barometer full contact_dates.csv") {
  df <- read_csv(
    filename,
    col_types = cols(
      .default = col_double(),
      postcode = col_character(),
      state = col_character(),
      gender = col_character(),
      vulnerable = col_character(),
      age_groups_pd = col_character(),
      city = col_character(),
      location = col_character(),
      household = col_character(),
      income = col_character(),
      parent = col_character(),
      employment = col_character(),
      phys_contact = col_character(),
      phys_distance = col_character(),
      wash_hands = col_character(),
      cough = col_character(),
      wave_date = col_date(format = ""),
      date = col_date(format = "%Y%m%d")
    )
  ) %>%
    # filter out leading numbers in responses
    mutate_at(
      vars(income, phys_distance, wash_hands, cough),
      ~substr(., 3, 100)
    ) %>%
    mutate_at(
      vars(vulnerable, city, phys_contact, parent),
      ~yesno_to_logical(.)
    ) %>%
    mutate_at(
      vars(starts_with("contacts_")),
      ~ifelse(. == -66, NA, .)
    ) %>%
    mutate(
      survey = "barometer",
      state = abbreviate_states(state),
      cough_any = NA,
      mask = NA,
      face_covering = NA,
      distancing_any = NA
    ) %>%
    rename(
      wave = barometer_week
    ) %>%
    select(-age_groups_pd)
  
  # fill in missing dates withthe wave date - the only information we have
  missing_date <- is.na(df$date)
  df$date[missing_date] <- df$wave_date[missing_date]
  df %>%
    select(-wave_date)
  
}

parse_all_uom_surveys <- function(dir = "~/not_synced/uom_surveys/unlocked") {
  
  dir %>%
    file.path(
      c(
        "SPSS Updated Data Covid19 Attitudes and Practices labels April2020.xlsx",
        "SPSS Wave 2 Data Covid19 Attitudes and Practices labels May2020.xlsx"
      )
    ) %>%
    mapply(
      parse_uom_survey,
      .,
      wave = c(-1, 0),
      SIMPLIFY = FALSE
    ) %>%
    bind_rows()
  
}

is_weekend <- function(date) {
  lubridate::wday(date, label = TRUE) %in% c("Sat", "Sun")
}

# get the expected fraction of the previous 24h that was on a weekend
weekend_weight <- function(date) {
  weekend_today <- is_weekend(date)
  weekend_yesterday <- is_weekend(date - 1)
  (weekend_today + weekend_yesterday) / 2
}

# read in and parse all the respondent-level survey data
parse_all_surveys <- function() {
  bind_rows(
    parse_all_uom_surveys(),
    parse_barometer(),
    parse_all_doh_surveys()
  ) %>%
    mutate(
      weekend_fraction = weekend_weight(date)
    ) %>%
    group_by(wave) %>%
    mutate(
      wave_date = median(date),
      wave_duration = as.numeric(max(date) - min(date))
    ) %>%
    ungroup()
}

# the vector of dates to use for each scenario
scenario_dates <- function(scenario) {
  
  switch(
    scenario$phase,
    importation = seq(as.Date("2020-03-01"),
                      as.Date("2020-04-30"),
                      by = 1),
    suppression = seq(as.Date("2020-05-01"),
                      as.Date("2020-06-30"),
                      by = 1),
    community = seq(as.Date("2020-07-01"),
                    as.Date("2020-08-31"),
                    by = 1)
  )
  
}

# reff C1 locals (except surveillance effect) with macro/microdistancing either
# at optimal effect (TRUE) or turned off (FALSE)
reff_distancing <- function(fitted_model, macro_effect = TRUE, micro_effect = TRUE) {
  
  de <- fitted_model$greta_arrays$distancing_effect
  infectious_days <- infectious_period(gi_cdf)
  
  # duration in the household
  HD <- de$HD_0
  
  # non-household contacts
  OC <- de$OC_0
  
  # reduction in transmission probability due to hygiene measures
  gamma <- 1
  
  if (macro_effect) {
    
    macro_trends <- trends_date_state(
      "outputs/macrodistancing_trends.RDS",
      fitted_model$data$dates$infection
    )
    
    # optimal date and state for reduction in contacts
    macro_optimum <- which(macro_trends == min(macro_trends), arr.ind = TRUE)[1, , drop = FALSE]
    
    # duration in the household increases at optimum
    h_optimal <- h_t_state(fitted_model$data$dates$infection)[macro_optimum]
    HD <- HD * h_optimal
    
    # number of non-household contacts decreases at optimum
    OC <- macro_trends[macro_optimum]
  }
  
  if (micro_effect) {
    
    # optimal date and state for reduction in contacts
    micro_trends <- trends_date_state(
      "outputs/microdistancing_trends.RDS",
      fitted_model$data$dates$infection
    )
    micro_optimum <- which(micro_trends == max(micro_trends), arr.ind = TRUE)[1, , drop = FALSE]
    
    # transmission probability is reduced at optimum
    gamma <- de$gamma_t_state[micro_optimum]
    
  }
  
  household_infections <- de$HC_0 * (1 - de$p ^ HD)
  non_household_infections <- OC * infectious_days *
    (1 - de$p ^ de$OD_0) * gamma
  
  household_infections + non_household_infections
  
}

# scalar reffs for locally- and overseas-acquired cases under different policy scenarios
counterfactual_reffs <- function(scenario, fitted_model) {
  
  # baseline reff due to household/non-household model
  baseline_local_reff <- reff_distancing(
    fitted_model,
    macro_effect = scenario$mobility_restrictions,
    micro_effect = scenario$physical_distancing
  )
  
  # reduction in reff due to contact tracing
  contact_tracing_effect <- switch(
    scenario$contact_tracing,
    none = 1,
    suboptimal = surveillance_effect(as.Date("2020-01-01"), gi_cdf),
    optimal = surveillance_effect(as.Date("2020-09-01"), gi_cdf)
  )
  
  # overall reff for locally-acquired cases
  local_reff <- baseline_local_reff * contact_tracing_effect
  
  # overall reff for overseas-acquired cases  
  import_reff <- local_reff
  if (scenario$overseas_quarantine) {
    log_q3 <- fitted_model$greta_arrays$log_q[3]
    import_reff <- exp(log_q3)
  }
  
  # return these scalar greta arrays
  module(local_reff, import_reff)
  
}

# things to do for each:
# 1. compute Reff C1s under each scenario

# 2. project case counts under Reff trajectory
#  - function to create greta array for locally-acquired case trajectories based on two reff trajectories, imported case counts, and initial local case counts
#  - function to set up imported case counts and initial case counts for each phase
#  - wrapper function to take in scenario config, do calculation of posteriors, and save outputs 


# the vectors of case counts to use for each scenario
scenario_cases <- function(scenario, fitted_model) {
  
  all_dates <- fitted_model$data$dates$infection
  all_imported <- rowSums(fitted_model$data$imported$cases)
  all_local <- rowSums(fitted_model$data$local$cases)
  
  scenario_dates <- scenario_dates(scenario)
  scenario_start <- min(scenario_dates)
  n_scenario_dates <- length(scenario_dates)
  
  during <- all_dates %in% scenario_dates
  before <- all_dates < scenario_start & all_dates >= (scenario_start - 21)
  
  list(
    local_cases = c(all_local[before], rep(0, n_scenario_dates)),
    imported_cases = all_imported[before | during],
    dates = all_dates[before | during],
    simulation_start = scenario_start,
    n_dates = n_scenario_dates
  )  
  
}

# given a fitted reff model and a scenario, simulate Reffs and the numbers of
# locally-acquired cases nationally
simulate_scenario <- function (index, scenarios, fitted_model, nsim = 5000) {
  
  scenario <- scenarios[index, ]
  
  reffs <- counterfactual_reffs(scenario, fitted_model)
  case_data <- scenario_cases(scenario, fitted_model)
  one <- ones(length(case_data$local_cases))
  
  # need to include 3 weeks of pre-simulation local and imported cases, so pad
  # dates. pass in a single column matrix of these things to do national simulations
  simulation <- forecast_locals(
    local_cases = as.matrix(case_data$local_cases),
    imported_cases = as.matrix(case_data$imported_cases),
    Reff_locals = reffs$local_reff * one,
    Reff_imports = reffs$import_reff * one,
    dates = case_data$dates,
    gi_cdf = gi_cdf,
    simulation_start = case_data$simulation_start
  )
  
  keep <- case_data$dates >= case_data$simulation_start
  local_cases <- simulation$local_cases[keep]
  imported_cases <- case_data$imported_cases[keep]
  dates <- case_data$dates[keep]
  
  reff_local <- reffs$local_reff
  reff_imported <- reffs$import_reff
  
  # get posterior samples
  sims <- calculate(
    local_cases, reff_local, reff_imported,
    values = fitted_model$draws,
    nsim = nsim
  )
  
  # handle the outputting
  local_cases <- tibble(
    sim = rep(seq_len(nsim), each = length(dates)),
    date = rep(dates, nsim),
    cases = as.vector(t(sims$local_cases[, , 1]))
  )
  
  reffs <- tibble(
    sim = seq_len(nsim),
    local = as.vector(sims$reff_local),
    imported = as.vector(sims$reff_imported),
  )
  
  # save these samples
  module(scenario, local_cases, reffs) %>%
    saveRDS(paste0("outputs/counterfactuals/scenario", index, ".RDS"))
  
}


mean.ecdf <- function(x, ...) {
  mean(evalq(rep.int(x, diff(c(0, round(nobs * y)))), environment(x)), ...)
}

count_in_window <- function(target_date, states, delay_data, window, date_tabulation) {
  dates <- delay_data %>%
    filter(state %in% states) %>%
    pull(!!date_tabulation)
  diff <- abs(dates - target_date)
  in_window <- diff <= window
  sum(in_window)
}

get_window_size <- function(
  target_date,
  states,
  delay_data,
  date_tabulation,
  n_min = 500,
  window_min = 7,
  window_max = 42
) {
  
  dates <- delay_data %>%
    filter(state %in% states) %>%
    pull(!!date_tabulation)
  
  # find the smallest window that yields the required number of counts
  for (window in window_min:window_max) {
    
    diff <- abs(dates - target_date)
    in_window <- diff <= window
    
    if (sum(in_window) >= n_min) {
      break()
    }
    
  }
  
  window
  
}

delay_ecdf <- function(target_date, states, window, delay_data, date_tabulation) {
  
  data <- delay_data %>%
    filter(state %in% states)
  dates <- pull(data, !!date_tabulation)
  delays <- pull(data, delay)
  
  diff <- abs(dates - target_date)
  in_window <- diff <= window
  
  valid_delays <- delays[in_window]
  
  if (length(valid_delays) > 0) {
    distribution <- ecdf(valid_delays)
  } else {
    distribution <- NULL
  }
  
  list(distribution)
  
}

ci_ribbon <- function(ci) {
  
  lo <- paste0("ci_", ci, "_lo")
  hi <- paste0("ci_", ci, "_hi")
  
  geom_ribbon(
    aes_string(ymin = lo,
               ymax = hi),
    alpha = 1/9
  )
}

# calculate a weighted average ecdf out of two (weight is the probability of the first)
weight_ecdf <- function(ecdf_1, ecdf_2, weight) {
  
  if (is.null(ecdf_1) | weight == 0) {
    return(ecdf_2)
  }
  if (is.null(ecdf_2) | weight == 1) {
    return(ecdf_1)
  }
  
  e1 <- environment(ecdf_1)
  e2 <- environment(ecdf_2)
  
  # reconcile the xs
  x_1 <- e1$x
  x_2 <- e2$x
  
  x <- sort(unique(c(x_1, x_2)))
  
  # get the two CDFs
  y_1 <- ecdf_1(x)
  y_2 <- ecdf_2(x)
  
  # get the two pdfs
  pdf_1 <- diff(c(0, y_1))
  pdf_2 <- diff(c(0, y_2))
  
  # get a weighted average of them
  pdf <- pdf_1 * weight + pdf_2 * (1 - weight)
  
  # convert back to a CDF
  y <- cumsum(pdf)
  
  # rebuild an ecdf object, the slow way
  method <- 2L
  yleft <- 0
  yright <- 1
  f <- e1$f
  n <- e1$nobs
  rval <- function (v) {
    stats:::.approxfun(x, y, v, method, yleft, yright, f)
  }
  class(rval) <- c("ecdf", "stepfun", class(rval))
  assign("nobs", n, envir = environment(rval))
  attr(rval, "call") <- attr(ecdf_1, "call")
  rval
  
}

# convert a vector fo cumulative probabilities into an ecdf object
make_ecdf <- function(y, x) {
  
  sims <- sample(x,
                 100,
                 prob = y,
                 replace = TRUE)
  
  ecdf_null <- ecdf(sims)
  envir <- environment(ecdf_null)
  
  # rebuild an ecdf object, the slow way
  method <- 2L
  yleft <- 0
  yright <- 1
  f <- envir$f
  n <- envir$nobs
  rval <- function (v) {
    stats:::.approxfun(x, y, v, method, yleft, yright, f)
  }
  class(rval) <- c("ecdf", "stepfun", class(rval))
  assign("nobs", n, envir = environment(rval))
  attr(rval, "call") <- attr(ecdf_null, "call")
  rval  
}




get_cis <- function(date, state, ecdf, weight, use_national) {
  
  deciles_lower <- seq(0.05, 0.45, by = 0.05)
  deciles_upper <- 1 - deciles_lower
  deciles <- c(deciles_lower, deciles_upper)
  decile_names <- paste0("ci_", (1 - 2 * deciles_lower) * 100)
  decile_names <- c(paste0(decile_names, "_lo"),
                    paste0(decile_names, "_hi"))
  
  cis <- quantile(ecdf, deciles)
  names(cis) <- decile_names
  cis
}

# Calculate time-varying delay distributions for each state, smoothly reverting
# to the national average when and where there are insufficient records to
# compute state-level distributions. Return a tibble of empirical CDFs of the
# distribution by state and date.
# Parameters:

# state, date, delay: vectors of equal length giving the data on observed delays
#   by date and state
# all_dates, all_states: optional vectors of dates and states for which to compute
#   delays (taken from linelist if not specified)
# direction: whether to tabulate delays by date in a 'forward' ('date' is at the
#   start of the delay) or 'backward' ('date' is at the end of the delay) manner
# delay_plausible_bounds: a vector of length 2 of plausible delays. Records with
#   values outside these bounds are assumed to be erroneous and removed.
# min_records: the minimum number of records required to reliably estimate the delay
#   distribution within a window
# absolute_min_records: the absolute minimum number of records to estimate a
#   state-level delay distribution within a window. If the number of records is
#   below this (even with the maximum window size), the national estimate is used
#   instead. If it is between this and 'min_records', the distribution is
#   estimated as a weighted average of the state and national distributions.
# min_window: the minimum window size (number of days wide) in which to estimate
#   the delay distribution
# max_window: the maximum window size in which to estimate the delay
#   distribution for each date at the state level. At the national level there is
#   no maximum applied
# national_exclusions: a tibble of states, start dates, and end dates denoting
#   times and places that should not contribute to the national estimate. If
#   either of the dates are NA, the earliest (or latest) dates in the linelist
#   are used
# revert_to_national: a logical for whether to revert to national estimates with
#   low counts (turn this off to e.g. run for a single state)
estimate_delays <- function(
  state,
  date,
  delay,
  all_dates = NULL,
  all_states = NULL,
  direction = c("forward", "backward"),
  min_records = 500,
  absolute_min_records = 100,
  min_window = 7,
  max_window = 56,
  national_exclusions = tibble(state = "VIC", start = as.Date("2020-06-14"), end = NA),
  revert_to_national = TRUE
) {
  
  direction <- match.arg(direction)
  
  # account for right-truncation when tabulating
  # which date to tabulate by
  if (direction == "forward") {
    date_from <- date
    date_to <- date + delay
    date_tabulation <- "date_from"
  } else {
    date_from <- date - delay
    date_to <- date
    date_tabulation <- "date_to"
  }
  
  delay_data <- tibble(
    state = state,
    date_from = date_from,
    date_to = date_to,
    delay = delay
  )
  
  if (is.null(all_dates)) {
    all_dates <- seq(
      min(delay_data$date_from),
      max(delay_data$date_to),
      by = 1
    )
  }
  
  if (is.null(all_states)) {
    all_states <- unique(delay_data$state)
  }
  
  date_state <- expand_grid(
    date = all_dates,
    state = all_states
  )
  
  # get the half-window size (number of days on either side of the target)
  absolute_max_window <- as.numeric(diff(range(all_dates)))
  min_window <- ceiling((min_window - 1) / 2)
  max_window <- floor((max_window - 1) / 2)
  
  # for each confirmation date, run the algorithm on each date
  statewide <- date_state %>%
    group_by(date, state) %>%
    mutate(
      window = get_window_size(
        date,
        state,
        delay_data = delay_data,
        date_tabulation = date_tabulation,
        n_min = min_records,
        window_min = min_window,
        window_max = max_window 
      ),
      count = count_in_window(
        date,
        state,
        delay_data = delay_data,
        date_tabulation = date_tabulation,
        window = window
      ),
      state_ecdf = delay_ecdf(
        date,
        state,
        window = window,
        delay_data = delay_data,
        date_tabulation = date_tabulation
      )
    )
  
  if (revert_to_national) {
    
    
    # fill in exclusion periods
    national_exclusions <- national_exclusions %>%
      mutate(
        start = as.Date(start),
        end = as.Date(end),
        start = replace_na(start, min(all_dates)),
        end = replace_na(end, max(all_dates))
      )
    
    # remove the specified data for estimating the national background distribution
    for (i in seq_len(nrow(national_exclusions))) {
      delay_data <- delay_data %>%
        filter(
          !(
            state == national_exclusions$state[i] &
              date_from >= national_exclusions$start[i] &
              date_to <= national_exclusions$end[i]
          )
        )
    }
    
    nationwide <- date_state %>%
      # arbitrarily pick one set of dates
      filter(state == "ACT") %>%
      select(-state) %>%
      group_by(date) %>%
      mutate(
        window = get_window_size(
          date,
          all_states,
          delay_data = delay_data,
          date_tabulation = date_tabulation,
          n_min = min_records,
          window_min = min_window,
          window_max = absolute_max_window
        ),
        national_ecdf = delay_ecdf(
          date,
          all_states,
          window = window,
          delay_data = delay_data,
          date_tabulation = date_tabulation
        )
      )
    
    # for statewide, replace any invalid ecdfs with the national one
    state_ecdfs <- statewide %>%
      right_join(
        nationwide %>%
          select(-window)
      ) %>%
      mutate(
        use_national = count < absolute_min_records,
        weight = pmin(1, count / min_records),
        weight = ifelse(use_national, 0, weight),
        ecdf = mapply(
          FUN = weight_ecdf,
          state_ecdf,
          national_ecdf,
          weight,
          SIMPLIFY = FALSE
        )
      ) %>%
      select(
        date, state, ecdf, weight, use_national
      )
    
  } else {
    
    state_ecdfs <- statewide %>%
      mutate(
        use_national = count < absolute_min_records,
        weight = pmin(1, count / min_records),
        weight = ifelse(use_national, 0, weight),
        ecdf = state_ecdf
      ) %>%
      select(
        date, state, ecdf, weight, use_national
      )
    
  }

  state_ecdfs
  
}

# plot changing delay distributions by state over time
plot_delays <- function(
  delay_distributions,
  date,
  state,
  delay,
  ylim = c(0, 20),
  hline_at = 0,
  intervention_at = interventions(), 
  base_colour = yellow
) {
  
  all_states <- unique(state)
  intervention_at <- intervention_at %>%
    filter(
      state %in% all_states
    )
  
  # mutate to output quantiles and then plot them
  quantiles <- delay_distributions %>%
    # plot it as the date of infection, not date of onset!
    mutate(date = date - 5) %>%
    pmap_dfr(get_cis) %>%
    bind_cols(delay_distributions, .) %>%
    mutate(
      median = vapply(
        ecdf,
        quantile,
        0.5,
        FUN.VALUE = numeric(1)
      ),
      mean = vapply(
        ecdf,
        mean,
        FUN.VALUE = numeric(1)
      )
    )
  
  # observed delays
  df_obs <- tibble(
    date = date,
    state = state,
    delay = delay,
    type = "Nowcast"
  )
  
  p <- quantiles %>%
    mutate(type = "Nowcast") %>%
    ggplot() + 
    
    aes(date, mean, fill = type) +
    
    facet_wrap(~state, ncol = 2) +
    
    xlab(element_blank()) +
    
    coord_cartesian(
      ylim = ylim,
      xlim = c(as.Date("2020-03-01"), max(delay_distributions$date))
    ) +
    scale_y_continuous(position = "right") +
    scale_x_date(date_breaks = "1 month", date_labels = "%e/%m") +
    scale_alpha(range = c(0, 0.5)) +
    scale_fill_manual(values = c("Nowcast" = base_colour)) +
    
    ci_ribbon("90") +
    ci_ribbon("80") +
    ci_ribbon("70") +
    ci_ribbon("60") +
    ci_ribbon("50") +
    ci_ribbon("40") +
    ci_ribbon("30") +
    ci_ribbon("20") +
    ci_ribbon("10") +
    
    geom_line(aes(y = ci_90_lo),
              colour = base_colour,
              alpha = 0.8) + 
    geom_line(aes(y = ci_90_hi),
              colour = base_colour,
              alpha = 0.8) + 
    
    geom_line(aes(y = mean),
              colour = grey(0.4),
              alpha = 1,
              size = 1) +
    
    # add shading for regions where the national distribution is used
    geom_ribbon(
      aes(ymin = -10, ymax = use_national * 100 - 10),
      fill = grey(1),
      alpha = 0.5,
      colour = grey(0.9),
      linetype = 3
    ) +
    
    # add horizontal and vertical lines for context
    geom_vline(
      aes(xintercept = date),
      data = intervention_at,
      colour = "grey80"
    ) +
    
    geom_hline(
      yintercept = hline_at,
      colour = "grey80"
    ) +
    
    # add points for true delays
    geom_point(
      aes(date, delay),
      data = df_obs,
      pch = 16,
      size = 0.2,
      alpha = 0.1
    ) +
    
    cowplot::theme_cowplot() +
    cowplot::panel_border(remove = TRUE) +
    theme(legend.position = "none",
          strip.background = element_blank(),
          strip.text = element_text(hjust = 0, face = "bold"),
          axis.title.y.right = element_text(vjust = 0.5, angle = 90),
          panel.spacing = unit(1.2, "lines"),
          axis.text.x = element_text(size = 8)
          )
  
  p
  
}

gaussian_smooth <- function (values, sd = 1, ...) {
  id <- seq_along(values)
  middle <- round(mean(id))
  diff <- id - middle
  weights <- exp(-0.5 * (diff / sd) ^ 2)
  weights <- weights / sum(weights)
  weighted_mean(values, weights, ...)
}

# given a dataframe of mobility data subsetted to a particular mobility metric,
# fit a generalised additive model for the trend and return a dataframe with the
# modelled mobility trend for all dates between min_date and max_date
predict_mobility_trend <- function(
  mobility,
  min_date = min(mobility$date),
  max_date = max(mobility$date)
) {
  
  # print(mobility$state[[1]])
  # print(mobility$lga[[1]])
  # print(mobility$datastream[[1]])
  
  all_dates <- seq(min_date, max_date, by = 1)
  
  min_data_date = min(mobility$date)
  max_data_date = max(mobility$date)
  
  # empty row to return on error (NULL apparently won't work)
  empty_df <- tibble::tibble(
      state_long = mobility$state_long[1],
      state = mobility$state[1],
      lga = mobility$lga[1],
      date = all_dates,
      predicted_trend = NA,
      datastream = mobility$datastream[1],
      fitted_trend = NA,
      fitted_trend_lower = NA,
      fitted_trend_upper = NA
    ) %>%
      left_join(
        mobility,
        by = c("lga", "date", "datastream", "state_long", "state")
      ) %>%
    relocate(
      trend,
      .after = "datastream"
    )
  
  # empty_df <- mobility %>%
  #   mutate(
  #     predicted_trend = NA,
  #     fitted_trend = NA,
  #     fitted_trend_lower = NA,
  #     fitted_trend_upper = NA
  #   )
  
  # empty_row <- structure(
  #   list(
  #     lga = character(0),
  #     datastream = character(0),
  #     state_long = character(0),
  #     state = character(0),
  #     date = structure(numeric(0), class = "Date"),
  #     predicted_trend = numeric(0),
  #     trend = numeric(0),
  #     fitted_trend = numeric(0),
  #     fitted_trend_lower = numeric(0),
  #     fitted_trend_upper = numeric(0)
  #   ),
  #   row.names = integer(0),
  #   class = c("tbl_df","tbl", "data.frame")
  # )
  
  public_holidays <- holiday_dates() %>%
    mutate(
      state = abbreviate_states(state)
    ) %>%
    rename(
      holiday = name
    )
  
  school_holidays <- school_holiday_dates() %>%
    mutate(
      state = abbreviate_states(state)
    )

  # need to deal with factor levels not in the original fit. I.e. when no date
  # is observed with that intervention. Remove any interventions not matching
  # the date and states in the data
  
  # # drop any stages for which there is no data (cannot be estimated)

  
  # create intervention step-change covariates
  interventions <- interventions(end_dates = TRUE) %>%
    filter(
      date <= max_data_date,
      date %in% mobility$date
    )
  
  if (nrow(interventions) == 0) {
    return(empty_df)
  }
  
  intervention_steps <- interventions %>%
    mutate(
      intervention_id = paste0(
        "intervention_",
        match(date, unique(date))
      )
    ) %>%
    group_by(intervention_id, state) %>%
    do(
      tibble(
        date = all_dates,
        intervention_effect = as.numeric(all_dates >= .$date)
      )
    ) %>%
    group_by(state, date) %>%
    summarise(
      intervention_stage = sum(intervention_effect),
      .groups = "drop"
    ) %>%
    mutate(
      intervention_stage = factor(intervention_stage)
    ) 
  
  df <- mobility %>%
    left_join(
      public_holidays,
      by = c("state", "date")
    ) %>%
    left_join(
      school_holidays,
      by = c("state", "date")
    ) %>%
    left_join(
      intervention_steps,
      by = c("state", "date")
    ) %>%
    mutate(
      holiday = replace_na(holiday, "none"),
      is_a_holiday = holiday != "none",
      is_a_school_holiday = !is.na(school_holiday),
      holiday = factor(holiday),
      date_num = as.numeric(date - min_date),
      dow = lubridate::wday(date, label = TRUE),
      dow = as.character(dow),
      intervention_stage = factor(
        intervention_stage,
        levels = unique(intervention_stage)
      )
    ) %>%
    filter(!is.na(trend))

  library(mgcv)
  
  m <- tryCatch(
    gam(trend ~
          
          # smooth variations in mobility
          s(date_num, k = 50) +
          
          # step changes around intervention impositions
          intervention_stage +
          
          # random effect on holidays (different for each holiday, but shrunk
          # to an average holiday effect which used to predict into future)
          is_a_holiday +
          s(holiday, bs = "re") +
          
          # constant effect for school holidays
          is_a_school_holiday +
          
          # day of the week effect
          dow,
        
        select = TRUE,
        gamma = 2,
        data = df),
    
    error = function(e) {
      NULL
    }
    
  )
  
  # return an empty row if there was an error
  if (is.null(m)) {
    return(empty_df)
  }
  
  # compute mean and standard deviation of Gaussian observation model for fitted data
  fit <- predict(m, se.fit = TRUE)
  fit$sd <- sqrt(var(residuals(m)) + fit$se.fit ^ 2)

  df_fitted <- df %>%
    # predict with fitted model (and get 90% CIs)
    mutate(
      fitted_trend = fit$fit,
      fitted_trend_upper = fit$fit + fit$sd * qnorm(0.025),
      fitted_trend_lower = fit$fit + fit$sd * qnorm(0.975),
    )
  
  # predict each date, *averaging over the weekday effect for each date*
  pred_df <- expand_grid(
    state_long = unique(df$state_long),
    dow = unique(df$dow),
    date = all_dates,
  ) %>%
    mutate(
      lga = df$lga[1],
      datastream = mobility$datastream[1],
      state = abbreviate_states(state_long)
    ) %>% 
    left_join(
      public_holidays,
      by = c("state", "date")
    ) %>%
    left_join(
      school_holidays,
      by = c("state", "date")
    ) %>%
    left_join(
      intervention_steps,
      by = c("state", "date")
    ) %>%
    mutate(
      holiday = replace_na(holiday, "none"),
      is_a_holiday = holiday != "none",
      # remove any named holidays not in the training data
      holiday = case_when(
        holiday %in% unique(df$holiday) ~ holiday,
        TRUE ~ "none"
      ),
      holiday = factor(holiday),
      is_a_school_holiday = !is.na(school_holiday),
      date_num = as.numeric(date - min_date),
      # clamp the smooth part of the prediction at both ends
      date_num = pmax(date_num, min_data_date - min_date),
      date_num = pmin(date_num, max_data_date - min_date)
    )
  
  # predict trends under these conditions, and average over day of the week
  pred_df <- pred_df %>%
    mutate(
      predicted_trend = predict(m, newdata = pred_df)
    ) %>%
    group_by(
      state_long, state, lga, datastream, date
    ) %>%
    summarise(
      predicted_trend = mean(predicted_trend),
      .groups = "drop"
    ) %>%
    group_by(
      state, lga, datastream
    ) %>%
    # smooth fitted curve over days of the week and holidays
    mutate(
      predicted_trend = slider::slide_dbl(
        predicted_trend,
        gaussian_smooth,
        na.rm = TRUE,
        sd = 2.8,
        .before = 5,
        .after = 5
      )
    ) %>%
    ungroup() %>%
    left_join(
      df_fitted %>%
        select(
          state, state_long, lga, date,
          datastream,
          trend,
          fitted_trend,
          fitted_trend_lower,
          fitted_trend_upper
        ),
      by = c("state", "state_long", "lga", "datastream", "date")
    )

  pred_df
  
}

# modify Reff of a fitted model by multiplying by multiplicative effect, drawn from a distribution
multiply_reff <- function(fitted_model, prior_mean, prior_range, quantile = 0.95) {
  prob <- 1 - (1 - quantile) / 2
  prior_sd <- abs(diff(prior_range)) / (2 * qnorm(prob))
  effect <- normal(prior_mean, prior_sd, truncation = c(0, Inf))
  fitted_model$greta_arrays$R_eff_loc_12 <- fitted_model$greta_arrays$R_eff_loc_12 * effect 
  fitted_model$greta_arrays$R_eff_loc_1 <- fitted_model$greta_arrays$R_eff_loc_1 * effect 
  fitted_model
}

# colours for plotting
blue <- "steelblue3"
purple <- "#C3A0E8"
green <- brewer.pal(8, "Set2")[1]
yellow <- brewer.pal(8, "Set2")[6]
blue_green <- colorRampPalette(c("blue", green))(10)[8]
yellow_green <- colorRampPalette(c("yellow", green))(10)[8]
orange <- brewer.pal(8, "Set2")[2]
pink <- brewer.pal(8, "Set2")[4]
fifo <- "#A8EB12"

# default cdf
gi_cdf <- nishiura_cdf()


url_exists <- function(address) {  
  tryCatch(
    {  
      con <- url(address)  
      a  <- capture.output(suppressWarnings(readLines(con)))  
      close(con)  
      TRUE;  
    },  
    error = function(err) {  
      occur <- grep("cannot open the connection", capture.output(err));  
      if(length(occur) > 0) FALSE;  
    }  
  )  
}

download_tidycovid <- function() {
  f <- tempfile(fileext = ".RDS")
  download.file(tidycovid_url, destfile = f, method='wget')
  tmp <- readRDS(f)
  saveRDS(tmp, file = "data/google_cmr/tidycovid_cache.RDS")  
}


fit_survey_gam <- function(
  fit_dat,
  pred_dat
){
  
  respondents <- fit_dat$respondents
  count <- fit_dat$count
  date <- fit_dat$date
  intervention_stage <- fit_dat$intervention_stage
  
  date_num <- as.numeric(date - min(date))
  
  m <- mgcv::gam(
    cbind(count, I(respondents - count)) ~ s(date_num) + intervention_stage,
    select = TRUE,
    family = stats::binomial,
  )
  
  
  pred_dat$date_num <- as.numeric(pred_dat$date - min(date))
  
  #pred <- predict(m, se.fit = TRUE, type = "link")
  pred <- predict(
    object = m,
    newdata = pred_dat,
    se.fit = TRUE,
    type = "link"
  )
  
  quantile95 <- qnorm(0.95)
  quantile75 <- qnorm(0.75)
  ci_90_hi <- pred$fit + (quantile95 * pred$se.fit)
  ci_90_lo <- pred$fit - (quantile95 * pred$se.fit)
  ci_50_hi <- pred$fit + (quantile75 * pred$se.fit)
  ci_50_lo <- pred$fit - (quantile75 * pred$se.fit)
  
  fitted <- m$family$linkinv(pred$fit) * pred_dat$distancing
  ci_90_hi <- m$family$linkinv(ci_90_hi) * pred_dat$distancing
  ci_90_lo <- m$family$linkinv(ci_90_lo) * pred_dat$distancing
  ci_50_hi <- m$family$linkinv(ci_50_hi) * pred_dat$distancing
  ci_50_lo <- m$family$linkinv(ci_50_lo) * pred_dat$distancing
  
  
  
  tibble(
    date = pred_dat$date,
    mean = fitted ,
    ci_90_lo,
    ci_50_lo,
    ci_50_hi,
    ci_90_hi
  )
  
}



get_ifr <- function(voc = TRUE) {
  
  # Age-structured infection fatality ratio (%) estimates from O'Driscoll et al.
  # 2020 https://doi.org/10.1038/s41586-020-2918-0 (Table S3 in supplement) and
  # Brazeau et al. 2020 (Imperial report 34)
  # https://www.imperial.ac.uk/mrc-global-infectious-disease-analysis/covid-19/report-34-ifr/
  # (Table 2 - estimate *with* seroreversion)
  
  # Brazeau et al has separate estimates for ages 80-84, 85-89, 90+, so we
  # recombine these based on ABS 2020 population age fractions into the 80+
  # category (50% ofover 80s are in 80-84, 30% are in 85-89, and 20% are 90 or
  # older)
  
  ifr <- tibble::tribble(
    ~age,      ~odriscoll,     ~brazeau,
    "0-4",          0.003,         0.00,
    "5-9",          0.001,         0.01,
    "10-14",        0.001,         0.01,
    "15-19",        0.003,         0.02,
    "20-24",        0.006,         0.02,
    "25-29",        0.013,         0.04,
    "30-34",        0.024,         0.06,
    "35-39",        0.040,         0.09,
    "40-44",        0.075,         0.15,
    "45-49",        0.121,         0.23,
    "50-54",        0.207,         0.36,
    "55-59",        0.323,         0.57,
    "60-64",        0.456,         0.89,
    "65-69",        1.075,         1.39,
    "70-74",        1.674,         2.17,
    "75-79",        3.203,         3.39,
    "80+",          8.292,         5.3*0.5 + 8.28*0.3 + 16.19*0.2
  )
  
  
  if(voc){
    # multiplier from Davis et al.
    ifr <- ifr %>%
      mutate(
        odriscoll = odriscoll*1.61,
        brazeau = brazeau*1.61
      )
  }
  
  return(ifr)
  
}


scrape_doses_timeseries <- function() {
  
  url <- "https://covidlive.com.au/report/daily-vaccinations/aus"
  
  # scrape the cumulative number of doses and convert to daily new doses
  scraped <- url %>%
    read_html() %>%
    html_nodes(
      "table"
    ) %>%
    .[[2]] %>%
    html_table(
      fill = TRUE
    ) %>%
    mutate(
      date = as.Date(DATE, format = "%d %B %y"),
      cumulative_doses = DOSES,
      cumulative_doses = gsub(",", "", cumulative_doses),
      cumulative_doses = as.numeric(cumulative_doses)
    ) %>%
    # compute new doses per day
    arrange(
      date
    ) %>%
    mutate(
      new_doses = diff(c(0, cumulative_doses))
    ) %>%
    select(
      date,
      new_doses
    )
  
  # fill in zeroes for any missing datess
  missing_dates <- expand_grid(
    date = seq(
      min(scraped$date),
      max(scraped$date),
      by = 1
    ),
    new_doses = 0
  ) %>%
    anti_join(
      scraped,
      by = "date"
    )
  
  # combine, reorder, compute cumulative number, drop leading 0s
  scraped %>%
    bind_rows(missing_dates) %>%
    arrange(date) %>%
    mutate(
      doses = cumsum(new_doses)
    ) %>%
    filter(
      doses > 0
    )
  
}


get_R <- function (transition_matrix, stable_age = NULL, tolerance = 0.001, max_iter = 1000, warm_start = TRUE) {
  #Re(eigen(x)$value[1])
  # function from STEPS
  # https://github.com/steps-dev/steps/blob/74c5359dd4470c4056cd799c53ef56d503ba69da/R/growth_transition_functions-class.R#L211
  # compute R from a transition (next generation) matrix
  
  # assign the stab;e age distribution if not known
  if (is.null(stable_age)) {
    # if using warm starts, try to find the recent stashed stable age distribution
    if (warm_start & exists(".ngm_stage_age_warm_start")) {
      stable_age <- .ngm_stage_age_warm_start
      # otherwise just start from scratch
    } else {
      stable_age <- rep(1, ncol(transition_matrix))
    }
  }
  
  old_stages <- stable_age
  converged <- FALSE
  iter <- 0
  old_Rs <- rep(.Machine$double.eps, ncol(transition_matrix))
  
  while (!converged & iter < max_iter) {
    new_stages <- transition_matrix %*% old_stages
    Rs <- new_stages / old_stages
    errors <- abs(1 - (Rs / old_Rs))
    converged <- all(errors < tolerance)
    old_Rs <- Rs
    old_stages <- new_stages
    iter <- iter + 1
  }
  
  if (!converged) {
    warning(
      "estimation of growth rate did not converge in ",
      max_iter,
      " iterations"
    )
  }
  
  # globally assign the stable age distribution if needed
  if (warm_start) {
    .ngm_stage_age_warm_start <<- new_stages / sum(new_stages)
  }
  
  # return the intrinsic growth rate
  Rs[1]
  
}

find_m <- function(R_target, transition_matrix, stable_age = NULL) {
  # this function from STEPS
  #https://github.com/steps-dev/steps/blob/74c5359dd4470c4056cd799c53ef56d503ba69da/R/growth_transition_functions-class.R#L266
  #
  # compute the m that calibrates a next generation matrix to R0
  
  obj <- function (m, R_target, transition_matrix, stable_age = NULL) {
    new_transition_matrix <- m*transition_matrix
    R_current <- get_R(new_transition_matrix, stable_age = stable_age)
    (R_current - R_target) ^ 2
  } 
  
  out <- stats::optimise(f = obj,
                         interval = c(0, 1),
                         R_target,
                         transition_matrix,
                         stable_age)
  out$minimum
  
  return(out$minimum)
}


disaggregate <- function(population, mask) {
  # disaggregate a population across age groups, based on the age groups in that
  # population and the national age distribution
  
  masked_age_distribution <- age_distribution * mask
  disaggregation <- masked_age_distribution / sum(masked_age_distribution)
  population * disaggregation
}

disaggregation_vec <- function(mask, distribution) {
  masked_distribution <- mask * distribution
  masked_distribution / sum(masked_distribution)
}

baseline_matrix <- function(
  R0 = 2.5,
  final_age_bin = 80,
  type = c("next generation", "exposure outcome", "contact")
  ) {

  type <- match.arg(type)
  
  # construct a next generation/exposure outcome/contact matrix for Australia
  # from Prem matrix
  
  # Prem 2017 contact matrix
  contact_matrix <- readxl::read_xlsx(
    path = "data/vaccinatinon/MUestimates_all_locations_1.xlsx",
    sheet = "Australia",
    col_types = rep("numeric", 16)
  ) %>%
    as.matrix
  
  # expand out to add an 80+ category the same as the 75-80 category
  matrix <- matrix(NA, 17, 17)
  matrix[17, 17] <- contact_matrix[16, 16]
  matrix[17, 1:16] <- contact_matrix[16, ]
  matrix[1:16, 17] <- contact_matrix[, 16]
  matrix[1:16, 1:16] <- contact_matrix
  
  # set names
  bin_names <- age_classes(80)$classes 
  dimnames(matrix) <- list(
    bin_names,
    bin_names
  )
  
  if (type != "contact") {
    
    age_disaggregation <- tibble::tribble(
      ~age_group_10y, ~age_group_5y,
      "0_9", "0-4",
      "0_9", "5-9",
      "10_19", "10-14",
      "10_19", "15-19",
      "20_29", "20-24",
      "20_29", "25-29",
      "30_39", "30-34", 
      "30_39", "35-39",
      "40_49", "40-44",
      "40_49", "45-49",
      "50_59", "50-54",
      "50_59", "55-59",
      "60_69", "60-64",
      "60_69", "65-69",
      "70+", "70-74",
      "70+", "75-79",
      "70+", "80+"
    )
    
    age_data_davies <- read_csv(
      "data/vaccinatinon/susceptibility_clinical_fraction_age_Davies.csv",
      col_types = cols(
        age_group = col_character(),
        rel_susceptibility_mean = col_double(),
        rel_susceptibility_median = col_double(),
        clinical_fraction_mean = col_double(),
        clinical_fraction_median = col_double()
      )
    )
    
    # calculate relative infectiousness (using relative differences in clinical
    # fraction by age and assumed relative infectiousness of asymptomatics) and
    # susceptibility by age
    asymp_rel_infectious <- 0.5
    age_contribution <- age_disaggregation %>%
      left_join(
        age_data_davies,
        by = c("age_group_10y" = "age_group")
      ) %>%
      mutate(
        rel_infectiousness = clinical_fraction_mean +
          asymp_rel_infectious * (1 - clinical_fraction_mean),
        rel_infectiousness = rel_infectiousness /
          max(rel_infectiousness),
        rel_susceptibility = rel_susceptibility_mean /
          max(rel_susceptibility_mean),
      ) %>%
      select(
        age_group_5y,
        rel_infectiousness,
        rel_susceptibility
      )
    
    # adjust infectiousness on columns and susceptibility on rows
    matrix <- sweep(matrix, 2, age_contribution$rel_infectiousness, FUN = "*")

    # for a next generation matrix (vector multiplier is distribution of
    # infectious state), apply susceptibility to rows. For an exposure outcome
    # matrix (vector multiplier is number of exposures age group is subjected
    # to) apply susceptibility to columns
    index <- switch(
      type,
      "next generation" = 1,
      "exposure outcome" = 2,
    )    
    matrix <- sweep(matrix, index, age_contribution$rel_susceptibility, FUN = "*")
    
  }
  
  # if a target R0 is provided, scale the matrix to match
  if (!is.null(R0)) {
    
    # calculate m - number of onward infections per relative contact 
    m <- find_m(
      R_target = R0,
      transition_matrix = matrix
    )
    
    matrix <- matrix * m
    
  }
  
  matrix
  
}

age_classes <- function(final_age_bin = 80, by = 5) {
  
  # compute age classes based on this spec
  ages_lower = seq(0, final_age_bin, by = by)
  n_ages <- length(ages_lower)
  ages_upper = ages_lower + by - 1
  ages_upper[n_ages] <- Inf
  
  age_classes <- c(
    paste(
      ages_lower[-n_ages],
      ages_upper[-n_ages],
      sep = "-"
    ),
    paste0(
      final_age_bin,
      "+"
    )
  )
  
  tibble::tibble(
    classes = age_classes,
    lower = ages_lower,
    upper = ages_upper
  )
  
}

get_age_distribution <- function(
  final_age_bin = 85,
  by = 5,
  population_total = 25693000
) {
  
  # check the final age bin in sensible
  if (final_age_bin > 85) {
    stop(
      "No age-specific population data for ages greater than 85",
      call. = TRUE
    )
  }
  
  ages <- age_classes(
    final_age_bin = final_age_bin,
    by = by
  )
  
  # Age structure of the Australian population by year of age, up to 100+
  # This is a "standard" distribution data frame but old population size data
  # from 2001 hence is adjusted later
  # aust_population_standard <- readxl::read_xls(
  #   path = "data/vaccinatinon/abs_standard_age_31010DO003_200106.xls",
  #   sheet = "Table_1",
  #   skip = 6,
  #   col_names = c(
  #     "age",
  #     "pop"
  #   )
  # ) %>%
  #   filter(age != "Total", !is.na(age), age != "© Commonwealth of Australia 2013") %>%
  #   mutate(
  #     age = case_when(
  #       age == "100 and over" ~ "100",
  #       TRUE ~ age
  #     ) %>%
  #       as.integer,
  #   )
  
  # use 2020 population, as this better matches proportion 80+
  aust_population_2020 <- readxl::read_xls(
    path = "data/vaccinatinon/abs_population_2020.xls",
    sheet = "Table_8",
    range = cell_rows(c(223:328)),
    col_names = as.character(1:10)
  ) %>%
    select(1, 10) %>%
    rename(
      age = "1",
      pop = "10"
    ) %>%
    select(
      age,
      pop
    ) %>%
    mutate(
      age = case_when(
        age == "85-89" ~ "85",
        age == "90-94" ~ "85",
        age == "95-99" ~ "85",
        age == "100 and over" ~ "85",
        TRUE ~ age
      )
    ) %>%
    filter(
      !grepl("-", age)
    ) %>%
    group_by(
      age
    ) %>%
    summarise(
      pop = sum(pop),
      .groups = "drop"
    ) %>%
    mutate(
      age = as.integer(age)
    ) %>%
    arrange(age)
  
  # aggregate into age classes and return
  age_class_fractions <- aust_population_2020 %>%
    mutate(
      age_class = cut(
        age,
        breaks = c(ages$lower - 1, Inf),
        labels = ages$classes
      ),
      age_class = as.character(age_class),
      age_class = factor(age_class, levels= unique(age_class))
    ) %>%
    group_by(
      age_class
    ) %>%
    summarise(
      pop = sum(pop),
      .groups = "drop"
    ) %>%
    mutate(
      fraction = pop / sum(pop),
      pop = fraction * population_total
    )
  
  age_class_fractions
  
}


phase_age_populations <- function() {
  
  # allocation leads to over vaccination in some groups - try to fix this!
  # also cap coverage (of single doses) at 2?
  # disaggregation vectors for different vaccination groups
  
  
  # get the age classes and age distributions in national population
  ages <- age_classes(final_age_bin = 80)
  national_distribution <- get_age_distribution(final_age_bin = 80)$fraction
  
  # remove under-15s from all vaccination (vaccines not approved for these ages)
  distribution <- disaggregation_vec(ages$lower >= 15, national_distribution)
  
  # disaggregation vectors (summing to 1) into age classes for different
  # population groups
  under_50 <- disaggregation_vec(ages$lower < 50, distribution)
  over_50 <- disaggregation_vec(ages$lower >= 50, distribution)
  working_under_50 <- disaggregation_vec(ages$lower >= 16 & ages$lower < 50,
                                         distribution)
  working_over_50 <- disaggregation_vec(ages$lower >= 50 & ages$lower < 65,
                                        distribution)
  over_50_under_65 <- disaggregation_vec(ages$lower >= 50 & ages$lower < 65, distribution)
  over_50_under_70 <- disaggregation_vec(ages$lower >= 50 & ages$lower < 70, distribution)
  over_65 <- disaggregation_vec(ages$lower >= 65, distribution)
  over_70_under_80 <- disaggregation_vec(ages$lower >= 70 & ages$lower < 80, distribution)
  over_80 <- disaggregation_vec(ages$lower >= 80, distribution)
  
  # population sizes in vaccination roll-out groups
  populations_1A <- list(
    aged_care_residents = 183000 * over_65,
    disability_residents_u50 = 5000 * under_50,
    disability_residents_o50 = 21000 * over_50_under_65,
    border_workers_u50 = 16000 * working_under_50,
    border_workers_o50 = 11000 * working_over_50,
    care_staff_u50 = 143000 * working_under_50,
    care_staff_o50 = 107000 * working_over_50,
    health_staff_priority_u50 = 222000 * working_under_50,
    health_staff_priority_o50 = 90000 * working_over_50
  )
  
  populations_1B <- list(
    elderly_o80 = 915000 * over_80,
    elderly_70_79 = 1857000 * over_70_under_80,
    health_staff_other_u50 = 267000 * working_under_50,
    health_staff_other_o50 = 126000 * working_over_50,
    atsi_o50 = 91000 * over_50_under_70,
    medical_condition_u50 = 896000 * under_50,
    medical_condition_o50 = 1167000 * over_50_under_70,
    priority_workers_u50 = 201000 * working_under_50,
    priority_workers_o50 = 67000 * working_over_50
  )
  
  # combine these for each phase
  list(
    phase_1A = Reduce(`+`, populations_1A),
    phase_1B = Reduce(`+`, populations_1B)
  )
  
}



doses_by_age <- function(n_doses, age_populations) {
  
  # assuming a single dose per person, preferential allocation to group 1A
  # and then subsequent partial coverage in 1B, guesstimate the number of doses
  # given out in each age group coverage in each age group
  population_1A <- sum(age_populations$phase_1A)
  population_1B <- sum(age_populations$phase_1B)
  n_doses_1A <- min(n_doses, population_1A)
  age_doses_1A <- n_doses_1A * age_populations$phase_1A / population_1A
  n_doses_1B <- n_doses - n_doses_1A
  age_doses_1B <- n_doses_1B * age_populations$phase_1B / population_1B
  
  # combine into total doses so far
  age_doses_1A + age_doses_1B
  
}

# compute the vaccine efficacy on overall transmission by combining the 
# efficacies in preventing iinfection, and prevensting onwards transmission
# for breakthrough infections
combine_efficacy <- function(infection, transmission) {
  1 - ((1 - infection) * (1 - transmission)) 
}


# proportion_pf and proportion_2_dose are only used if the other proportion
# arguments are not specified
average_efficacy <- function(
  efficacy_pf_2_dose = 0.9685,
  efficacy_az_2_dose = 0.93,
  efficacy_pf_1_dose = 0.8317,
  efficacy_az_1_dose = 0.892,
  proportion_pf = 0.5,
  proportion_2_dose = 0.2,
  proportion_pf_2_dose = proportion_pf * proportion_2_dose,
  proportion_az_2_dose = (1 - proportion_pf) * proportion_2_dose,
  proportion_pf_1_dose = proportion_pf * (1 - proportion_2_dose),
  proportion_az_1_dose = (1 - proportion_pf) * (1 - proportion_2_dose)
) {
  
  # based on Pritchard MedRXiv / Harris via ATAGI advice paper
  # single dose calculations
  # AZ: 1 - (1 - 0.64) * (1 - 0.47) = 0.809
  # PF: 1 - (1 - 0.67) * (1 - 0.49) = 0.832
  
  # AZ: 1 - (1 - 0.8) * (1 - 0.65) = 0.93
  # Pf: 1 - (1 - 0.91) * (1 - 0.65) = 0.9685
  
  # compute the average efficacy of doses, given the proportion of vaccines from
  # each provider, and the proportion people who are vaccined with 2 doses - these
  # are estiamtes for b.1.1.7, from James Wood's email
  
  # proportion of each vaccine
  proportion_az <- 1 - proportion_pf
  
  # proportion fully dosed
  proportion_1_dose <- 1 - proportion_2_dose # may need edit when 
  
  efficacy_mean <- proportion_pf_2_dose * efficacy_pf_2_dose +
    proportion_pf_1_dose * efficacy_pf_1_dose +
    proportion_az_2_dose * efficacy_az_2_dose +
    proportion_az_1_dose * efficacy_az_1_dose 
  
  efficacy_mean 
  
}

average_ifr_efficacy <- function() {
  average_efficacy(
    efficacy_pf_2_dose = 0.95,
    efficacy_az_2_dose = 0.95,
    efficacy_pf_1_dose = 0.8,
    efficacy_az_1_dose = 0.8
  )
}

average_transmission_efficacy <- function() {
  average_efficacy(
    efficacy_pf_2_dose = 0.9685,
    efficacy_az_2_dose = 0.93,
  )
}


vaccination_transmission_effect <- function(
  age_coverage,
  efficacy_mean,
  next_generation_matrix,
  R0 = NULL
) {
  # given vaccination coverage in each age group, the average vaccine efficacy (by
  # age or overall), and the baseline next generation matrix, compute the
  # reduction in transmission for each age and overall
  
  age_transmission_reduction <- 1 - age_coverage * efficacy_mean
  vc_next_gen_matrix <- sweep(
    next_generation_matrix,
    2,
    age_transmission_reduction,
    FUN = "*"
  )
  
  if (is.null(R0)) {
    R0 <-  get_R(next_generation_matrix)
  }
  Rv <- get_R(vc_next_gen_matrix)
  overall <- Rv /R0
  
  list(
    by_age = age_transmission_reduction,
    overall = overall
  )
  
}

vaccination_ifr_effect <- function(
  age_coverage,
  efficacy_mean,
  ifr
) {
  
  age_structure <- get_age_distribution(80)
  
  # compute age-specific IFRs, post vaccination
  age_reduction <- 1 - (age_coverage * efficacy_mean)
  age_odriscoll <- ifr$odriscoll * age_reduction
  age_brazeau <- ifr$brazeau * age_reduction
  
  overall_odriscoll <- sum(age_structure$fraction * age_odriscoll)
  overall_brazeau <- sum(age_structure$fraction * age_brazeau)
  
  list(
    age_reduction = age_reduction,
    age = list(
      odriscoll = age_odriscoll,
      brazeau = age_brazeau
    ),
    overall = list(
      odriscoll = overall_odriscoll,
      brazeau = overall_brazeau
    )
  )
  
}


summarise_effect <- function(
  n_doses,
  age_populations,
  age_distribution,
  next_generation_matrix,
  ifr
) {
  
  # Given a number of doses, compute vaccine coverage in each age group, the
  # effect on reducing transmission among the whole population, and the efect on
  # reducing transmission in the most-vaccinated age group
  
  # compute coverage
  age_doses <- doses_by_age(n_doses, age_populations)
  age_coverage <- age_doses / age_distribution$pop
  
  # compute effect on transmission  
  transmission_effect <- vaccination_transmission_effect(
    age_coverage = age_coverage,
    efficacy_mean = average_transmission_efficacy(),
    next_generation_matrix = next_generation_matrix
  )
  
  # compute effect on IFR
  ifr_effect <- vaccination_ifr_effect(
    age_coverage = age_coverage,
    efficacy_mean = average_ifr_efficacy(),
    ifr = ifr
  )
  
  list(
    coverage_by_age = age_coverage,
    transmission = transmission_effect,
    ifr = ifr_effect
  )
  
}



overall_transmission_effect <- function (
  n_doses,
  age_populations,
  age_distribution,
  next_generation_matrix,
  ifr
) {
  all_effects <- summarise_effect(
    n_doses,
    age_populations,
    age_distribution,
    next_generation_matrix,
    ifr
  )
  all_effects$transmission$overall
}

overall_ifr_effect <- function (
  n_doses,
  age_populations,
  age_distribution,
  next_generation_matrix,
  ifr
) {
  all_effects <- summarise_effect(
    n_doses,
    age_populations,
    age_distribution,
    next_generation_matrix,
    ifr
  )
  all_effects$overall_ifr_reduction
}


extract_overall_transmission_effect <- function(x) {
  # extract results for the overall transmission effect from a list of outputs
  # from summarise_effect
  x$transmission$overall
}


extract_overall_ifr_effect <- function(x, which = c("odriscoll", "brazeau")) {
  # extract results for the population-wide IFR from a list of outputs from
  # summarise_effect
  which <- match.arg(which)
  x$ifr$overall[[which]]
}

extract_over_70_ifr_effect <- function(x, which = c("odriscoll", "brazeau")) {
  # extract age-specific IFRs after vaccination  from a list of outputs from
  # summarise_effect, then population weight them to represent the IFR for the
  # population over 70
  
  which <- match.arg(which)
  age_effects <- x$ifr$age[[which]]
  weighting <- get_age_distribution(80) %>%
    mutate(
      mask = case_when(
        age_class %in% c("70-74", "75-79", "80+") ~ 1,
        TRUE ~ 0
      ),
      weights = mask * fraction,
      weights = weights / sum(weights)
    ) %>%
    pull(weights)
  
  sum(age_effects * weighting)
}


simulate_variant <- function(
  .fitted_model = fitted_model,
  dir = "outputs/projection/",
  subdir = NA,
  ratio_samples = FALSE,
  variant = c("wt", "alpha", "delta")
){
  
  variant <- match.arg(variant)
  
  if(is.na(subdir)){
    subdir <- variant
  }
 
  data <- .fitted_model$data
  dates <- .fitted_model$data$dates$mobility
  
  de <- .fitted_model$greta_arrays$distancing_effect
  
  p <- de$p
  
  prop_var <- prop_variant(dates = dates)
  prop_alpha <- prop_var$prop_alpha
  prop_delta <- prop_var$prop_delta
  prop_wt    <- prop_var$prop_wt
  
  phi_alpha       <- normal(1.454, 0.055, truncation = c(0, Inf))
  phi_delta_alpha <- normal(1.421, 0.033, truncation = c(0, Inf))
  
  phi_delta <- phi_alpha * phi_delta_alpha
  
  # phi_star <- prop_wt * 1 + prop_alpha * phi_alpha + prop_delta * phi_delta
  # 
  # p <- p_star ^ (1/phi_star)
  
  if(variant == "wt") {
    prop_wt_hat    <- prop_wt    * 0 + 1
    prop_alpha_hat <- prop_alpha * 0 
    prop_delta_hat <- prop_delta * 0
  }else if(variant == "alpha") {
    prop_wt_hat    <- prop_wt    * 0
    prop_alpha_hat <- prop_alpha * 0 + 1
    prop_delta_hat <- prop_delta * 0
  } else if(variant == "delta") {
    prop_wt_hat    <- prop_wt    * 0
    prop_alpha_hat <- prop_alpha * 0
    prop_delta_hat <- prop_delta * 0 + 1
  } 
  
  phi_hat <- prop_wt_hat * 1 + prop_alpha_hat * phi_alpha + prop_delta_hat * phi_delta
  
  p_hat <- p ^ phi_hat
  
  infectious_days <- infectious_period(gi_cdf)
  
  h_t <- h_t_state(dates)
  HD_t <- de$HD_0 * h_t
  
  household_infections <- de$HC_0 * (1 - p_hat ^ HD_t)
  non_household_infections <- de$OC_t_state * de$gamma_t_state *
    infectious_days * (1 - p_hat ^ de$OD_0)
  R_t <- household_infections + non_household_infections
  R_eff_loc_1_no_surv <- extend(R_t, data$n_dates_project)
  
  
  # multiply by the surveillance effect to get component 1
  surveillance_reff_local_reduction <- surveillance_effect(
    dates = data$dates$infection_project,
    cdf = gi_cdf,
    states = data$states
  )
  # 
  
  wt_fitted_model <- .fitted_model
  wt_fitted_model$greta_arrays$R_eff_loc_1 <- R_eff_loc_1_no_surv * surveillance_reff_local_reduction
  
  
  dir.create(dir, showWarnings = FALSE)
  write_reff_sims(wt_fitted_model, paste(dir, subdir, sep = "/"), write_reff_12 = FALSE)
  
  if(ratio_samples) {
    ratio <- wt_fitted_model$greta_arrays$R_eff_loc_1 / .fitted_model$greta_arrays$R_eff_loc_1
    ratio_vec <- c(ratio)
    ratio_sims <- calculate(ratio_vec, values = .fitted_model$draws, nsim = 2000)
    ratio_samples <- t(ratio_sims[[1]][, , 1])
    colnames(ratio_samples) <- paste0("sim", 1:2000)
    
    tibble(
      date = rep(.fitted_model$data$dates$infection_project, .fitted_model$data$n_states),
      state = rep(.fitted_model$data$states, each = .fitted_model$data$n_dates_project),
    ) %>%
      mutate(date_onset = date + 5) %>%
      cbind(ratio_samples) %>%
      write_csv(
        file.path(dir, "r_eff_1_ratio_samples.csv")
      )
    
    # read_csv("outputs/projection/wild_type/r_eff_1_local_samples.csv") %>%
    #   filter(date == as.Date("2020-04-11")) %>%
    #   pivot_longer(
    #     cols = starts_with("sim"),
    #     names_to = "sim"
    #   ) %>%
    #   group_by(state, date) %>%
    #   summarise(
    #     mean = mean(value),
    #     median = median(value),
    #     lower = quantile(value, 0.05),
    #     upper = quantile(value, 0.95),
    #     p_exceedance = mean(value > 1)
    #   )
  }
  
}

# given a timeseries of cumulative doses (for contiguous consecutive dates), and
# an assumed ideal inter-dose period, return the timeseries of the cumulative
# number of people being fully-vaccinated (having received both doses), assuming
# that second-doses are prioritised for vaccination and are always given at the
# earliest availability after the inter-dose period has elapsed.
model_vaccination_coverage <- function (daily_new_doses, inter_dose_days = 28) {
  
  # empty vectors for the number of first and second doses distributed
  first_doses <- second_doses <- daily_new_doses * 0
  
  # empty scalar for the queue of people due to receive their second dose but
  # who could not yet receive it due to insufficient doses availables
  queueing_for_second_dose <- 0
  
  # iterate through days, 
  for (day in seq_along(daily_new_doses)) {
    
    # the number of new second doses eligible today
    lagged_day <- day - inter_dose_days
    if (lagged_day >= 1) {
      target_second_doses <- first_doses[lagged_day]
    } else {
      target_second_doses <- 0
    }
    
    # add on any waiting from previous days
    target_second_doses <- target_second_doses + queueing_for_second_dose
    
    # calculate the maximum number of second doses that can be given out
    # (assuming these are the priority)
    second_doses[day] <- min(daily_new_doses[day], target_second_doses)
    
    # tally up any second doses in waiting that were not vaccinated today, to
    # wait for the next day
    queueing_for_second_dose <- max(0, target_second_doses - second_doses[day])
    
    # assign any remaining doses as first doses
    first_doses[day] <- daily_new_doses[day] - second_doses[day]
    
  }
  
  # cumulative number of fully-vaccinated people
  cumsum(second_doses)
  
}

# get a dataframe of vaccine coverage estimates
vaccination_coverage <- function() {
  
  # scrape a timeseries of the cumulative number of doses given out
  scrape_doses_timeseries() %>%
    # apply an allocation model to compute the cumulative number of people
    # receiving their second dose (fully vaccinated)
    mutate(
      fully_vaccinated = model_vaccination_coverage(new_doses)
    ) %>%
    # get the fraction of vaccinated people who are fully vaccinated
    mutate(
      partially_vaccinated = doses - (fully_vaccinated * 2),
      vaccinated = fully_vaccinated + partially_vaccinated,
      proportion_2_dose = fully_vaccinated / vaccinated
    )
  
}


hist_prior_posterior <- function(greta_array, draws, nsim = 1000, ...)  {
  
  prior_sim <- calculate(greta_array, nsim = nsim)[[1]]
  posterior_sim <- calculate(greta_array, values = draws, nsim = nsim)[[1]]
  
  prior_sim <- c(prior_sim)
  posterior_sim <- c(posterior_sim)
  xlim <- range(c(prior_sim, posterior_sim))
  
  op <- par()
  on.exit(par(op))
  par(mfrow = c(1, 2))
  
  hist(c(prior_sim), xlim = xlim, main = "Prior", ...)
  hist(c(posterior_sim), xlim = xlim, main = "Posterior", ...)
  
}

read_reff_samples <- function(
  sample.file
){
  
  read.csv(sample.file) %>%
    as_tibble %>%
    pivot_longer(
      cols = starts_with("sim"),
      values_to = "value",
      names_to = "name"
    )  %>%
    group_by(date, state) %>%
    summarise(
      med = median(value),
      lw5 = quantile(value, 0.05),
      up95 = quantile(value, 0.95),
      lw25 = quantile(value, 0.25),
      up75 = quantile(value, 0.75)
    ) %>%
    ungroup %>%
    mutate(date = as.Date(as.character(date)))
}


write_mobility_dates <- function(mobility, dir = "outputs/"){
  mobility %>%
    group_by(datastream) %>%
    summarise(
      latest = max(date),
      earliest = min(date)
    ) %>%
    write_csv(
      file = file.path(dir, 'mobility_dates.csv')
    )
}


lga_age_population <- function() {
  file <- "data/population/32350ds0003_2020.xls"
  names_1 <- file %>%
    read_excel(
      sheet = "Table 3",
      skip = 8,
      n_max = 1
    ) %>%
    names()
  names_2 <- file %>%
    read_excel(
      sheet = "Table 3",
      skip = 7,
      n_max = 1
    ) %>%
    names()
  
  names <- c(names_1[1:4], names_2[-(1:4)])
  pop <- file %>%
    read_excel(
      sheet = "Table 3",
      skip = 10,
      col_names = names
    ) %>%
    select(
      -`S/T code`,
      -`Total persons`
    ) %>%
    rename(
      `85+` = `85 and over`,
      state = `S/T name`,
      LGA_NAME19 = `LGA name`,
      LGA_CODE19 = `LGA code`
    ) %>%
    pivot_longer(
      cols = -c(state, LGA_NAME19, LGA_CODE19),
      names_to = "age",
      values_to = "population"
    ) %>%
    # death to em-dashes
    mutate(
      age = str_replace(age, "–", "-")
    )
  
}


# Account for lag in vaccination effect.

# For those with only 1 dose, no increase for 1 week, then linear increase from
# 0% to 100% over the next two weeks. For those with 2 doses, linear increase
# from 0% to 100% over the next two weeks.

# Compute a correction factor to downgrade the coverage (by number of doses)
# based on the fraction of all people with that number of doses that received it
# at each point in time the last 2-3 weeks.

# note that for first doses, this needs to be *all* first doses, since otherwise
# people leave and screw up the calculation
immunity_lag_correction <- function(date, coverage,
                                    weeks_increase = 2,
                                    weeks_wait = 1) {
  
  # compute the current coverage (by dose/vaccine)
  max_date <- which.max(date)
  latest_coverage <- coverage[max_date]
  
  # compute the diff of coverages for all dates to get the proportion of the
  # population added on/by that date
  new_coverages <- diff(c(0, coverage))
  
  # compute the ratio of the proportion added on each previous date to the
  # current coverage (should sum to 1)
  date_weights <- new_coverages / latest_coverage
  
  # multiply each of those ratios by the relative effect based on the date difference¿
  week_diff <- as.numeric(date[max_date] - date) / 7
  relative_effect <- pmax(0, pmin(1, (week_diff - weeks_wait) / weeks_increase))
  
  # sum the ratios to get the correction multiplier
  correction <- sum(relative_effect * date_weights)
  
  if (is.na(correction)) {
    correction <- 0
  }
  
  correction
  
}

average_daily_doses <- function (
  air_current,
  latest_data_date = max(air_current$date),
  previous_days_average = 0:27
) {
  
  air_current %>%
    filter(
      date %in% (latest_data_date - previous_days_average)
    ) %>%
    group_by(
      lga, age_air_80,
    ) %>%
    summarise(
      across(
        starts_with("dose"),
        ~ (max(.x) - min(.x)) / n()
      ),
      .groups = "drop"
    )
  
}

# turn the forecast into a function, and add on a method to add extra vaccinations in some LGAs/weeks
forecast_vaccination <- function(
  # current vaccination coverage timeseries
  air_current,
  # which previous days to average over in computing daily vaccination rate
  # (default previous 4 weeks)
  previous_days_average = 0:27,
  # end of simulations
  max_date = as.Date("2021-09-30"),
  # inter-dose intervals - assumed the same everywhere
  pfizer_interval_weeks = 3,
  az_interval_weeks = 8,
  # proportion of population accepting vaccines
  max_coverages = NULL,
  # optional file of additional doses
  extra_doses = NULL,
  # the name of this scenario
  scenario_name = "baseline"
) {
  
  if (is.null(max_coverages)) {
    max_coverages <- air_current %>%
      distinct(age_air_80) %>%
      mutate(
        hypothetical_max_coverage = 1
      )
  }
  
  latest_data_date <- max(air_current$date)
  
  # correct the populations in air current to account for eligibility of different age groups
  air_current <- air_current %>%
    mutate(
      # correct age group populations for eligibility of ages within them, and
      # the rate of vaccine acceptance
      eligible_population = case_when(
        age_air_80 %in% c("0-11") ~ 0,
        TRUE ~ population
      )
    )
  
  # compute daily average numbers of doses in each age group and lga over the past weeks
  # start with an average, then try a random effects model (shrinkage & extrapolation will help for small populations)
  dailies <- average_daily_doses(
    air_current = air_current,
    latest_data_date = latest_data_date,
    previous_days_average = previous_days_average
  )
  
  # cumulative number of doses as the most recent time point
  starting <- air_current %>%
    filter(
      date == latest_data_date
    ) %>%
    rename_with(
      .fn = function(x) paste0("starting_", x),
      .cols = starts_with("dose")
    ) %>%
    select(
      -date
    )
  
  # compute maximum coverages by age and LGA (allowing them to exceed the
  # eligible population coverages in the observed data)
  max_coverage <- air_current %>%
    filter(
      date == max(date)
    ) %>%
    # compute coverage as at the latest date for each age and LGA (capped at 100%)
    mutate(
      any_doses = dose_1_Pfizer + dose_1_AstraZeneca,
      observed_max_coverage = any_doses / eligible_population,
      observed_max_coverage = pmin(1, observed_max_coverage),
      # some ages have zero eligible population, so set observed max coverage to 0
      observed_max_coverage = replace_na(observed_max_coverage, 0),
    ) %>%
    select(
      lga, age_air_80, observed_max_coverage
    ) %>%
    left_join(
      max_coverages,
      by = c("age_air_80")
    ) %>%
    # compute the maximum of the two for capping vaccination
    mutate(
      max_coverage = pmax(observed_max_coverage, hypothetical_max_coverage)
    ) %>%
    select(
      -observed_max_coverage
    )
  
  # future dates for each LGA and age, based on recent rate of vaccination
  future_doses <- expand_grid(
    lga = unique(air_current$lga),
    age_air_80 = unique(air_current$age_air_80),
    date = seq(latest_data_date + 1, max_date, by = 1)
  ) %>%
    # add on daily vaccination rates (previous average)
    left_join(
      dailies,
      by = c("lga", "age_air_80")
    )

  # optionally add on some extra doses to represent an alternate vaccination
  # scenario
  if (!is.null(extra_doses)) {
    
    future_doses <- future_doses %>%
      bind_rows(
        extra_doses
      ) %>%
      group_by(
        lga, age_air_80, date,# eligible_population
      ) %>%
      summarise(
        across(
          starts_with("dose"),
          sum
        ),
        .groups = "drop"
      ) %>%
      filter(
        date <= max_date
      )
    
  }
  
  # compute the cumulative sum to get total future doses by each day
  air_forecast <- future_doses %>%
    arrange(lga, age_air_80, date) %>%
    group_by(
      lga,
      age_air_80,
      # eligible_population
    ) %>%
    mutate(
      across(
        starts_with("dose"),
        cumsum
      )
    ) %>%
    # add on current observed total number of doses
    left_join(
      starting,
      by = c("lga", "age_air_80")
    ) %>%
    mutate(
      dose_1_AstraZeneca = dose_1_AstraZeneca + starting_dose_1_AstraZeneca,
      dose_1_Pfizer = dose_1_Pfizer + starting_dose_1_Pfizer,
      dose_2_AstraZeneca = dose_2_AstraZeneca + starting_dose_2_AstraZeneca,
      dose_2_Pfizer = dose_2_Pfizer + starting_dose_2_Pfizer
    ) %>%
    select(
      -starts_with("starting")
    ) %>%
    # add on a forecast flag
    mutate(
      forecast = TRUE
    )
  
  # add the forecast to the current air data to get the full time series
  # compute the weighted number of cumulative dose 2s (interpolating linearly
  # between that expected under the current dose 2 vaccination rate and the
  # lagged dose 1 cumulative count)
  air_current_future <- air_current %>%
    mutate(
      forecast = FALSE
    ) %>%
    bind_rows(
      air_forecast
    ) %>%
    # compute the daily doses
    group_by(
      lga,
      age_air_80
    ) %>%
    arrange(
      date
    ) %>%
    mutate(
      # compute lagged dose 1s to get more mechanistic forecast of dose 2s
      dose_2_AstraZeneca_lag = lag(dose_1_AstraZeneca, az_interval_weeks * 7),
      dose_2_Pfizer_lag = lag(dose_1_Pfizer, pfizer_interval_weeks * 7),
      # and weight them with the ones based on the current rates, to get a
      # smoother transition over 7 days
      dose_2_recent_weight = pmax(0, 1 - pmin(1, as.numeric(date - latest_data_date) / 14)),
      dose_2_AstraZeneca = if_else(
        forecast,
        dose_2_AstraZeneca * dose_2_recent_weight + dose_2_AstraZeneca_lag * (1 - dose_2_recent_weight),
        dose_2_AstraZeneca
      ),
      dose_2_Pfizer = if_else(
        forecast,
        dose_2_Pfizer * dose_2_recent_weight + dose_2_Pfizer_lag * (1 - dose_2_recent_weight),
        dose_2_Pfizer
      )
    ) %>%
    select(
      -ends_with("lag"),
      -dose_2_recent_weight
    ) %>%
    mutate(
      across(
        starts_with("dose"),
        round
      )
    )
    
  # add on the max coverages (with different thresholds)
  air_saturated <- air_current_future %>%
    full_join(
      max_coverage,
      by = c("lga", "age_air_80")
    ) %>%
    arrange(date, lga, age_air_80) %>%
    mutate(
      accepting_population = eligible_population * max_coverage
    ) %>%
    # where the number of vaccinations exceeds the maximum population coverage, cap it
    mutate(
      # compute extra doses
      dose_1_extra = pmax(0, (dose_1_AstraZeneca + dose_1_Pfizer) - accepting_population),
      dose_2_extra = pmax(0, (dose_2_AstraZeneca + dose_2_Pfizer) - accepting_population)
    )
  
  # compute fraction of doses that are Pfizer - *as at the date the
  # population was saturated* otherwise it will keep adjusting
  # retrospectively
  air_final_fraction_dose_1 <- air_saturated %>%
    filter(
      dose_1_extra == 0
    ) %>%
    group_by(lga, age_air_80, hypothetical_max_coverage) %>%
    filter(
      date == max(date)
    ) %>%
    ungroup() %>%
    mutate(
      dose_1_Pfizer_fraction = dose_1_Pfizer / (dose_1_AstraZeneca + dose_1_Pfizer),
    ) %>%
    select(
      lga, age_air_80, hypothetical_max_coverage, dose_1_Pfizer_fraction 
    )
  
  air_final_fraction_dose_2 <- air_saturated %>%
    filter(
      dose_2_extra == 0
    ) %>%
    group_by(lga, age_air_80, hypothetical_max_coverage) %>%
    filter(
      date == max(date)
    ) %>%
    ungroup() %>%
    mutate(
      dose_2_Pfizer_fraction = dose_2_Pfizer / (dose_2_AstraZeneca + dose_2_Pfizer)
    ) %>%
    select(
      lga, age_air_80, hypothetical_max_coverage, dose_2_Pfizer_fraction 
    )
  
  # cap these extra doses, keeping the allocation between AZ and Pfizer constant after saturation   
  air <- air_saturated %>%
    left_join(
      air_final_fraction_dose_1,
      by = c("lga", "age_air_80", "hypothetical_max_coverage")
    ) %>%
    left_join(
      air_final_fraction_dose_2,
      by = c("lga", "age_air_80", "hypothetical_max_coverage")
    ) %>%
    mutate(
      
      # fill in any missing fractions (those that have not yet reached saturation)
      dose_1_Pfizer_fraction = replace_na(dose_1_Pfizer_fraction, 0),
      dose_2_Pfizer_fraction = replace_na(dose_2_Pfizer_fraction, 0),
      
      # compute the number of Pfizer doses at saturation
      dose_1_Pfizer_maximum = (accepting_population * dose_1_Pfizer_fraction),
      dose_1_AstraZeneca_maximum = (accepting_population * (1 - dose_1_Pfizer_fraction)),
      dose_2_Pfizer_maximum = (accepting_population * dose_2_Pfizer_fraction),
      dose_2_AstraZeneca_maximum = (accepting_population * (1 - dose_2_Pfizer_fraction)),
      
      # compute the numbers of excess doses
      dose_1_Pfizer_extra = pmax(0, dose_1_Pfizer - dose_1_Pfizer_maximum),
      dose_1_AstraZeneca_extra = pmax(0, dose_1_AstraZeneca - dose_1_AstraZeneca_maximum),
      dose_2_Pfizer_extra = pmax(0, dose_2_Pfizer - dose_2_Pfizer_maximum),
      dose_2_AstraZeneca_extra = pmax(0, dose_2_AstraZeneca - dose_2_AstraZeneca_maximum),
      
      # remove the excess doses
      dose_1_Pfizer = dose_1_Pfizer - dose_1_Pfizer_extra,
      dose_1_AstraZeneca = dose_1_AstraZeneca - dose_1_AstraZeneca_extra,
      dose_2_Pfizer = dose_2_Pfizer - dose_2_Pfizer_extra,
      dose_2_AstraZeneca = dose_2_AstraZeneca - dose_2_AstraZeneca_extra
      
    )
  
  air %>%
    mutate(scenario = scenario_name) %>%
    select(
      -ends_with("maximum")
    ) %>%
    ungroup()
  
}

# scenario for a bolus of extra Pfizer dose 1s delivered to a given age group
# and set of lgas over a given time period, with rate based on previous observed
# rates
extra_pfizer <- function (
  air_current,
  dose_1_dates = max(air_current$date) + 1:21,
  n_extra_pfizer = 670000,
  target_ages_air = c("15-29", "30-39"),
  target_lgas = lgas_of_concern,
  previous_days_average = 0:27,
  dose_interval = 8 * 7
) {
  
  air_current %>%
    # get average daily doses
    average_daily_doses(
      previous_days_average = previous_days_average
    ) %>%
    # filter to only LGAs of concern, and age groups of concern
    filter(
      lga %in% target_lgas,
      age_air_80 %in% target_ages_air
    ) %>%
    # mask out types of dose not required
    mutate(
      across(
        c(starts_with("dose_2"), ends_with("AstraZeneca")),
        ~ .x * 0
      )
    ) %>%
    # add on range of dates on which to overload doses
    full_join(
      expand_grid(
        lga = target_lgas,
        date = dose_1_dates
      ),
      by = "lga"
    ) %>%
    # normalise and multiply by number of doses
    mutate(
      dose_1_Pfizer = n_extra_pfizer * dose_1_Pfizer / sum(dose_1_Pfizer)
    ) %>%
    # add on dose 2s the required number of days into the future
    add_pfizer_dose_2s(
      dose_interval = dose_interval
    )
  
}

# given daily dose 1 allocations, assign their dose 2 allocations 'dose_interval' days into the future
add_pfizer_dose_2s <- function(
  dose_1s,
  dose_interval = 8 * 7
) {
  
  if (is.finite(dose_interval)) {
    # add 670K more dose 2s delivered some weeks after this to get the full scenario
    dose_2s <- dose_1s %>%
      mutate(
        dose_2_Pfizer = dose_1_Pfizer,
        dose_1_Pfizer = 0,
        date = date + dose_interval
      )
  } else{
    dose_2s <- NULL
  }
  
  bind_rows(
    dose_1s,
    dose_2s
  )
  
}

# compute excess Pfizer dose 1s from the bolus under this scenario
extra_pfizer_dose_1_excess <- function (
  extra_pfizer,
  air
) {
  extra_pfizer %>%
    filter(dose_1_Pfizer > 0) %>%
    arrange(lga, age_air_80, date) %>%
    group_by(lga, age_air_80) %>%
    mutate(
      across(
        starts_with("dose"),
        cumsum
      )
    ) %>%
    ungroup() %>%
    full_join(
      air %>%
        filter(
          lga %in% extra_pfizer$lga,
          age_air_80 %in% extra_pfizer$age_air_80,
          date %in% extra_pfizer$date
        ) %>%
        select(
          c(date, lga, age_air_80, coverage_scenario, ends_with("extra"))
        ),
      by = c("date", "lga", "age_air_80")
    ) %>%
    mutate(
      dose_1_Pfizer_scenario_extra = pmin(dose_1_Pfizer_extra, dose_1_Pfizer)
    ) %>%
    select(
      lga, age_air_80, coverage_scenario, date, dose_1_Pfizer_scenario_extra
    ) %>%
    # collapse across age groups and LGAs
    group_by(date, coverage_scenario) %>%
    summarise(
      dose_1_Pfizer_scenario_extra = sum(dose_1_Pfizer_scenario_extra),
      .groups = "drop"
    )
  
}

# compute the number of 12-15 year olds in the stated LGAs, and the fraction
# of them in each of the two AIR age bins
child_pop <- function(lgas) {
  lga_child_pop <- pop_air %>%
    filter(
      age_air %in% c("0-14", "15-29"),
      LGA_NAME19 %in% lgas
    ) %>%
    group_by(age_air) %>%
    summarise(
      population = sum(population)
    ) %>%
    pivot_wider(
      names_from = age_air,
      values_from = population
    ) %>%
    mutate(
      # compute the number of 12-15 year olds
      `12-14` = `0-14` * pop_disagg$fraction_0_14_eligible_child,
      `15` = `15-29` * pop_disagg$fraction_15_29_eligible_child,
      `12-15` = `12-14` + `15`,
      # compute the fraction of each bin that are eligible
      fraction_12_15_in_0_14 = `12-14` / `0-14`,
      fraction_12_15_in_15_29 = `15` / `15-29`,
    )
}

# give out extra doses to all 12-15 year olds in the stated lgas
extra_pfizer_child <- function (
  air_current,
  dose_1_dates,
  n_extra_pfizer,
  target_adult_ages_air = c("15-29", "30-39"),
  target_lgas = lgas_of_concern,
  previous_days_average,
  n_child_doses = lga_child_pop(lgas)$`12-15`
) {
  
  lga_child_pop <- child_pop(target_lgas)
  
  # give out 140K dose 1s to 12-15 year olds in the LGAs of concern, proportionally to population
  child_doses <- extra_pfizer(
    air_current = air_current,
    dose_1_dates = dose_1_dates,
    n_extra_pfizer = 0,
    target_ages_air = c("0-14", "15-29"),
    target_lgas = target_lgas,
    previous_days_average = previous_days_average,
    dose_interval = Inf
  ) %>%
    left_join(
      pop_air,
      by = c(age_air_80 = "age_air", lga = "LGA_NAME19")
    ) %>%
    select(
      -LGA_CODE19
    ) %>%
    # adjust populations to get the number of 12-15 year olds in each age bin,
    # then compute fraction of 12-15 population in each AIR age band and LGA
    mutate(
      population = case_when(
        age_air_80 == "0-14" ~ population * lga_child_pop$fraction_12_15_in_0_14,
        age_air_80 == "15-29" ~ population * lga_child_pop$fraction_12_15_in_15_29
      ),
      fraction = population / sum(population)
    ) %>%
    # allocate Pfizer dose 1s to these LGAs
    mutate(
      dose_1_AstraZeneca = 0,
      dose_1_Pfizer = fraction * n_child_doses,
      dose_2_AstraZeneca = 0,
      dose_2_Pfizer = 0,
    ) %>%
    select(
      -population,
      -fraction
    ) %>%
    add_pfizer_dose_2s(
      dose_interval = 8 * 7
    )
  
  # give out remaining dose 1s to 16-39 year olds in LGAs of concern
  adult_doses <- extra_pfizer(
    air_current = air_current,
    dose_1_dates = dose_1_dates,
    n_extra_pfizer = pmax(0, n_extra_pfizer - n_child_doses),
    target_ages_air = target_adult_ages_air,
    target_lgas = target_lgas,
    previous_days_average = previous_days_average,
    dose_interval = 8 * 7
  )
  
  # join these, and sum across ages since there are two different 15-29
  # allocations. Note that the daily number of doses in 15-29 is an allocation
  # based on the population of 15 year olds, plus an allocation of additional
  # doses for the 15-29 age category as a whole, based on the rate of vaccination
  # in this age band. If only 16+ were vaccinated previously, this would be
  # identical to doing allocation on 15 year olds (based on population) and on
  # 16-29 year olds (based on previous rate). In reality, a small number of
  # priority 15 year olds will already have been vaccinated so this will lead to a
  # slightly too-fast allocation in those LGAs. But this should be a very small
  # bias, irrelevant at the scale of all 12 LGAs combined, and irrelevant by the
  # time the population is saturated.
  bind_rows(
    child_doses,
    adult_doses,
  ) %>%
    group_by(
      lga, age_air_80, date
    ) %>%
    summarise(
      across(
        starts_with("dose"),
        sum
      ),
      .groups = "drop"
    )
}


# lookup to disaggregate coverages to 5y age groups
age_lookup <- function() {
  tibble::tribble(
    ~age_5y, ~age_air, ~age_abs, ~age_air_80,
    "0-4", "0-14", "0-4", "0-14",
    "5-9", "0-14", "5-9", "0-14",
    "10-14", "0-14", "10-14", "0-14",
    "15-19", "15-29", "15-19", "15-29",
    "20-24", "15-29", "20-24", "15-29",
    "25-29", "15-29", "25-29", "15-29",
    "30-34", "30-39", "30-34", "30-39",
    "35-39", "30-39", "35-39", "30-39",
    "40-44", "40-49", "40-44", "40-49",
    "45-49", "40-49", "45-49", "40-49",
    "50-54", "50-59", "50-54", "50-59",
    "55-59", "50-59", "55-59", "50-59",
    "60-64", "60-69", "60-64", "60-69",
    "65-69", "60-69", "65-69", "60-69",
    "70-74", "70-79", "70-74", "70-79",
    "75-79", "70-79", "75-79", "70-79",
    "80+", "80-84", "80-84", "80+",
    "80+", "85+", "85+", "80+"
  )
}

# get postcode total- and age-specific-populations:
postcode_age_pop <- function() {
  
  # load ABS postcode to LGA concordance
  postcode_lga_lookup <- read_xlsx(
    "data/spatial/abs/CA_POSTCODE_2018_LGA_2018.xlsx",
    sheet = 4,
    skip = 5
  ) %>%
    filter(
      row_number() > 1,
      !is.na(RATIO)
    ) %>%
    select(
      postcode = POSTCODE_2018...1,
      lga = LGA_NAME_2018,
      weight = RATIO
    ) %>%
    # normalise weights
    group_by(postcode) %>%
    mutate(
      weight = weight / sum(weight)
    )
  
  # load meshblock populations
  mb_pops <- read_csv(
    "data/spatial/abs/2016 census mesh block counts.csv",
    col_types = cols(
      MB_CODE_2016 = col_double(),
      MB_CATEGORY_NAME_2016 = col_character(),
      AREA_ALBERS_SQKM = col_double(),
      Dwelling = col_double(),
      Person = col_double(),
      State = col_double()
    )
  )
  
  # load meshblock to postcode concordance
  poa_mb <- read_csv(
    "data/spatial/abs/POA_2016_AUST.csv",
    col_types = cols(
      MB_CODE_2016 = col_double(),
      POA_CODE_2016 = col_character(),
      POA_NAME_2016 = col_character(),
      AREA_ALBERS_SQKM = col_double()
    )
  )
  
  # sum populations by postcode
  postcode_pop <- poa_mb %>%
    select(
      -AREA_ALBERS_SQKM
    ) %>%
    left_join(
      mb_pops,
      by = "MB_CODE_2016"
    ) %>%
    select(
      postcode = POA_NAME_2016,
      population = Person
    ) %>%
    group_by(
      postcode
    ) %>%
    summarise(
      across(
        population,
        sum
      )
    )
  
  # join weighted LGA age-populations to postcodes and normalise to get
  # approximate age distributions for each postcode
  postcode_age_distribution <- postcode_lga_lookup %>%
    full_join(
      lga_age_population(),
      by = c(lga = "LGA_NAME19")
    ) %>%
    filter(
      !is.na(weight),
      !is.na(population)
    ) %>%
    group_by(
      postcode, age
    ) %>%
    # get LGA-level weighted age population by postcode
    summarise(
      population = sum(population * weight),
      .groups = "drop"
    ) %>%
    # convert to populations to fractions
    group_by(
      postcode
    ) %>%
    mutate(
      fraction = population / sum(population)
    ) %>%
    select(
      -population
    )
  
  # join age distributions to populations and multiply to get approximate postcode age populations 
  postcode_age_pop <- postcode_pop %>%
    full_join(
      postcode_age_distribution,
      by = "postcode"
    ) %>%
    mutate(
      population = population * fraction
    ) %>%
    select(
      -fraction
    ) %>%
    left_join(
      age_lookup(),
      by = c("age" = "age_abs"),
    )
  
  postcode_age_pop
  
}

get_poa_lga_correspondence <- function() {
  
  # meshblock populations
  mb_pop <- read_csv(
    "data/spatial/abs/2016 census mesh block counts.csv",
    col_types = cols(
      MB_CODE_2016 = col_double(),
      MB_CATEGORY_NAME_2016 = col_character(),
      AREA_ALBERS_SQKM = col_double(),
      Dwelling = col_double(),
      Person = col_double(),
      State = col_double()
    )
  )
  
  # meshblock - postal area relationships
  mb_poa <- read_csv(
    "data/spatial/abs/POA_2016_AUST.csv",
    col_types = cols(
      MB_CODE_2016 = col_double(),
      POA_CODE_2016 = col_character(),
      POA_NAME_2016 = col_character(),
      AREA_ALBERS_SQKM = col_double()
    )
  )
  
  # meshblock - LGA relationships
  mb_lga <- read_csv(
    "data/spatial/abs/LGA_2019_NSW.csv",
    col_types = cols(
      MB_CODE_2016 = col_double(),
      LGA_CODE_2019 = col_double(),
      LGA_NAME_2019 = col_character(),
      STATE_CODE_2016 = col_double(),
      STATE_NAME_2016 = col_character(),
      AREA_ALBERS_SQKM = col_double()
    )
  )
  
  # join the meshblock files to get LGA, POA, and population, and compute
  # population weight to map from postcode to LGA
  poa_lga_correspondence <- mb_lga %>%
    left_join(
      mb_poa,
      by = "MB_CODE_2016"
    ) %>%
    left_join(
      mb_pop,
      by = "MB_CODE_2016"
    ) %>%
    select(
      MB_CODE_2016,
      POA_CODE_2016,
      LGA_NAME_2019,
      POPULATION = Person
    ) %>%
    group_by(
      POA_CODE_2016, LGA_NAME_2019
    ) %>%
    summarise(
      POPULATION = sum(POPULATION),
      .groups = "drop"
    ) %>%
    arrange(
      POA_CODE_2016,
      LGA_NAME_2019
    ) %>%
    group_by(
      POA_CODE_2016
    ) %>%
    mutate(
      weight = POPULATION / sum(POPULATION),
      weight = replace_na(weight, 1)
    ) %>%
    ungroup() %>%
    select(
      POSTCODE = POA_CODE_2016,
      LGA_NAME_2019,
      weight
    )
  
}

lga_age_population_nsw <- function() {
  read_excel(
    "data/population/2019 NSW Population Projections ASGS 2019 LGA.xlsx",
    sheet = "LGA Sex Age projections",
    skip = 5
  ) %>%
    select(
      lga = `ASGS 2019 LGA`,
      sex = Sex,
      age = `Age 2 digit (85+)`,
      population = `2021`
    ) %>%
    filter(
      lga != "NSW Total",
      !is.na(population)
    ) %>%
    mutate(
      age = case_when(
        age == "00-04" ~ "0-4",
        age == "05-09" ~ "5-9",
        TRUE ~ age
      )
    ) %>%
    group_by(
      lga, age
    ) %>%
    summarise(
      across(
        population,
        sum
      ),
      .groups = "drop"
    )
}

get_nsw_baseline_matrix <- function(R = 3.6, asymp_rel_infectious = 0.5,
                                    age_breaks = c(0, 5, 10, 12, 16, 20, 25,
                                                   30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80,
                                                   Inf)) {
  
  require(conmat)
  
  # fit contact models for each setting to polymod
  contact_model <- fit_setting_contacts(
    contact_data_list = get_polymod_setting_data(),
    population = get_polymod_population()
  )
  
  # predict to NSW
  nsw_settings <- predict_setting_contacts(
    population = abs_age_state("NSW"),
    contact_model = contact_model,
    per_capita_household_size = get_per_capita_household_size(state = "NSW"),
    age_breaks = age_breaks
  )
  
  # get transmission parameters from conmat
  setting_transmission_matrices <- get_setting_transmission_matrices(
    age_breaks = age_breaks,
    asymptomatic_relative_infectiousness = 0.5
  )
  
  # combine them
  settings <- c("home", "school", "work", "other")
  
  setting_ngms_unscaled <- mapply(
    FUN = "*",
    nsw_settings[settings],
    setting_transmission_matrices[settings],
    SIMPLIFY = FALSE
  )
  
  # create and scale the NGM
  ngm_unscaled <- Reduce("+", setting_ngms_unscaled)
  m <- find_m(R, ngm_unscaled)
  ngm <- ngm_unscaled * m
  
  ngm
  
}

# vaccine efficacy estimates
ve <- function(
  parameter = c("susceptibility", "onward"),
  fraction_pfizer = 0.5,
  fraction_dose_2 = 0.9,
  fraction_az_dose_1 = (1 - fraction_pfizer) * (1 - fraction_dose_2),
  fraction_az_dose_2 = (1 - fraction_pfizer) * fraction_dose_2,
  fraction_pfizer_dose_1 = fraction_pfizer * (1 - fraction_dose_2),
  fraction_pfizer_dose_2 = fraction_pfizer * fraction_dose_2,
  dose_2_as_extra = FALSE
) {
  
  # compute vaccine efficacy against susceptibility or onward transmission, for
  # any mix of product and number of doses
  total <- fraction_az_dose_1 +
    fraction_az_dose_2 +
    fraction_pfizer_dose_1 +
    fraction_pfizer_dose_2
  
  if (any(abs(total - 1) > 1e6)) {
    stop("fractions do not sum to one")
  }
  
  parameter <- match.arg(parameter)
  
  ve_estimates <- list(
    susceptibility = list(
      az = list(
        dose_1 = 0.46,
        dose_2 = 0.67
      ),
      pfizer = list(
        dose_1 = 0.57,
        dose_2 = 0.80
      )
    ),
    onward = list(
      az = list(
        dose_1 = 0.02,
        dose_2 = 0.36
      ),
      pfizer = list(
        dose_1 = 0.13,
        dose_2 = 0.65
      )
    )
  )

  # if we want the VE estimates for dose 2 to be expressed as the additional
  # effect on top of dose 1, modify the dose 2 estimates. This is used to
  # capture efficacy over time with lags to acquired immunity, so that the
  # immunity does not drop to 0 at the date of the second dose
  if (dose_2_as_extra) {
    for (parameter in names(ve_estimates)) {
      for (vaccine in names(ve_estimates$susceptibility)) {
        estimate <- ve_estimates[[parameter]][[vaccine]]
        ve_estimates[[parameter]][[vaccine]]$dose_2 <- estimate$dose_2 - estimate$dose_1
      }
    }
  }
  
  ve_estimates[[parameter]]$az$dose_1 * fraction_az_dose_1 +
    ve_estimates[[parameter]]$az$dose_2 * fraction_az_dose_2 +
    ve_estimates[[parameter]]$pfizer$dose_1 * fraction_pfizer_dose_1 +
    ve_estimates[[parameter]]$pfizer$dose_2 * fraction_pfizer_dose_2
  
  
}
